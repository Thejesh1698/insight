{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a0f310-7a5a-4e2b-9359-fdc7111e8130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Role name fetched using \"aws iam list-roles --query 'Roles[?contains(RoleName, `SageMaker`)]'\" in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae8e0e-fd24-4870-9f7f-6f2d8757aa16",
   "metadata": {},
   "source": [
    "## Defining sagemaker roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bc8e0f-2ba5-4e0c-8ed3-c238c3de48c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Create an IAM client\n",
    "iam = boto3.client('iam')\n",
    "# List roles\n",
    "response = iam.list_roles()\n",
    "# If you want to filter for SageMaker roles, you can do it in Python\n",
    "sagemaker_roles = [role for role in response['Roles'] if 'SageMaker' in role['RoleName']]\n",
    "# `sagemaker_roles` now contains a list of SageMaker roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bcddd2-87a2-46d9-a255-7493c713bb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AmazonSageMaker-ExecutionRole-20231030T210397'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_roles[0]['RoleName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c85d3a-5038-4664-8233-54f715e2ebfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name ravi_tej to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::005418323977:role/service-role/AmazonSageMaker-ExecutionRole-20231030T210397\n",
      "sagemaker bucket: sagemaker-ap-south-1-005418323977\n",
      "sagemaker session region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20231030T210397')['Role']['Arn']\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04101810-0801-4a8f-b539-f3eacab7b8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca8391-3908-49fb-8623-3a2ace44c45e",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cebd9-ef26-4904-bb36-a9ddb4e014be",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7454e98d-a08a-4f1c-ba12-077fb2e9a114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_system_prompt =  '''\n",
    "You are the chief editor for a leading Indian financial and business news website. You evaluate critical attributes of articles to gate keep content quality. For many attributes, you will first provide a brief analysis of 15 to 30 words, followed by assessment.\n",
    "\n",
    "1. analysis_is_financial_or_business_news (short text) : <analyse if article pertains to finance/business or not. government policies directly impacting indian corporations or investors are ok, but not if aren't>\n",
    "2. is_financial_or_business_news (True or False) : <True or False based on previous attribute>\n",
    "3. analysis_of_relevant_for_india (short text) : <analyse if article is relevant for indians. for example articles about 401k or small foreign companies won't be relevant for india. however changes to fed interest rates or nasdaq or large multinational important news will be relevant>\n",
    "4. relevant_for_india (True or False) : <True or False based on previous attribute>\n",
    "5. analysis_of_article_validity_duration (short text) : <Analyse relevance duration: Stock fluctuations, 1 day; significant policy changes, weeks; educational content is timeless unless it refers to any tax or other regulations in which case only 30 days. International news in India has shorter lifespan. popular topics are usually not timeless; quarterly analysis is valid for a week, yearly for a couple of weeks and a much longer one for a month>\n",
    "6. article_validity_duration (one of 1, 3, 7, 14, 30, -1) : <calculate number of days based on previous attribute. -1: timeless. 1: article is relevant only for that day. 3: for a couple of days. 7: for a week. 14: for a couple of weeks. 30: for a month>\n",
    "7. analysis_of_popularity (short text) : <analyse likely popularity of article - if its for niche audience, moderate_popularity or should be part of breaking_news section, depending on number of people who will be impacted by the news and the scale of the event. foreign entities known in india but not very popular will be mostly niche or rarely moderately popular. articles targeted to very specific business or pratices will be niche. infotainment business and financial articles with some drama are likely to be more popular. articles with a list of rules without compelling story-telling will be for niche audience>\n",
    "8. popularity (one of niche, moderately_popular, breaking_news) : <based on previous attribute>\n",
    "9. analysis_of_article_type (short text) : <analyse if the article is majorly factual, is an opinion piece, analysis, educational or likely sponsored. factual articles relay events. opinion pieces have predictions either from the author or from statements without data. analysis pieces have substantial data to justify. if an article is overly zealous on certain stock and seems like an ad, then it is sponsored>\n",
    "10. article_type (one of fact, opinion, analysis, educational, sponsored) : <based on previous attribute>\n",
    "11. analysis_of_article_sentiment (short_text): <analyse if the sentiment of the article is bullish, bearish or NA. balanced is NA>\n",
    "12. article_sentiment (one of bull, bear, NA): <based on previous attribute>\n",
    "13. headline_suggestion (short text) : <Write a headline based on the content of the article>\n",
    "14. summary (text of 60 words) : <Generate concise, entity-dense summary. The summary should become highly dense but easily understood without the Article. Don't keep the summary too short, but limit it to no more than 60 words>\n",
    "\n",
    "your response should be a json structure with all the 14 above keys without missing any key. It is very important that the response is directly readable with json.loads(). no preamble or postamble. respond in the exact following structure:\n",
    "\n",
    "{\n",
    "\"analysis_is_financial_or_business_news\": \"\",\n",
    "\"is_financial_or_business_news\": \"\",\n",
    "\"analysis_of_relevant_for_india\": \"\",\n",
    "\"relevant_for_india\": \"\",\n",
    "\"analysis_of_article_validity_duration\": \"\",\n",
    "\"article_validity_duration\": \"\",\n",
    "\"analysis_of_popularity\": \"\",\n",
    "\"popularity\": \"\",\n",
    "\"analysis_of_article_type\": \"\",\n",
    "\"article_type\": \"\",\n",
    "\"analysis_of_article_sentiment\": \"\",\n",
    "\"article_sentiment\": \"\",\n",
    "\"headline_suggestion\": \"\",\n",
    "\"summary\": \"\"\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88b5ca-85d3-477b-9979-c1a0d5dafb20",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc61adcb-bd1c-48e2-b1be-cba338a7a0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from random import randint\n",
    "import sys\n",
    "sys.path.append(\"../scripts/utils\")\n",
    "from pack_dataset import pack_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68482869-4c3a-44d8-92fc-9cfe23275ab2",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d12915-99b3-49b0-a53d-fa4e1d31626a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "# model_id = 'ehartford/dolphin-2.0-mistral-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "300c6269-75a9-41cf-a6cc-be79209a6d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0da848-579d-46d0-9990-ea3784aad418",
   "metadata": {},
   "source": [
    "### Formatting the GPT responses for prompt output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba60830-f907-4d3d-a05e-90a0581ef691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('train_set_full_27_nov.json', 'r') as f:\n",
    "    train_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76aa1b2-3afd-450e-b719-097907da563f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# structuring the format\n",
    "for art_id in train_set:\n",
    "    res = train_set[art_id]['response']\n",
    "    train_set[art_id] = {'content': train_set[art_id]['content'], 'response': {}}\n",
    "    train_set[art_id]['response']['attributes'] = res[0]\n",
    "    train_set[art_id]['response']['summaries'] = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3bd827-0185-4db4-873f-c5ba4a62bc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_validity_duration(val):\n",
    "    val = int(val)\n",
    "    valid_days = [-1, 1, 3, 7, 14, 30]\n",
    "    if val in valid_days:\n",
    "        return val\n",
    "    else:\n",
    "        for i in valid_days:\n",
    "            if val > i:\n",
    "                valid_value = i\n",
    "        return valid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b584ed-5ccf-450c-87d5-fbb6550e7fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_validity_duration('365')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a7d9ab6-1cbd-4990-a68e-6a5fa744da89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for art_id in train_set:\n",
    "    train_set[art_id]['unified_response'] = train_set[art_id]['response']['attributes']\n",
    "    train_set[art_id]['unified_response']['summary'] = train_set[art_id]['response']['summaries'][0]['denser_summary']\n",
    "    train_set[art_id]['unified_response']['is_financial_or_business_news'] = True if int(train_set[art_id]['response']['attributes']['is_financial_or_business_news']) == 1 else False if int(train_set[art_id]['response']['attributes']['is_financial_or_business_news']) == 0 else None\n",
    "    train_set[art_id]['unified_response']['relevant_for_india'] = True if int(train_set[art_id]['response']['attributes']['relevant_for_india']) == 1 else False if int(train_set[art_id]['response']['attributes']['relevant_for_india']) == 0 else None\n",
    "    train_set[art_id]['unified_response']['article_validity_duration'] = correct_validity_duration(train_set[art_id]['response']['attributes']['article_validity_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "60e3a32a-12ee-433b-9014-52895bcddab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convenience Fee: Definition Examples and How to Avoid Them: What Is a Convenience Fee A convenience fee is a fee charged by a seller when a consumer pays with an electronic payment card rather than by a standard form of payment accepted by the business. Standard payments include cash check or an Automated Clearing House (ACH) transfer. Convenience fees can be a fixed dollar amount or a percentage of the transaction amount usually 2% to 3% and must be disclosed to the consumer in advance. Types of payments where the payee typically charges a convenience fee include mortgage payments property tax payments college tuition and taxes. Understanding a Convenience Fee Convenience fees can help a business cover some of the costs imposed through electronic payment processing. Businesses have to pay a merchant fee every time one of their customers uses a credit card. For most businesses such as department stores and grocery stores a merchant fee is just a cost of doing business. On the other hand a movie theater or concert venue typically takes payment at the box office so an alternative payment channel such as the phone or online via credit card would result in additional fees for them thus they would charge a convenience fee for doing business in this way. Its important to note that a convenience fee is different than a surcharge. A surcharge is the ability to charge extra just for the benefit of using a credit card while a convenience fee is for a specific use such as taxes or tuition or payment through alternative channels such as by phone or online. Example of a Convenience Fee Suppose that you wanted to pay the Internal Revenue Service (IRS) by credit card. The IRS will accept credit card payments through several different payment processing companies and they all charge convenience fees as allowed by the credit card companies. One might charge 2.49% with a $3.95 minimum while another might charge 3.93% with a $2.00 minimum. Thus if you need to send the IRS $2000 and you wanted to pay by credit card you could be required to pay a maximum convenience fee of 0.0393  $2000  $78.60. Convenience Fee Regulations Some people might not mind paying a convenience fee for the benefit of using an electronic payment card particularly if the benefit of earning rewards on the card outweighs the cost of the convenience fee. However this practice is regulated by both state legislation and card networks. As a regulated activity businesses must be cautious in instituting convenience fees and surcharges for customers. Ten states have laws on the book outlawing convenience fees and surcharges but some of these laws have been invalidated by court rulings. However surcharges remain fully illegal in Connecticut Massachusetts and Puerto Rico. For the states that allow merchants to set their own surcharge levels there are caps at approximately 4%. Credit Card Company Policies on Convenience Fees Every credit card provider has different rules on convenience fees. Some are more thorough than others and it may be worth researching the best credit cards to find those with the best policies. Below are the rules of some of the major credit card providers: Mastercard: Allows for convenience fees as long as they are used for all transactions and methods of payment. Visa: Allows for convenience fees only if the payment is through an alternative channel such a by phone or online and the business first notifies the consumer and that the fee is a flat rate not a percentage of the sale. American Express: Has a policy that does not include convenience fees nor surcharges. Discover: Has a policy that also does not include convenience fees nor surcharges. How to Avoid Convenience Fees There are really only two options when it comes to convenience fees either to pay the fee or to use another form of payment such as cash. In many cases some businesses such as gas stations offer discounts when a consumer pays by cash. Its always worth asking a business if they offer a cash discount. Convenience fees are meant to be disclosed at the point of sale so if you discover you have been charged a fee after the fact its important to take this up with your credit card company. How Do You Avoid Convenience Fees for Movie Tickets Some movie theaters charge convenience fees for online or advance orders. If you want to avoid these fees try ordering your ticket at the box office on the day of the show. This runs the risk that your chosen seats may not be available. How Do You Avoid Convenience Fees When Paying Rent Some landlords allow their tenants to pay rent online through a third-party payment system which may charge a percentage of the rent as a convenience fee. Depending on the terms of your lease you may be able to avoid these fees by paying with a check money order cash or other form of non-online payment. Some states prohibit landlords from requiring any form of payment that comes with mandatory surcharges in others it is perfectly legal. What Is the Convenience Fee for Turbotax Intuit charges a 2.49% convenience fee if you choose to pay your taxes with credit card when you file your taxes with TurboTax. The Bottom Line A convenience fee is an additional charge for online or credit card payments sometimes charged by a third-party payment processor. Convenience fees are normally a flat fee or a small percentage of the total payment but they can add up to a substantial sum if the payments are large or recurring. You may be able to avoid convenience fees by paying with cash or another form of payment.'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['6555c7e14b13023f9348e982']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d6e215e4-5e6f-47df-ae09-d6dc67ba986e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['651de33fa662d76276b803d9', '6555c2124b13023f9348d13c', '65367ff01e5cc42b1b143d0d', '651e1ddca662d76276b892c7', '652ebbb81e5cc42b1b1399e8', '6555b49f4b13023f9348b6d6', '65316f831e5cc42b1b1404ea', '65309c0a1e5cc42b1b13a6b6', '651e17dfa662d76276b884de', '651e2032a662d76276b8985d', '6555c1124b13023f9348ce01', '6556d9654b13023f934aee38', '6540c74a2936d70acf71e4e7', '651de21ea662d76276b800b4', '6536800e1e5cc42b1b143f17', '655b3e4c4b13023f934af3c3', '651de220a662d76276b800b9', '652ebbdc1e5cc42b1b139b7a', '65316fb61e5cc42b1b14140a', '6555c8154b13023f9349066e', '6555c99c4b13023f93492ae9', '6555cb5b4b13023f9349cbff', '6555c7df4b13023f9348e8ab', '6555c4794b13023f9348d8af', '6555c80e4b13023f934901f0', '652ebba21e5cc42b1b139964', '6555c37f4b13023f9348d497', '654856d2dc4fa72a6c403a46', '6555bfd74b13023f9348c8a6', '6555c0614b13023f9348cb14', '6555c7c64b13023f9348e0e6', '6555ca964b13023f93494a79', '653168c31e5cc42b1b13b8a4', '6555c8214b13023f93490e78', '6555c4754b13023f9348d89a', '651e19e6a662d76276b88985', '6555bfee4b13023f9348c912', '6555cc104b13023f934a4915', '651e0e02a662d76276b86db8', '653170021e5cc42b1b14318b', '653169231e5cc42b1b13d290', '6555cbe94b13023f934a2dbd', '6555b54e4b13023f9348ba8b', '6555c8254b13023f93491114', '653169361e5cc42b1b13d66e', '65316fc71e5cc42b1b141a0f', '6555cc984b13023f934aaa72', '65367ffa1e5cc42b1b143d9d', '6555c8464b13023f93492443', '65316ff01e5cc42b1b142a4f', '6555b5994b13023f9348bd7b', '6555ccbc4b13023f934ac6de', '6555cc924b13023f934aa5a9', '651e0fcda662d76276b871f0', '651df7f2a662d76276b838d8', '653168921e5cc42b1b13b03d', '6555b4ae4b13023f9348b72c', '65316eed1e5cc42b1b13e9d8', '6555cbe14b13023f934a2822', '65316aeb1e5cc42b1b13dd9d', '6555cc2d4b13023f934a5ed8', '6555c80a4b13023f9348ff6a', '653170011e5cc42b1b143137', '651e1d27a662d76276b8910d', '6555c7e24b13023f9348ea37', '651e21cda662d76276b89c2f', '6555c8224b13023f93490f22', '65316f681e5cc42b1b13fe29', '651dd871a662d76276b7e732', '652df9085669b40a3b5ab6f8', '6555ccbb4b13023f934ac5c3', '6555c8134b13023f9349056d', '6555cc014b13023f934a3ea6', '652fc7f51e5cc42b1b139fb6', '651dc5bba662d76276b7b4cf', '6531690f1e5cc42b1b13ccc9', '651df501a662d76276b83248', '6555cc8e4b13023f934aa3c3', '652f4a881e5cc42b1b139dcd', '651e1561a662d76276b87eef', '651dfc6ca662d76276b84305', '65316fa91e5cc42b1b140fb2', '653169141e5cc42b1b13ce18', '651df185a662d76276b82936', '6555c06a4b13023f9348cb2f', '6555c2be4b13023f9348d2cc', '651e1599a662d76276b87f76', '652fcc771e5cc42b1b13a1fd', '653169371e5cc42b1b13d68f', '6555cbff4b13023f934a3d7e', '6555cc584b13023f934a7e0d', '651dcb91a662d76276b7c4b0', '651e06f7a662d76276b85d73', '653168b91e5cc42b1b13b6bc', '651dc50fa662d76276b7b339', '6555c8364b13023f93491d1c', '6555c8c24b13023f9349274d', '6555c0dd4b13023f9348cd1b', '6555cbd94b13023f934a223b', '6555ccdb4b13023f934ae0f4', '651e0d4ba662d76276b86c0e', '6555cc884b13023f934a9fbc', '6555cb184b13023f9349a019', '6531691b1e5cc42b1b13d071', '6536802e1e5cc42b1b144217', '651dc7b3a662d76276b7ba22', '6555ca784b13023f93493d10', '6555bff14b13023f9348c91e', '6555cbf34b13023f934a34e4', '65316fad1e5cc42b1b1410ee', '651dfe34a662d76276b84769', '6541bf4cdc4fa72a6c4036a8', '651e0f9ca662d76276b87181', '6555ca854b13023f934941db', '65316f001e5cc42b1b13ebec', '651dcf2ca662d76276b7ce9c', '65316f4d1e5cc42b1b13f85f', '6555cc264b13023f934a59be', '6555ca734b13023f93493b70', '6555cccf4b13023f934ad76a', '65316ef81e5cc42b1b13eafb', '6555c3f84b13023f9348d667', '65316e9b1e5cc42b1b13e5b7', '65316ee91e5cc42b1b13e96d', '651e1613a662d76276b88093', '6555c1254b13023f9348ce50', '651dc7f2a662d76276b7bad9', '6555cb764b13023f9349ddfb', '6531693d1e5cc42b1b13d750', '6555cbe04b13023f934a26d3', '651dcd13a662d76276b7c8c4', '653168f11e5cc42b1b13c419', '651dfbe4a662d76276b841ae', '653680281e5cc42b1b14416d', '651e12e6a662d76276b8793f', '651de4d4a662d76276b8080e', '651dd8ada662d76276b7e7cb', '6555c30f4b13023f9348d37a', '651de55ba662d76276b80963', '651e00b6a662d76276b84db7', '6555cbf04b13023f934a3294', '6555ccbd4b13023f934ac7dc', '651df829a662d76276b83915', '652fca671e5cc42b1b13a0eb', '651dfbada662d76276b84124', '65316f8e1e5cc42b1b1407c5', '6555b4934b13023f9348b69d', '6555cb534b13023f9349c650', '65367b381e5cc42b1b143bb5', '651e1bf7a662d76276b88e2f', '6555ca474b13023f9349314d', '6555c7e14b13023f9348e982', '651de570a662d76276b80999', '6555cb684b13023f9349d4db', '6555cbe54b13023f934a2aac', '6555cc554b13023f934a7c73', '651e0b8ca662d76276b86806', '6555cc184b13023f934a4f41', '6555cbed4b13023f934a301d', '6555c7f54b13023f9348f2fb', '651dd7cda662d76276b7e580', '651df639a662d76276b83526', '651dc812a662d76276b7bb2d', '6555b53e4b13023f9348ba18', '652f4a721e5cc42b1b139cfc', '651dd0cfa662d76276b7d331', '6549a84fdc4fa72a6c403b4e', '651e1ca1a662d76276b88fc6', '65316a481e5cc42b1b13dbcd', '65316d0d1e5cc42b1b13e1b3', '652ebbba1e5cc42b1b1399fc', '651e185ba662d76276b88604', '653680241e5cc42b1b144108', '65316f601e5cc42b1b13fc54', '651e0721a662d76276b85dde', '651dee25a662d76276b82078', '651dc6e2a662d76276b7b7bd', '6555cb4f4b13023f9349c3c3', '6555c7fc4b13023f9348f6b7', '651e112da662d76276b87532', '6555ca7b4b13023f93493dc5', '651de6c8a662d76276b80d3b', '65367ff01e5cc42b1b143d10', '6555ca9a4b13023f93494c6a', '6555c1434b13023f9348cedb', '651dc9f6a662d76276b7c05e', '6555c7fb4b13023f9348f645', '6555c4644b13023f9348d857', '6555c1844b13023f9348cfdd', '6555c7f04b13023f9348f09a', '65316fac1e5cc42b1b1410ad', '651e17ada662d76276b88469', '653168bd1e5cc42b1b13b770', '65316eb81e5cc42b1b13e666', '651de443a662d76276b80699', '6531692d1e5cc42b1b13d4e2', '653168bd1e5cc42b1b13b75d', '653169551e5cc42b1b13d960', '651deff0a662d76276b8250c', '6555c8204b13023f93490d78', '655c57e5eee55a44e0ac1e7a', '6555b5724b13023f9348bbe5', '651df8f2a662d76276b83a8a', '652ebbd61e5cc42b1b139b23', '653168cd1e5cc42b1b13baec', '651e147ba662d76276b87cd1', '6555cb954b13023f9349f3e2', '6555c83d4b13023f934921c7', '6555c0b84b13023f9348cc83', '6555c1024b13023f9348cdbb', '6555c25c4b13023f9348d202', '651dcedea662d76276b7cdc3', '6555c28a4b13023f9348d26d', '6555c8384b13023f93491e21', '651df964a662d76276b83b9f', '6555cc5b4b13023f934a7ffa', '6555c7cd4b13023f9348e2ea', '6555ca714b13023f93493a98', '652ebbc91e5cc42b1b139a8a', '651e2462a662d76276b8a25f', '6555cc3a4b13023f934a6864', '6555ccc14b13023f934acb0b', '6555cc034b13023f934a3fac', '651dc6d3a662d76276b7b795', '6531692a1e5cc42b1b13d461', '653168ca1e5cc42b1b13ba13', '652ebbca1e5cc42b1b139a91', '6555c83f4b13023f934922b5', '6555c0844b13023f9348cba2', '652ebbc31e5cc42b1b139a4d', '651e1647a662d76276b88118', '6555cc184b13023f934a4ed8', '6555b5a44b13023f9348bdef', '651e0398a662d76276b85525', '6555b58b4b13023f9348bcee', '651e2030a662d76276b89859', '6531683c1e5cc42b1b13a842', '6555c7e34b13023f9348ea9d', '6555c30c4b13023f9348d369', '65367ff11e5cc42b1b143d28', '6555cb944b13023f9349f33b', '6555c7ab4b13023f9348dd7c', '65316f061e5cc42b1b13ec8e', '6555c8374b13023f93491df6', '652fcea91e5cc42b1b13a31f', '651e111ea662d76276b8750a', '6555cc854b13023f934a9d92', '651de594a662d76276b809f5', '651dfaa1a662d76276b83e94', '6555ccc04b13023f934aca23', '651dd7d7a662d76276b7e59d', '6555cc7c4b13023f934a9792', '6544625adc4fa72a6c403947', '6555c3724b13023f9348d46e', '6555cc624b13023f934a8562', '654c4b50dc4fa72a6c403d42', '6555c7ea4b13023f9348edc7', '6555cbe54b13023f934a2a75', '6555cb9b4b13023f9349f7bd', '6555ca934b13023f93494878', '6555b4d34b13023f9348b7f3', '6555c4754b13023f9348d89f', '652fc9371e5cc42b1b13a04f', '6555cb084b13023f9349961c', '652fc7861e5cc42b1b139f81', '6555cc244b13023f934a57d2', '6555cad94b13023f93497656', '651df88ba662d76276b839a6', '6555cc634b13023f934a8682', '6555ca954b13023f934949da', '6555cac24b13023f934965ee', '651deae5a662d76276b817f6', '6555cae54b13023f93497ec1', '6555cbed4b13023f934a30da', '6555caf44b13023f934988a9', '651dee4ca662d76276b820e4', '6555c80f4b13023f93490257', '6525a73a598c6618f0608911', '6555cc184b13023f934a4f2e', '6555cb4b4b13023f9349c15e', '651dd8f7a662d76276b7e883', '651e0385a662d76276b854fb', '653168881e5cc42b1b13aeda', '6555c7f44b13023f9348f266', '653170041e5cc42b1b143263', '653680061e5cc42b1b143e6e', '6555c7f64b13023f9348f36e', '652e31045669b40a3b5ab745', '653168ae1e5cc42b1b13b49e', '6555cac94b13023f93496aa5', '651dda13a662d76276b7eb67', '65316f101e5cc42b1b13edf7', '65316f9d1e5cc42b1b140bff', '651dd314a662d76276b7d9e4', '652e310f5669b40a3b5ab74e', '6555c82a4b13023f93491441', '651e1baea662d76276b88d8b', '6555c8034b13023f9348fb23', '651df6aca662d76276b83630', '6555c8034b13023f9348fafa', '6555c0e04b13023f9348cd29', '65446255dc4fa72a6c403915', '6555cc074b13023f934a4299', '65316f8a1e5cc42b1b1406af', '6555c7f64b13023f9348f37c', '6531693c1e5cc42b1b13d73d', '653169141e5cc42b1b13ce58', '6555c9fb4b13023f93492d9f', '6555caad4b13023f93495801', '65316f0d1e5cc42b1b13ed87', '651df296a662d76276b82c25', '6555c11c4b13023f9348ce2e', '651e0b60a662d76276b8679f', '6555cb7c4b13023f9349e220', '6555b5984b13023f9348bd6a', '651df28ea662d76276b82c10', '6555bfa04b13023f9348c7bc', '6555c8644b13023f934926fa', '653168aa1e5cc42b1b13b40a', '651df191a662d76276b82957', '65316ddc1e5cc42b1b13e3b7', '65316ce01e5cc42b1b13e151', '653168871e5cc42b1b13aebf', '6555c5194b13023f9348db5f', '6555c4364b13023f9348d798', '6555ca634b13023f934936b0', '65316f6f1e5cc42b1b13ffcb', '651ddaa0a662d76276b7ecce', '651dfa3ea662d76276b83da2', '6555cc9a4b13023f934aabcd', '65316f901e5cc42b1b14086a', '6555ccc24b13023f934acbd9', '6555c8174b13023f934907ac', '65316f951e5cc42b1b1409a6', '6555c7ee4b13023f9348ef9a', '6555cc874b13023f934a9ed1', '651ddb05a662d76276b7eddc', '6555cbcd4b13023f934a19ec', '6555c4d94b13023f9348da48', '6555cc214b13023f934a556a', '653168c61e5cc42b1b13b937', '653168731e5cc42b1b13ac4b', '6555ca8d4b13023f934945bb', '65367fef1e5cc42b1b143d09', '65316ff41e5cc42b1b142be3', '6555c0bd4b13023f9348cc94', '653168e71e5cc42b1b13c156', '651dff56a662d76276b84a31', '6555ccd14b13023f934ad899', '653168cc1e5cc42b1b13baa5', '6555c8114b13023f9349040a', '6555cc874b13023f934a9f50', '653169041e5cc42b1b13c986', '6555b5c14b13023f9348bf5c', '6555c7fd4b13023f9348f71f', '6536801b1e5cc42b1b144026', '6555c83b4b13023f9349209f', '6555c9804b13023f93492a89', '6555c9b74b13023f93492b57', '6555c8024b13023f9348fa66', '6555cad34b13023f93497200', '6531ed6e1e5cc42b1b1437b6', '651e1afaa662d76276b88bf7', '651de9c1a662d76276b814e5', '6555c8464b13023f93492431', '651e10f6a662d76276b874b0', '65316f101e5cc42b1b13ee06', '651df8a6a662d76276b839e0', '653168b31e5cc42b1b13b5a4', '6555cb194b13023f9349a109', '651dd065a662d76276b7d207', '651e0983a662d76276b8634c', '653168a21e5cc42b1b13b2c3', '6555cba14b13023f9349fb8b', '652fcd741e5cc42b1b13a27e', '6555c7c64b13023f9348e0fd', '651dd8f3a662d76276b7e87b', '654f26894b13023f9348b31b', '6555cc884b13023f934a9fd8', '651e0dfda662d76276b86da8', '6555c8114b13023f93490380', '6531700d1e5cc42b1b1435e3', '6555cc0c4b13023f934a466a', '6555cb564b13023f9349c8a2', '6555b4dd4b13023f9348b82e', '651e0275a662d76276b85246', '6555c0114b13023f9348c9b0', '6555c2434b13023f9348d1c0', '65316fd41e5cc42b1b141f1f', '6540c8aa2936d70acf71ed8f', '653168ba1e5cc42b1b13b6d1', '6555cb7c4b13023f9349e219', '6555caac4b13023f9349574b', '6555ccd14b13023f934ad8db', '6555c0f64b13023f9348cd83', '653168981e5cc42b1b13b120', '6555cb5e4b13023f9349cde7', '651e09b4a662d76276b863b3', '6555cab34b13023f93495ba6', '651e0cc2a662d76276b86acb', '651dd1e8a662d76276b7d66d', '6555cc514b13023f934a78fb', '6531691a1e5cc42b1b13cfe5', '6555cbe14b13023f934a2825', '653169241e5cc42b1b13d2e5', '651e2252a662d76276b89d66', '6555cb764b13023f9349de07', '652ebb9c1e5cc42b1b13993e', '6555cc4d4b13023f934a76ac', '6555cc5e4b13023f934a8260', '652fcacd1e5cc42b1b13a11f', '6531692b1e5cc42b1b13d497', '6555caf54b13023f934989b1', '6555c7d54b13023f9348e51b', '651dd35ca662d76276b7daaa', '653169c01e5cc42b1b13da51', '6555ccc14b13023f934acb3b', '653168ff1e5cc42b1b13c81f', '6531693b1e5cc42b1b13d714', '651e0b4da662d76276b86775', '6555ccb14b13023f934abd41', '6555bf834b13023f9348c76b', '65367fd21e5cc42b1b143bf7', '6555cc5f4b13023f934a82e0', '651e0764a662d76276b85e7e', '6531683b1e5cc42b1b13a832', '651e1997a662d76276b888d5', '6555cb674b13023f9349d3cb', '6555c82e4b13023f934917b0', '6555bebd4b13023f9348c514', '651e08bba662d76276b8618d', '655d6eeee1d8342798a44987', '651df8cca662d76276b83a31', '6555cbe74b13023f934a2c0b', '651dc6e2a662d76276b7b7be', '6555cca54b13023f934ab3d7', '6555c7e74b13023f9348ec0d', '65316f201e5cc42b1b13f051', '653170071e5cc42b1b1433da', '651e1be2a662d76276b88e01', '6555cc6f4b13023f934a8e9d', '6541bf4bdc4fa72a6c40368b', '651e1c01a662d76276b88e44', '6555c81c4b13023f93490b2e', '651e20d4a662d76276b899e5', '65316ef71e5cc42b1b13ead8', '6555b5d84b13023f9348c27b', '65316ffb1e5cc42b1b142e7d', '651dc7a8a662d76276b7ba00', '655d6f15e1d8342798a44a14', '652fd16f1e5cc42b1b13a435', '6555c8134b13023f93490564', '651dd25aa662d76276b7d7c4', '651e062da662d76276b85b8b', '6555cb644b13023f9349d1b0', '651dcbf0a662d76276b7c5b8', '6555c5234b13023f9348db8a', '653680071e5cc42b1b143e7d', '6555ca3e4b13023f93493050', '651dc9eda662d76276b7c045', '6555c52a4b13023f9348dbb6', '652ebb821e5cc42b1b1398dd', '6555c4b54b13023f9348d9a9', '6555c80b4b13023f9348ff98', '652fd0741e5cc42b1b13a3b8', '653680291e5cc42b1b144198', '6555cb5f4b13023f9349ce2e', '651dee60a662d76276b8211c', '65316fb91e5cc42b1b141517', '6555cbf54b13023f934a35db', '652fcc921e5cc42b1b13a20a', '652fc8dd1e5cc42b1b13a026', '6555cb1a4b13023f9349a148', '651e1114a662d76276b874f3', '6555cade4b13023f934979b2', '6555bfc64b13023f9348c855', '6555cc604b13023f934a83c7', '65367ffc1e5cc42b1b143dbf', '652ebbce1e5cc42b1b139abb', '6555ca704b13023f93493a48', '653169461e5cc42b1b13d850', '6555c82c4b13023f9349161a', '652ebbcd1e5cc42b1b139ab2', '651e2097a662d76276b89954', '6540c8a82936d70acf71ed1a', '6555c8264b13023f9349113e', '65316f761e5cc42b1b140179', '6555cc194b13023f934a4f9c', '652ebbc61e5cc42b1b139a63', '6555cb304b13023f9349af9c', '65316eb91e5cc42b1b13e671', '653680301e5cc42b1b14425a', '651dde6ba662d76276b7f6c4', '651e2190a662d76276b89ba6', '65316eaf1e5cc42b1b13e624', '651dc7ada662d76276b7ba13', '651dfea7a662d76276b8488a', '652fc72d1e5cc42b1b139f56', '6555cc544b13023f934a7b67', '6555c8224b13023f93490eff', '653168c31e5cc42b1b13b889', '65316e5c1e5cc42b1b13e4db', '6555cc324b13023f934a6257', '6555c8264b13023f934911c0', '6555cbf94b13023f934a38da', '6555cc694b13023f934a8ac7', '6555cc0f4b13023f934a4834', '651dcbf9a662d76276b7c5d3', '6555cbfa4b13023f934a39e0', '6555c90d4b13023f9349287b', '655d6f13e1d8342798a44a00', '651dc474a662d76276b7b1bf', '6555c0d34b13023f9348ccf6', '6555cc924b13023f934aa625', '652fd2cd1e5cc42b1b13a4e3', '653168c31e5cc42b1b13b8a6', '651e0026a662d76276b84c48', '6555c8124b13023f9349045d', '651e1b8ca662d76276b88d38', '6555c7c24b13023f9348e036', '6555cc354b13023f934a64ff', '6555c8334b13023f93491b17', '6555cb004b13023f934990d6', '65316e651e5cc42b1b13e4ef', '6555ccbe4b13023f934ac8a8', '65316d661e5cc42b1b13e27e', '651e2092a662d76276b89946', '6555cc294b13023f934a5b6e', '651dd464a662d76276b7dd5b', '655191564b13023f9348b3ac', '6555bf734b13023f9348c741', '6555caf44b13023f93498908', '6555cc7e4b13023f934a990b', '653168ec1e5cc42b1b13c2c6', '6555c0344b13023f9348ca5c', '6555cc304b13023f934a60f1', '6555cbde4b13023f934a25d6', '6555cbc54b13023f934a149b', '6555c0384b13023f9348ca67', '6525a728598c6618f06087ec', '6555cb644b13023f9349d22d', '6555c7be4b13023f9348df7c', '65316f4b1e5cc42b1b13f7da', '6555cc574b13023f934a7ddf', '6555ca924b13023f93494857', '6555c8554b13023f934925fd', '653170001e5cc42b1b1430c4', '6555cafb4b13023f93498d68', '6555c8114b13023f93490416', '6555caa54b13023f93495295', '65316ebd1e5cc42b1b13e690', '6555c7a94b13023f9348dd66', '6555c8174b13023f9349079b', '653169341e5cc42b1b13d621', '653169261e5cc42b1b13d36d', '651ddfa3a662d76276b7fa22', '6555c8324b13023f93491a2b', '651e17a3a662d76276b8844e', '6555cbe24b13023f934a28c0', '6555b4854b13023f9348b65f', '6555bfdc4b13023f9348c8ba', '6555cc274b13023f934a5a79', '651df974a662d76276b83bca', '6531684e1e5cc42b1b13a92c', '651de194a662d76276b7ff54', '6555c2544b13023f9348d1ef', '6555cc354b13023f934a6487', '6555c9324b13023f9349293f', '6555caff4b13023f93499009', '6555ccbf4b13023f934ac902', '651e1990a662d76276b888c4', '6555cc374b13023f934a65d0', '6555cc8c4b13023f934aa231', '653169091e5cc42b1b13caf1', '6555c8294b13023f934913b3', '651e167aa662d76276b8818a', '652fd14e1e5cc42b1b13a424', '6555c30d4b13023f9348d36f', '6555cbdc4b13023f934a2474', '65316f661e5cc42b1b13fda4', '651de168a662d76276b7fee7', '653168901e5cc42b1b13aff2', '651def1ea662d76276b82308', '653169121e5cc42b1b13cd8d', '65316f0c1e5cc42b1b13ed68', '6555b5d04b13023f9348c0db', '651e0f7da662d76276b87136', '651e265aa662d76276b8a6fd', '6555bec24b13023f9348c531', '6555c8084b13023f9348fe34', '651dd0e2a662d76276b7d368', '653168871e5cc42b1b13aeac', '651dd2eda662d76276b7d976', '6555ca0d4b13023f93492e27', '65316a751e5cc42b1b13dc68', '651de9b2a662d76276b814be', '6555cad24b13023f934970ef', '6555c7e34b13023f9348ea69', '6555c8044b13023f9348fb4d', '6555b5254b13023f9348b98d', '653168a11e5cc42b1b13b286', '65316f3a1e5cc42b1b13f4b0', '6531ed6e1e5cc42b1b1437a1', '6555c9434b13023f9349296d', '651df9a7a662d76276b83c46', '651e2107a662d76276b89a61', '652ebbdb1e5cc42b1b139b68', '6555c7f74b13023f9348f3e7', '653170071e5cc42b1b1433c6', '65316b4f1e5cc42b1b13de6c', '651dcf4ca662d76276b7cef6', '6555c0a34b13023f9348cc2f', '6555cc7d4b13023f934a987a', '6555caa44b13023f934951f7', '651deb65a662d76276b81947', '6555cc014b13023f934a3ea9', '651e1f8ca662d76276b896e2', '651df70da662d76276b83718', '6555bf764b13023f9348c749', '6555ccc44b13023f934acd52', '6555cbb94b13023f934a0c96', '65316fae1e5cc42b1b141162', '65316f121e5cc42b1b13ee4f', '65316eef1e5cc42b1b13ea12', '65316f3d1e5cc42b1b13f521', '65316f5f1e5cc42b1b13fc24', '652ebb8a1e5cc42b1b1398f5', '6555cb874b13023f9349ea48', '6555ca994b13023f93494c06', '651de0eda662d76276b7fda1', '651e17b8a662d76276b88483', '651df783a662d76276b83831', '65310c541e5cc42b1b13a74d', '6555cc0a4b13023f934a4484', '653170051e5cc42b1b1432de', '651e1647a662d76276b88119', '6531694f1e5cc42b1b13d90e', '6531689a1e5cc42b1b13b163', '651def72a662d76276b823e0', '653168ff1e5cc42b1b13c80b', '65316ede1e5cc42b1b13e889', '651dc89fa662d76276b7bcba', '6531690e1e5cc42b1b13cc6b', '6555cbe74b13023f934a2c17', '652fc71f1e5cc42b1b139f4f', '6555c8064b13023f9348fc8a', '6555cbe14b13023f934a283c', '6555cc5b4b13023f934a808f', '652ebbbd1e5cc42b1b139a14', '653170061e5cc42b1b14331f', '653169131e5cc42b1b13cdc7', '651df2c1a662d76276b82c98', '6555cc6b4b13023f934a8c50', '6555cc884b13023f934a9fd2', '651ded13a662d76276b81d97', '6555cc744b13023f934a92ac', '6536800a1e5cc42b1b143ebc', '6555c0174b13023f9348c9ca', '6555cc614b13023f934a84d9', '6555c4c24b13023f9348d9e3', '651df4a2a662d76276b83161', '65316eca1e5cc42b1b13e723', '6555cc074b13023f934a4300', '6555cc464b13023f934a7139', '65316e651e5cc42b1b13e4f2', '6555cbe94b13023f934a2dd0', '651dccfea662d76276b7c88c', '651dd107a662d76276b7d3cf', '6555cc554b13023f934a7c5e', '6555c8564b13023f93492606', '651e0277a662d76276b8524d', '6555c7fb4b13023f9348f61d', '6555cafe4b13023f93498fb2', '6555c8184b13023f934908b2', '651dffeea662d76276b84bbc', '653168e11e5cc42b1b13bfb2', '6555c4504b13023f9348d80a', '6555c8014b13023f9348f975', '6555c81b4b13023f93490a7a', '653168721e5cc42b1b13ac3b', '6555c0674b13023f9348cb27', '6555b4b54b13023f9348b75e', '653169031e5cc42b1b13c919', '65316f071e5cc42b1b13eccb', '65316f6d1e5cc42b1b13ff47', '6555ccc44b13023f934acd61', '651e0c73a662d76276b86a15', '651dd71da662d76276b7e3c2', '6555cb9f4b13023f9349fa71', '6555cbe74b13023f934a2c6c', '651ddbada662d76276b7ef88', '6555cba44b13023f9349fdd1', '653168901e5cc42b1b13afe1', '6531690b1e5cc42b1b13cb89', '6555cab84b13023f93495f7c', '651df5b2a662d76276b833f3', '6555c7ec4b13023f9348ee4a', '654cbbcadc4fa72a6c403e10', '6555bf514b13023f9348c6ee', '652fc9321e5cc42b1b13a04d', '6555cb4f4b13023f9349c363', '6555c0084b13023f9348c97b', '6555c82b4b13023f9349150e', '6555ccd14b13023f934ad8ab', '651df9b1a662d76276b83c5d', '653169611e5cc42b1b13d9b2', '6555cbf64b13023f934a3718', '65316f1c1e5cc42b1b13efc1', '651e088ba662d76276b86119', '653168bd1e5cc42b1b13b78d', '655c5595eee55a44e0ac1e51', '6555c3af4b13023f9348d540', '6555c7e94b13023f9348ed19', '652ebb8c1e5cc42b1b139900', '6555c84a4b13023f934924ce', '651dca7ca662d76276b7c1c9', '651de65da662d76276b80c15', '652fcb701e5cc42b1b13a173', '6531687e1e5cc42b1b13ad8c', '651df878a662d76276b83979', '6555c80e4b13023f934901a6'])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4170a1f6-6876-44e0-a49b-efaadbc5f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [train_set[k]['unified_response']['article_sentiment'] for k in train_set.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "591cf938-1be2-45b8-b530-ce820b5189e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NA', 'bear', 'bull'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38740626-8aaa-420f-b334-d62b552f7bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Hermes 2.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fa47ec6c-4363-45e2-a457-3436b23edb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 32000]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<|im_end|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa78b2c5-65fe-4f1c-ad68-709d5eed2156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|> system\\nYou are Hermes 2.<|im_end|> \\n<|im_start|> user\\nHello, who are you?<|im_end|> \\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.apply_chat_template(messages, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f82069a-65ac-4ec0-8397-bffbdb750c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(train_row):\n",
    "    instruction = f\"<|im_start|>system\\n{new_system_prompt}<|im_end|>\\n\"\n",
    "    # not adding context as instruction ends with |actual_article|\n",
    "    context = f\"### <|im_start|>user\\n{train_row['content']}|im_end|>\\n\"\n",
    "    response = f\"### <|im_start|>assistant\\n{train_row['response']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    prompt = re.sub(r'\\n+','\\n',prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957d6c8-05b4-4581-82c8-bb073b9b59cc",
   "metadata": {},
   "source": [
    "### Article Truncation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9bd04673-53bb-4759-af8f-5188844be617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_tokens(text, encoder):  # Placeholder for your actual token calculation function\n",
    "    # Your implementation will go here.\n",
    "    return len(encoder.encode(text))  # Example: counting characters as tokens\n",
    "\n",
    "def truncate_text_to_token_limit(text,encoder, token_limit):\n",
    "    # First, check if the whole text is under the token limit\n",
    "    if calculate_tokens(text, encoder) <= token_limit:\n",
    "        return text  # The entire text is within the limit\n",
    "\n",
    "    def is_under_limit(index):\n",
    "        # Use the provided function to calculate tokens for the substring\n",
    "        return calculate_tokens(text[:index], encoder) <= token_limit\n",
    "\n",
    "    left, right = 0, len(text)\n",
    "    valid_limit = 0  # This will hold the index of the last valid token position\n",
    "\n",
    "    # Binary search to find the token limit\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2  # Find the midpoint\n",
    "        if is_under_limit(mid):\n",
    "            # If the midpoint is under the limit, store it as a valid limit\n",
    "            valid_limit = mid\n",
    "            left = mid + 1  # Move the left boundary to the right\n",
    "        else:\n",
    "            right = mid - 1  # Move the right boundary to the left\n",
    "\n",
    "    # Find the last space before the valid_limit to ensure we're at a word boundary\n",
    "    space_index = text.rfind(' ', 0, valid_limit)\n",
    "    if space_index == -1:\n",
    "        # If there's no space, we've hit the start of the text\n",
    "        return text[:valid_limit]  # Return up to the valid limit even if mid-word\n",
    "\n",
    "    # Return the text up to the last word within the token limit\n",
    "    return text[:space_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d85f6-f780-4ebe-91f1-2b6068561d4a",
   "metadata": {},
   "source": [
    "### Article Size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c46f5d5-fc39-4c48-a913-17f812c060b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_sizes = {k: len(tokenizer.encode(train_set[k]['content'])) for k in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1cb55561-2e1b-4fe4-a74f-d448a5519efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'651de33fa662d76276b803d9': 110,\n",
       " '6555c2124b13023f9348d13c': 710,\n",
       " '65367ff01e5cc42b1b143d0d': 765,\n",
       " '651e1ddca662d76276b892c7': 688,\n",
       " '652ebbb81e5cc42b1b1399e8': 874,\n",
       " '6555b49f4b13023f9348b6d6': 1053,\n",
       " '65316f831e5cc42b1b1404ea': 2289,\n",
       " '65309c0a1e5cc42b1b13a6b6': 799,\n",
       " '651e17dfa662d76276b884de': 376,\n",
       " '651e2032a662d76276b8985d': 1143,\n",
       " '6555c1124b13023f9348ce01': 191,\n",
       " '6556d9654b13023f934aee38': 717,\n",
       " '6540c74a2936d70acf71e4e7': 2195,\n",
       " '651de21ea662d76276b800b4': 320,\n",
       " '6536800e1e5cc42b1b143f17': 1380,\n",
       " '655b3e4c4b13023f934af3c3': 642,\n",
       " '651de220a662d76276b800b9': 361,\n",
       " '652ebbdc1e5cc42b1b139b7a': 3510,\n",
       " '65316fb61e5cc42b1b14140a': 511,\n",
       " '6555c8154b13023f9349066e': 1538,\n",
       " '6555c99c4b13023f93492ae9': 143,\n",
       " '6555cb5b4b13023f9349cbff': 1726,\n",
       " '6555c7df4b13023f9348e8ab': 204,\n",
       " '6555c4794b13023f9348d8af': 503,\n",
       " '6555c80e4b13023f934901f0': 1898,\n",
       " '652ebba21e5cc42b1b139964': 438,\n",
       " '6555c37f4b13023f9348d497': 395,\n",
       " '654856d2dc4fa72a6c403a46': 214,\n",
       " '6555bfd74b13023f9348c8a6': 862,\n",
       " '6555c0614b13023f9348cb14': 186,\n",
       " '6555c7c64b13023f9348e0e6': 1264,\n",
       " '6555ca964b13023f93494a79': 810,\n",
       " '653168c31e5cc42b1b13b8a4': 750,\n",
       " '6555c8214b13023f93490e78': 660,\n",
       " '6555c4754b13023f9348d89a': 747,\n",
       " '651e19e6a662d76276b88985': 690,\n",
       " '6555bfee4b13023f9348c912': 1255,\n",
       " '6555cc104b13023f934a4915': 1579,\n",
       " '651e0e02a662d76276b86db8': 515,\n",
       " '653170021e5cc42b1b14318b': 233,\n",
       " '653169231e5cc42b1b13d290': 467,\n",
       " '6555cbe94b13023f934a2dbd': 556,\n",
       " '6555b54e4b13023f9348ba8b': 3669,\n",
       " '6555c8254b13023f93491114': 993,\n",
       " '653169361e5cc42b1b13d66e': 1028,\n",
       " '65316fc71e5cc42b1b141a0f': 639,\n",
       " '6555cc984b13023f934aaa72': 535,\n",
       " '65367ffa1e5cc42b1b143d9d': 1273,\n",
       " '6555c8464b13023f93492443': 811,\n",
       " '65316ff01e5cc42b1b142a4f': 641,\n",
       " '6555b5994b13023f9348bd7b': 504,\n",
       " '6555ccbc4b13023f934ac6de': 384,\n",
       " '6555cc924b13023f934aa5a9': 1143,\n",
       " '651e0fcda662d76276b871f0': 707,\n",
       " '651df7f2a662d76276b838d8': 245,\n",
       " '653168921e5cc42b1b13b03d': 1254,\n",
       " '6555b4ae4b13023f9348b72c': 2467,\n",
       " '65316eed1e5cc42b1b13e9d8': 1351,\n",
       " '6555cbe14b13023f934a2822': 528,\n",
       " '65316aeb1e5cc42b1b13dd9d': 521,\n",
       " '6555cc2d4b13023f934a5ed8': 864,\n",
       " '6555c80a4b13023f9348ff6a': 695,\n",
       " '653170011e5cc42b1b143137': 632,\n",
       " '651e1d27a662d76276b8910d': 236,\n",
       " '6555c7e24b13023f9348ea37': 1144,\n",
       " '651e21cda662d76276b89c2f': 227,\n",
       " '6555c8224b13023f93490f22': 2089,\n",
       " '65316f681e5cc42b1b13fe29': 417,\n",
       " '651dd871a662d76276b7e732': 805,\n",
       " '652df9085669b40a3b5ab6f8': 225,\n",
       " '6555ccbb4b13023f934ac5c3': 494,\n",
       " '6555c8134b13023f9349056d': 570,\n",
       " '6555cc014b13023f934a3ea6': 830,\n",
       " '652fc7f51e5cc42b1b139fb6': 2084,\n",
       " '651dc5bba662d76276b7b4cf': 578,\n",
       " '6531690f1e5cc42b1b13ccc9': 680,\n",
       " '651df501a662d76276b83248': 603,\n",
       " '6555cc8e4b13023f934aa3c3': 720,\n",
       " '652f4a881e5cc42b1b139dcd': 232,\n",
       " '651e1561a662d76276b87eef': 273,\n",
       " '651dfc6ca662d76276b84305': 581,\n",
       " '65316fa91e5cc42b1b140fb2': 698,\n",
       " '653169141e5cc42b1b13ce18': 385,\n",
       " '651df185a662d76276b82936': 453,\n",
       " '6555c06a4b13023f9348cb2f': 786,\n",
       " '6555c2be4b13023f9348d2cc': 1006,\n",
       " '651e1599a662d76276b87f76': 421,\n",
       " '652fcc771e5cc42b1b13a1fd': 2837,\n",
       " '653169371e5cc42b1b13d68f': 553,\n",
       " '6555cbff4b13023f934a3d7e': 465,\n",
       " '6555cc584b13023f934a7e0d': 1541,\n",
       " '651dcb91a662d76276b7c4b0': 533,\n",
       " '651e06f7a662d76276b85d73': 1285,\n",
       " '653168b91e5cc42b1b13b6bc': 552,\n",
       " '651dc50fa662d76276b7b339': 363,\n",
       " '6555c8364b13023f93491d1c': 721,\n",
       " '6555c8c24b13023f9349274d': 874,\n",
       " '6555c0dd4b13023f9348cd1b': 1304,\n",
       " '6555cbd94b13023f934a223b': 503,\n",
       " '6555ccdb4b13023f934ae0f4': 2209,\n",
       " '651e0d4ba662d76276b86c0e': 515,\n",
       " '6555cc884b13023f934a9fbc': 1173,\n",
       " '6555cb184b13023f9349a019': 1051,\n",
       " '6531691b1e5cc42b1b13d071': 994,\n",
       " '6536802e1e5cc42b1b144217': 1557,\n",
       " '651dc7b3a662d76276b7ba22': 973,\n",
       " '6555ca784b13023f93493d10': 1531,\n",
       " '6555bff14b13023f9348c91e': 142,\n",
       " '6555cbf34b13023f934a34e4': 588,\n",
       " '65316fad1e5cc42b1b1410ee': 558,\n",
       " '651dfe34a662d76276b84769': 748,\n",
       " '6541bf4cdc4fa72a6c4036a8': 232,\n",
       " '651e0f9ca662d76276b87181': 572,\n",
       " '6555ca854b13023f934941db': 1667,\n",
       " '65316f001e5cc42b1b13ebec': 691,\n",
       " '651dcf2ca662d76276b7ce9c': 517,\n",
       " '65316f4d1e5cc42b1b13f85f': 927,\n",
       " '6555cc264b13023f934a59be': 337,\n",
       " '6555ca734b13023f93493b70': 1987,\n",
       " '6555cccf4b13023f934ad76a': 855,\n",
       " '65316ef81e5cc42b1b13eafb': 739,\n",
       " '6555c3f84b13023f9348d667': 198,\n",
       " '65316e9b1e5cc42b1b13e5b7': 13,\n",
       " '65316ee91e5cc42b1b13e96d': 19,\n",
       " '651e1613a662d76276b88093': 1038,\n",
       " '6555c1254b13023f9348ce50': 888,\n",
       " '651dc7f2a662d76276b7bad9': 402,\n",
       " '6555cb764b13023f9349ddfb': 238,\n",
       " '6531693d1e5cc42b1b13d750': 447,\n",
       " '6555cbe04b13023f934a26d3': 1430,\n",
       " '651dcd13a662d76276b7c8c4': 1226,\n",
       " '653168f11e5cc42b1b13c419': 533,\n",
       " '651dfbe4a662d76276b841ae': 493,\n",
       " '653680281e5cc42b1b14416d': 1169,\n",
       " '651e12e6a662d76276b8793f': 1059,\n",
       " '651de4d4a662d76276b8080e': 466,\n",
       " '651dd8ada662d76276b7e7cb': 581,\n",
       " '6555c30f4b13023f9348d37a': 618,\n",
       " '651de55ba662d76276b80963': 1296,\n",
       " '651e00b6a662d76276b84db7': 622,\n",
       " '6555cbf04b13023f934a3294': 990,\n",
       " '6555ccbd4b13023f934ac7dc': 327,\n",
       " '651df829a662d76276b83915': 843,\n",
       " '652fca671e5cc42b1b13a0eb': 1964,\n",
       " '651dfbada662d76276b84124': 554,\n",
       " '65316f8e1e5cc42b1b1407c5': 516,\n",
       " '6555b4934b13023f9348b69d': 2610,\n",
       " '6555cb534b13023f9349c650': 2113,\n",
       " '65367b381e5cc42b1b143bb5': 659,\n",
       " '651e1bf7a662d76276b88e2f': 447,\n",
       " '6555ca474b13023f9349314d': 1068,\n",
       " '6555c7e14b13023f9348e982': 1168,\n",
       " '651de570a662d76276b80999': 259,\n",
       " '6555cb684b13023f9349d4db': 705,\n",
       " '6555cbe54b13023f934a2aac': 436,\n",
       " '6555cc554b13023f934a7c73': 475,\n",
       " '651e0b8ca662d76276b86806': 591,\n",
       " '6555cc184b13023f934a4f41': 895,\n",
       " '6555cbed4b13023f934a301d': 1106,\n",
       " '6555c7f54b13023f9348f2fb': 1647,\n",
       " '651dd7cda662d76276b7e580': 1593,\n",
       " '651df639a662d76276b83526': 303,\n",
       " '651dc812a662d76276b7bb2d': 525,\n",
       " '6555b53e4b13023f9348ba18': 2563,\n",
       " '652f4a721e5cc42b1b139cfc': 1146,\n",
       " '651dd0cfa662d76276b7d331': 383,\n",
       " '6549a84fdc4fa72a6c403b4e': 232,\n",
       " '651e1ca1a662d76276b88fc6': 1245,\n",
       " '65316a481e5cc42b1b13dbcd': 505,\n",
       " '65316d0d1e5cc42b1b13e1b3': 283,\n",
       " '652ebbba1e5cc42b1b1399fc': 849,\n",
       " '651e185ba662d76276b88604': 744,\n",
       " '653680241e5cc42b1b144108': 1113,\n",
       " '65316f601e5cc42b1b13fc54': 495,\n",
       " '651e0721a662d76276b85dde': 590,\n",
       " '651dee25a662d76276b82078': 794,\n",
       " '651dc6e2a662d76276b7b7bd': 981,\n",
       " '6555cb4f4b13023f9349c3c3': 728,\n",
       " '6555c7fc4b13023f9348f6b7': 1208,\n",
       " '651e112da662d76276b87532': 823,\n",
       " '6555ca7b4b13023f93493dc5': 476,\n",
       " '651de6c8a662d76276b80d3b': 500,\n",
       " '65367ff01e5cc42b1b143d10': 1351,\n",
       " '6555ca9a4b13023f93494c6a': 2109,\n",
       " '6555c1434b13023f9348cedb': 972,\n",
       " '651dc9f6a662d76276b7c05e': 957,\n",
       " '6555c7fb4b13023f9348f645': 1751,\n",
       " '6555c4644b13023f9348d857': 794,\n",
       " '6555c1844b13023f9348cfdd': 666,\n",
       " '6555c7f04b13023f9348f09a': 371,\n",
       " '65316fac1e5cc42b1b1410ad': 626,\n",
       " '651e17ada662d76276b88469': 747,\n",
       " '653168bd1e5cc42b1b13b770': 1074,\n",
       " '65316eb81e5cc42b1b13e666': 895,\n",
       " '651de443a662d76276b80699': 2994,\n",
       " '6531692d1e5cc42b1b13d4e2': 794,\n",
       " '653168bd1e5cc42b1b13b75d': 489,\n",
       " '653169551e5cc42b1b13d960': 428,\n",
       " '651deff0a662d76276b8250c': 527,\n",
       " '6555c8204b13023f93490d78': 526,\n",
       " '655c57e5eee55a44e0ac1e7a': 339,\n",
       " '6555b5724b13023f9348bbe5': 595,\n",
       " '651df8f2a662d76276b83a8a': 398,\n",
       " '652ebbd61e5cc42b1b139b23': 852,\n",
       " '653168cd1e5cc42b1b13baec': 501,\n",
       " '651e147ba662d76276b87cd1': 287,\n",
       " '6555cb954b13023f9349f3e2': 4101,\n",
       " '6555c83d4b13023f934921c7': 1357,\n",
       " '6555c0b84b13023f9348cc83': 426,\n",
       " '6555c1024b13023f9348cdbb': 841,\n",
       " '6555c25c4b13023f9348d202': 1211,\n",
       " '651dcedea662d76276b7cdc3': 314,\n",
       " '6555c28a4b13023f9348d26d': 537,\n",
       " '6555c8384b13023f93491e21': 3147,\n",
       " '651df964a662d76276b83b9f': 275,\n",
       " '6555cc5b4b13023f934a7ffa': 820,\n",
       " '6555c7cd4b13023f9348e2ea': 980,\n",
       " '6555ca714b13023f93493a98': 765,\n",
       " '652ebbc91e5cc42b1b139a8a': 570,\n",
       " '651e2462a662d76276b8a25f': 653,\n",
       " '6555cc3a4b13023f934a6864': 864,\n",
       " '6555ccc14b13023f934acb0b': 2107,\n",
       " '6555cc034b13023f934a3fac': 523,\n",
       " '651dc6d3a662d76276b7b795': 248,\n",
       " '6531692a1e5cc42b1b13d461': 409,\n",
       " '653168ca1e5cc42b1b13ba13': 453,\n",
       " '652ebbca1e5cc42b1b139a91': 857,\n",
       " '6555c83f4b13023f934922b5': 710,\n",
       " '6555c0844b13023f9348cba2': 625,\n",
       " '652ebbc31e5cc42b1b139a4d': 650,\n",
       " '651e1647a662d76276b88118': 354,\n",
       " '6555cc184b13023f934a4ed8': 309,\n",
       " '6555b5a44b13023f9348bdef': 5222,\n",
       " '651e0398a662d76276b85525': 790,\n",
       " '6555b58b4b13023f9348bcee': 563,\n",
       " '651e2030a662d76276b89859': 358,\n",
       " '6531683c1e5cc42b1b13a842': 824,\n",
       " '6555c7e34b13023f9348ea9d': 2417,\n",
       " '6555c30c4b13023f9348d369': 1646,\n",
       " '65367ff11e5cc42b1b143d28': 1246,\n",
       " '6555cb944b13023f9349f33b': 1365,\n",
       " '6555c7ab4b13023f9348dd7c': 1315,\n",
       " '65316f061e5cc42b1b13ec8e': 241,\n",
       " '6555c8374b13023f93491df6': 1085,\n",
       " '652fcea91e5cc42b1b13a31f': 2407,\n",
       " '651e111ea662d76276b8750a': 669,\n",
       " '6555cc854b13023f934a9d92': 1514,\n",
       " '651de594a662d76276b809f5': 539,\n",
       " '651dfaa1a662d76276b83e94': 357,\n",
       " '6555ccc04b13023f934aca23': 655,\n",
       " '651dd7d7a662d76276b7e59d': 433,\n",
       " '6555cc7c4b13023f934a9792': 502,\n",
       " '6544625adc4fa72a6c403947': 245,\n",
       " '6555c3724b13023f9348d46e': 415,\n",
       " '6555cc624b13023f934a8562': 691,\n",
       " '654c4b50dc4fa72a6c403d42': 237,\n",
       " '6555c7ea4b13023f9348edc7': 823,\n",
       " '6555cbe54b13023f934a2a75': 896,\n",
       " '6555cb9b4b13023f9349f7bd': 582,\n",
       " '6555ca934b13023f93494878': 1298,\n",
       " '6555b4d34b13023f9348b7f3': 801,\n",
       " '6555c4754b13023f9348d89f': 457,\n",
       " '652fc9371e5cc42b1b13a04f': 2486,\n",
       " '6555cb084b13023f9349961c': 1285,\n",
       " '652fc7861e5cc42b1b139f81': 1900,\n",
       " '6555cc244b13023f934a57d2': 567,\n",
       " '6555cad94b13023f93497656': 1363,\n",
       " '651df88ba662d76276b839a6': 662,\n",
       " '6555cc634b13023f934a8682': 1061,\n",
       " '6555ca954b13023f934949da': 1118,\n",
       " '6555cac24b13023f934965ee': 618,\n",
       " '651deae5a662d76276b817f6': 831,\n",
       " '6555cae54b13023f93497ec1': 1262,\n",
       " '6555cbed4b13023f934a30da': 521,\n",
       " '6555caf44b13023f934988a9': 1040,\n",
       " '651dee4ca662d76276b820e4': 583,\n",
       " '6555c80f4b13023f93490257': 2799,\n",
       " '6525a73a598c6618f0608911': 498,\n",
       " '6555cc184b13023f934a4f2e': 1962,\n",
       " '6555cb4b4b13023f9349c15e': 1584,\n",
       " '651dd8f7a662d76276b7e883': 384,\n",
       " '651e0385a662d76276b854fb': 637,\n",
       " '653168881e5cc42b1b13aeda': 386,\n",
       " '6555c7f44b13023f9348f266': 528,\n",
       " '653170041e5cc42b1b143263': 520,\n",
       " '653680061e5cc42b1b143e6e': 2261,\n",
       " '6555c7f64b13023f9348f36e': 598,\n",
       " '652e31045669b40a3b5ab745': 574,\n",
       " '653168ae1e5cc42b1b13b49e': 653,\n",
       " '6555cac94b13023f93496aa5': 814,\n",
       " '651dda13a662d76276b7eb67': 517,\n",
       " '65316f101e5cc42b1b13edf7': 553,\n",
       " '65316f9d1e5cc42b1b140bff': 992,\n",
       " '651dd314a662d76276b7d9e4': 534,\n",
       " '652e310f5669b40a3b5ab74e': 670,\n",
       " '6555c82a4b13023f93491441': 1780,\n",
       " '651e1baea662d76276b88d8b': 1507,\n",
       " '6555c8034b13023f9348fb23': 1497,\n",
       " '651df6aca662d76276b83630': 617,\n",
       " '6555c8034b13023f9348fafa': 2185,\n",
       " '6555c0e04b13023f9348cd29': 1138,\n",
       " '65446255dc4fa72a6c403915': 227,\n",
       " '6555cc074b13023f934a4299': 429,\n",
       " '65316f8a1e5cc42b1b1406af': 680,\n",
       " '6555c7f64b13023f9348f37c': 898,\n",
       " '6531693c1e5cc42b1b13d73d': 426,\n",
       " '653169141e5cc42b1b13ce58': 514,\n",
       " '6555c9fb4b13023f93492d9f': 1097,\n",
       " '6555caad4b13023f93495801': 661,\n",
       " '65316f0d1e5cc42b1b13ed87': 1150,\n",
       " '651df296a662d76276b82c25': 1436,\n",
       " '6555c11c4b13023f9348ce2e': 758,\n",
       " '651e0b60a662d76276b8679f': 693,\n",
       " '6555cb7c4b13023f9349e220': 1459,\n",
       " '6555b5984b13023f9348bd6a': 303,\n",
       " '651df28ea662d76276b82c10': 307,\n",
       " '6555bfa04b13023f9348c7bc': 2590,\n",
       " '6555c8644b13023f934926fa': 2313,\n",
       " '653168aa1e5cc42b1b13b40a': 592,\n",
       " '651df191a662d76276b82957': 320,\n",
       " '65316ddc1e5cc42b1b13e3b7': 1212,\n",
       " '65316ce01e5cc42b1b13e151': 262,\n",
       " '653168871e5cc42b1b13aebf': 577,\n",
       " '6555c5194b13023f9348db5f': 929,\n",
       " '6555c4364b13023f9348d798': 461,\n",
       " '6555ca634b13023f934936b0': 1626,\n",
       " '65316f6f1e5cc42b1b13ffcb': 558,\n",
       " '651ddaa0a662d76276b7ecce': 593,\n",
       " '651dfa3ea662d76276b83da2': 839,\n",
       " '6555cc9a4b13023f934aabcd': 422,\n",
       " '65316f901e5cc42b1b14086a': 1871,\n",
       " '6555ccc24b13023f934acbd9': 550,\n",
       " '6555c8174b13023f934907ac': 523,\n",
       " '65316f951e5cc42b1b1409a6': 377,\n",
       " '6555c7ee4b13023f9348ef9a': 761,\n",
       " '6555cc874b13023f934a9ed1': 843,\n",
       " '651ddb05a662d76276b7eddc': 243,\n",
       " '6555cbcd4b13023f934a19ec': 387,\n",
       " '6555c4d94b13023f9348da48': 254,\n",
       " '6555cc214b13023f934a556a': 590,\n",
       " '653168c61e5cc42b1b13b937': 496,\n",
       " '653168731e5cc42b1b13ac4b': 537,\n",
       " '6555ca8d4b13023f934945bb': 1334,\n",
       " '65367fef1e5cc42b1b143d09': 670,\n",
       " '65316ff41e5cc42b1b142be3': 494,\n",
       " '6555c0bd4b13023f9348cc94': 474,\n",
       " '653168e71e5cc42b1b13c156': 535,\n",
       " '651dff56a662d76276b84a31': 224,\n",
       " '6555ccd14b13023f934ad899': 372,\n",
       " '653168cc1e5cc42b1b13baa5': 959,\n",
       " '6555c8114b13023f9349040a': 1100,\n",
       " '6555cc874b13023f934a9f50': 707,\n",
       " '653169041e5cc42b1b13c986': 824,\n",
       " '6555b5c14b13023f9348bf5c': 2071,\n",
       " '6555c7fd4b13023f9348f71f': 1062,\n",
       " '6536801b1e5cc42b1b144026': 539,\n",
       " '6555c83b4b13023f9349209f': 756,\n",
       " '6555c9804b13023f93492a89': 787,\n",
       " '6555c9b74b13023f93492b57': 1425,\n",
       " '6555c8024b13023f9348fa66': 589,\n",
       " '6555cad34b13023f93497200': 888,\n",
       " '6531ed6e1e5cc42b1b1437b6': 262,\n",
       " '651e1afaa662d76276b88bf7': 376,\n",
       " '651de9c1a662d76276b814e5': 399,\n",
       " '6555c8464b13023f93492431': 800,\n",
       " '651e10f6a662d76276b874b0': 496,\n",
       " '65316f101e5cc42b1b13ee06': 277,\n",
       " '651df8a6a662d76276b839e0': 998,\n",
       " '653168b31e5cc42b1b13b5a4': 567,\n",
       " '6555cb194b13023f9349a109': 1329,\n",
       " '651dd065a662d76276b7d207': 613,\n",
       " '651e0983a662d76276b8634c': 520,\n",
       " '653168a21e5cc42b1b13b2c3': 1643,\n",
       " '6555cba14b13023f9349fb8b': 2555,\n",
       " '652fcd741e5cc42b1b13a27e': 1749,\n",
       " '6555c7c64b13023f9348e0fd': 378,\n",
       " '651dd8f3a662d76276b7e87b': 642,\n",
       " '654f26894b13023f9348b31b': 786,\n",
       " '6555cc884b13023f934a9fd8': 500,\n",
       " '651e0dfda662d76276b86da8': 235,\n",
       " '6555c8114b13023f93490380': 221,\n",
       " '6531700d1e5cc42b1b1435e3': 621,\n",
       " '6555cc0c4b13023f934a466a': 615,\n",
       " '6555cb564b13023f9349c8a2': 1044,\n",
       " '6555b4dd4b13023f9348b82e': 2586,\n",
       " '651e0275a662d76276b85246': 294,\n",
       " '6555c0114b13023f9348c9b0': 896,\n",
       " '6555c2434b13023f9348d1c0': 348,\n",
       " '65316fd41e5cc42b1b141f1f': 562,\n",
       " '6540c8aa2936d70acf71ed8f': 693,\n",
       " '653168ba1e5cc42b1b13b6d1': 807,\n",
       " '6555cb7c4b13023f9349e219': 910,\n",
       " '6555caac4b13023f9349574b': 95,\n",
       " '6555ccd14b13023f934ad8db': 434,\n",
       " '6555c0f64b13023f9348cd83': 1150,\n",
       " '653168981e5cc42b1b13b120': 1194,\n",
       " '6555cb5e4b13023f9349cde7': 1709,\n",
       " '651e09b4a662d76276b863b3': 414,\n",
       " '6555cab34b13023f93495ba6': 1109,\n",
       " '651e0cc2a662d76276b86acb': 435,\n",
       " '651dd1e8a662d76276b7d66d': 609,\n",
       " '6555cc514b13023f934a78fb': 2026,\n",
       " '6531691a1e5cc42b1b13cfe5': 732,\n",
       " '6555cbe14b13023f934a2825': 1021,\n",
       " '653169241e5cc42b1b13d2e5': 764,\n",
       " '651e2252a662d76276b89d66': 850,\n",
       " '6555cb764b13023f9349de07': 1219,\n",
       " '652ebb9c1e5cc42b1b13993e': 714,\n",
       " '6555cc4d4b13023f934a76ac': 979,\n",
       " '6555cc5e4b13023f934a8260': 142,\n",
       " '652fcacd1e5cc42b1b13a11f': 1810,\n",
       " '6531692b1e5cc42b1b13d497': 844,\n",
       " '6555caf54b13023f934989b1': 1020,\n",
       " '6555c7d54b13023f9348e51b': 317,\n",
       " '651dd35ca662d76276b7daaa': 555,\n",
       " '653169c01e5cc42b1b13da51': 827,\n",
       " '6555ccc14b13023f934acb3b': 474,\n",
       " '653168ff1e5cc42b1b13c81f': 674,\n",
       " '6531693b1e5cc42b1b13d714': 491,\n",
       " '651e0b4da662d76276b86775': 448,\n",
       " '6555ccb14b13023f934abd41': 504,\n",
       " '6555bf834b13023f9348c76b': 319,\n",
       " '65367fd21e5cc42b1b143bf7': 1180,\n",
       " '6555cc5f4b13023f934a82e0': 865,\n",
       " '651e0764a662d76276b85e7e': 253,\n",
       " '6531683b1e5cc42b1b13a832': 435,\n",
       " '651e1997a662d76276b888d5': 550,\n",
       " '6555cb674b13023f9349d3cb': 2186,\n",
       " '6555c82e4b13023f934917b0': 941,\n",
       " '6555bebd4b13023f9348c514': 276,\n",
       " '651e08bba662d76276b8618d': 777,\n",
       " '655d6eeee1d8342798a44987': 974,\n",
       " '651df8cca662d76276b83a31': 348,\n",
       " '6555cbe74b13023f934a2c0b': 355,\n",
       " '651dc6e2a662d76276b7b7be': 765,\n",
       " '6555cca54b13023f934ab3d7': 1216,\n",
       " '6555c7e74b13023f9348ec0d': 2598,\n",
       " '65316f201e5cc42b1b13f051': 292,\n",
       " '653170071e5cc42b1b1433da': 1810,\n",
       " '651e1be2a662d76276b88e01': 411,\n",
       " '6555cc6f4b13023f934a8e9d': 1207,\n",
       " '6541bf4bdc4fa72a6c40368b': 236,\n",
       " '651e1c01a662d76276b88e44': 460,\n",
       " '6555c81c4b13023f93490b2e': 970,\n",
       " '651e20d4a662d76276b899e5': 421,\n",
       " '65316ef71e5cc42b1b13ead8': 295,\n",
       " '6555b5d84b13023f9348c27b': 921,\n",
       " '65316ffb1e5cc42b1b142e7d': 808,\n",
       " '651dc7a8a662d76276b7ba00': 684,\n",
       " '655d6f15e1d8342798a44a14': 395,\n",
       " '652fd16f1e5cc42b1b13a435': 1507,\n",
       " '6555c8134b13023f93490564': 614,\n",
       " '651dd25aa662d76276b7d7c4': 441,\n",
       " '651e062da662d76276b85b8b': 542,\n",
       " '6555cb644b13023f9349d1b0': 1273,\n",
       " '651dcbf0a662d76276b7c5b8': 929,\n",
       " '6555c5234b13023f9348db8a': 1988,\n",
       " '653680071e5cc42b1b143e7d': 1952,\n",
       " '6555ca3e4b13023f93493050': 943,\n",
       " '651dc9eda662d76276b7c045': 368,\n",
       " '6555c52a4b13023f9348dbb6': 365,\n",
       " '652ebb821e5cc42b1b1398dd': 797,\n",
       " '6555c4b54b13023f9348d9a9': 267,\n",
       " '6555c80b4b13023f9348ff98': 1084,\n",
       " '652fd0741e5cc42b1b13a3b8': 1685,\n",
       " '653680291e5cc42b1b144198': 1002,\n",
       " '6555cb5f4b13023f9349ce2e': 546,\n",
       " '651dee60a662d76276b8211c': 875,\n",
       " '65316fb91e5cc42b1b141517': 620,\n",
       " '6555cbf54b13023f934a35db': 865,\n",
       " '652fcc921e5cc42b1b13a20a': 2121,\n",
       " '652fc8dd1e5cc42b1b13a026': 2127,\n",
       " '6555cb1a4b13023f9349a148': 1707,\n",
       " '651e1114a662d76276b874f3': 747,\n",
       " '6555cade4b13023f934979b2': 927,\n",
       " '6555bfc64b13023f9348c855': 578,\n",
       " '6555cc604b13023f934a83c7': 686,\n",
       " '65367ffc1e5cc42b1b143dbf': 1607,\n",
       " '652ebbce1e5cc42b1b139abb': 576,\n",
       " '6555ca704b13023f93493a48': 498,\n",
       " '653169461e5cc42b1b13d850': 792,\n",
       " '6555c82c4b13023f9349161a': 1315,\n",
       " '652ebbcd1e5cc42b1b139ab2': 584,\n",
       " '651e2097a662d76276b89954': 204,\n",
       " '6540c8a82936d70acf71ed1a': 346,\n",
       " '6555c8264b13023f9349113e': 558,\n",
       " '65316f761e5cc42b1b140179': 2239,\n",
       " '6555cc194b13023f934a4f9c': 633,\n",
       " '652ebbc61e5cc42b1b139a63': 4851,\n",
       " '6555cb304b13023f9349af9c': 768,\n",
       " '65316eb91e5cc42b1b13e671': 563,\n",
       " '653680301e5cc42b1b14425a': 2101,\n",
       " '651dde6ba662d76276b7f6c4': 558,\n",
       " '651e2190a662d76276b89ba6': 743,\n",
       " '65316eaf1e5cc42b1b13e624': 905,\n",
       " '651dc7ada662d76276b7ba13': 623,\n",
       " '651dfea7a662d76276b8488a': 404,\n",
       " '652fc72d1e5cc42b1b139f56': 2383,\n",
       " '6555cc544b13023f934a7b67': 1022,\n",
       " '6555c8224b13023f93490eff': 968,\n",
       " '653168c31e5cc42b1b13b889': 1053,\n",
       " '65316e5c1e5cc42b1b13e4db': 573,\n",
       " '6555cc324b13023f934a6257': 675,\n",
       " '6555c8264b13023f934911c0': 972,\n",
       " '6555cbf94b13023f934a38da': 788,\n",
       " '6555cc694b13023f934a8ac7': 682,\n",
       " '6555cc0f4b13023f934a4834': 1079,\n",
       " '651dcbf9a662d76276b7c5d3': 471,\n",
       " '6555cbfa4b13023f934a39e0': 403,\n",
       " '6555c90d4b13023f9349287b': 1279,\n",
       " '655d6f13e1d8342798a44a00': 516,\n",
       " '651dc474a662d76276b7b1bf': 682,\n",
       " '6555c0d34b13023f9348ccf6': 734,\n",
       " '6555cc924b13023f934aa625': 382,\n",
       " '652fd2cd1e5cc42b1b13a4e3': 1913,\n",
       " '653168c31e5cc42b1b13b8a6': 472,\n",
       " '651e0026a662d76276b84c48': 546,\n",
       " '6555c8124b13023f9349045d': 1676,\n",
       " '651e1b8ca662d76276b88d38': 787,\n",
       " '6555c7c24b13023f9348e036': 1662,\n",
       " '6555cc354b13023f934a64ff': 685,\n",
       " '6555c8334b13023f93491b17': 579,\n",
       " '6555cb004b13023f934990d6': 1012,\n",
       " '65316e651e5cc42b1b13e4ef': 593,\n",
       " '6555ccbe4b13023f934ac8a8': 515,\n",
       " '65316d661e5cc42b1b13e27e': 353,\n",
       " '651e2092a662d76276b89946': 692,\n",
       " '6555cc294b13023f934a5b6e': 515,\n",
       " '651dd464a662d76276b7dd5b': 886,\n",
       " '655191564b13023f9348b3ac': 243,\n",
       " '6555bf734b13023f9348c741': 546,\n",
       " '6555caf44b13023f93498908': 1143,\n",
       " '6555cc7e4b13023f934a990b': 404,\n",
       " '653168ec1e5cc42b1b13c2c6': 256,\n",
       " '6555c0344b13023f9348ca5c': 591,\n",
       " '6555cc304b13023f934a60f1': 495,\n",
       " '6555cbde4b13023f934a25d6': 539,\n",
       " '6555cbc54b13023f934a149b': 608,\n",
       " '6555c0384b13023f9348ca67': 519,\n",
       " '6525a728598c6618f06087ec': 398,\n",
       " '6555cb644b13023f9349d22d': 671,\n",
       " '6555c7be4b13023f9348df7c': 900,\n",
       " '65316f4b1e5cc42b1b13f7da': 2774,\n",
       " '6555cc574b13023f934a7ddf': 349,\n",
       " '6555ca924b13023f93494857': 431,\n",
       " '6555c8554b13023f934925fd': 602,\n",
       " '653170001e5cc42b1b1430c4': 1104,\n",
       " '6555cafb4b13023f93498d68': 898,\n",
       " '6555c8114b13023f93490416': 815,\n",
       " '6555caa54b13023f93495295': 839,\n",
       " '65316ebd1e5cc42b1b13e690': 597,\n",
       " '6555c7a94b13023f9348dd66': 1012,\n",
       " '6555c8174b13023f9349079b': 486,\n",
       " '653169341e5cc42b1b13d621': 736,\n",
       " '653169261e5cc42b1b13d36d': 540,\n",
       " '651ddfa3a662d76276b7fa22': 289,\n",
       " '6555c8324b13023f93491a2b': 892,\n",
       " '651e17a3a662d76276b8844e': 294,\n",
       " '6555cbe24b13023f934a28c0': 516,\n",
       " '6555b4854b13023f9348b65f': 545,\n",
       " '6555bfdc4b13023f9348c8ba': 288,\n",
       " '6555cc274b13023f934a5a79': 810,\n",
       " '651df974a662d76276b83bca': 1617,\n",
       " '6531684e1e5cc42b1b13a92c': 1992,\n",
       " '651de194a662d76276b7ff54': 364,\n",
       " '6555c2544b13023f9348d1ef': 518,\n",
       " '6555cc354b13023f934a6487': 974,\n",
       " '6555c9324b13023f9349293f': 1401,\n",
       " '6555caff4b13023f93499009': 812,\n",
       " '6555ccbf4b13023f934ac902': 947,\n",
       " '651e1990a662d76276b888c4': 906,\n",
       " '6555cc374b13023f934a65d0': 576,\n",
       " '6555cc8c4b13023f934aa231': 572,\n",
       " '653169091e5cc42b1b13caf1': 773,\n",
       " '6555c8294b13023f934913b3': 915,\n",
       " '651e167aa662d76276b8818a': 425,\n",
       " '652fd14e1e5cc42b1b13a424': 2378,\n",
       " '6555c30d4b13023f9348d36f': 1528,\n",
       " '6555cbdc4b13023f934a2474': 410,\n",
       " '65316f661e5cc42b1b13fda4': 1799,\n",
       " '651de168a662d76276b7fee7': 229,\n",
       " '653168901e5cc42b1b13aff2': 323,\n",
       " '651def1ea662d76276b82308': 421,\n",
       " '653169121e5cc42b1b13cd8d': 445,\n",
       " '65316f0c1e5cc42b1b13ed68': 922,\n",
       " '6555b5d04b13023f9348c0db': 576,\n",
       " '651e0f7da662d76276b87136': 590,\n",
       " '651e265aa662d76276b8a6fd': 698,\n",
       " '6555bec24b13023f9348c531': 388,\n",
       " '6555c8084b13023f9348fe34': 864,\n",
       " '651dd0e2a662d76276b7d368': 200,\n",
       " '653168871e5cc42b1b13aeac': 760,\n",
       " '651dd2eda662d76276b7d976': 766,\n",
       " '6555ca0d4b13023f93492e27': 718,\n",
       " '65316a751e5cc42b1b13dc68': 467,\n",
       " '651de9b2a662d76276b814be': 1053,\n",
       " '6555cad24b13023f934970ef': 3602,\n",
       " '6555c7e34b13023f9348ea69': 994,\n",
       " '6555c8044b13023f9348fb4d': 1730,\n",
       " '6555b5254b13023f9348b98d': 1885,\n",
       " '653168a11e5cc42b1b13b286': 247,\n",
       " '65316f3a1e5cc42b1b13f4b0': 473,\n",
       " '6531ed6e1e5cc42b1b1437a1': 259,\n",
       " '6555c9434b13023f9349296d': 1400,\n",
       " '651df9a7a662d76276b83c46': 641,\n",
       " '651e2107a662d76276b89a61': 613,\n",
       " '652ebbdb1e5cc42b1b139b68': 906,\n",
       " '6555c7f74b13023f9348f3e7': 1277,\n",
       " '653170071e5cc42b1b1433c6': 669,\n",
       " '65316b4f1e5cc42b1b13de6c': 870,\n",
       " '651dcf4ca662d76276b7cef6': 723,\n",
       " '6555c0a34b13023f9348cc2f': 627,\n",
       " '6555cc7d4b13023f934a987a': 714,\n",
       " '6555caa44b13023f934951f7': 2797,\n",
       " '651deb65a662d76276b81947': 389,\n",
       " '6555cc014b13023f934a3ea9': 1037,\n",
       " '651e1f8ca662d76276b896e2': 1610,\n",
       " '651df70da662d76276b83718': 704,\n",
       " '6555bf764b13023f9348c749': 442,\n",
       " '6555ccc44b13023f934acd52': 386,\n",
       " '6555cbb94b13023f934a0c96': 1416,\n",
       " '65316fae1e5cc42b1b141162': 1068,\n",
       " '65316f121e5cc42b1b13ee4f': 258,\n",
       " '65316eef1e5cc42b1b13ea12': 741,\n",
       " '65316f3d1e5cc42b1b13f521': 2154,\n",
       " '65316f5f1e5cc42b1b13fc24': 816,\n",
       " '652ebb8a1e5cc42b1b1398f5': 1344,\n",
       " '6555cb874b13023f9349ea48': 1644,\n",
       " '6555ca994b13023f93494c06': 1064,\n",
       " '651de0eda662d76276b7fda1': 695,\n",
       " '651e17b8a662d76276b88483': 353,\n",
       " '651df783a662d76276b83831': 490,\n",
       " '65310c541e5cc42b1b13a74d': 546,\n",
       " '6555cc0a4b13023f934a4484': 285,\n",
       " '653170051e5cc42b1b1432de': 2333,\n",
       " '651e1647a662d76276b88119': 284,\n",
       " '6531694f1e5cc42b1b13d90e': 457,\n",
       " '6531689a1e5cc42b1b13b163': 977,\n",
       " '651def72a662d76276b823e0': 493,\n",
       " '653168ff1e5cc42b1b13c80b': 543,\n",
       " '65316ede1e5cc42b1b13e889': 268,\n",
       " '651dc89fa662d76276b7bcba': 488,\n",
       " '6531690e1e5cc42b1b13cc6b': 505,\n",
       " '6555cbe74b13023f934a2c17': 661,\n",
       " '652fc71f1e5cc42b1b139f4f': 3710,\n",
       " '6555c8064b13023f9348fc8a': 1024,\n",
       " '6555cbe14b13023f934a283c': 384,\n",
       " '6555cc5b4b13023f934a808f': 467,\n",
       " '652ebbbd1e5cc42b1b139a14': 941,\n",
       " '653170061e5cc42b1b14331f': 552,\n",
       " '653169131e5cc42b1b13cdc7': 941,\n",
       " '651df2c1a662d76276b82c98': 607,\n",
       " '6555cc6b4b13023f934a8c50': 370,\n",
       " '6555cc884b13023f934a9fd2': 1746,\n",
       " '651ded13a662d76276b81d97': 375,\n",
       " '6555cc744b13023f934a92ac': 1623,\n",
       " '6536800a1e5cc42b1b143ebc': 1395,\n",
       " '6555c0174b13023f9348c9ca': 396,\n",
       " '6555cc614b13023f934a84d9': 1254,\n",
       " '6555c4c24b13023f9348d9e3': 825,\n",
       " '651df4a2a662d76276b83161': 601,\n",
       " '65316eca1e5cc42b1b13e723': 1094,\n",
       " '6555cc074b13023f934a4300': 548,\n",
       " '6555cc464b13023f934a7139': 716,\n",
       " '65316e651e5cc42b1b13e4f2': 728,\n",
       " '6555cbe94b13023f934a2dd0': 696,\n",
       " '651dccfea662d76276b7c88c': 644,\n",
       " '651dd107a662d76276b7d3cf': 246,\n",
       " '6555cc554b13023f934a7c5e': 767,\n",
       " '6555c8564b13023f93492606': 972,\n",
       " '651e0277a662d76276b8524d': 1523,\n",
       " '6555c7fb4b13023f9348f61d': 775,\n",
       " '6555cafe4b13023f93498fb2': 633,\n",
       " '6555c8184b13023f934908b2': 2156,\n",
       " '651dffeea662d76276b84bbc': 1156,\n",
       " '653168e11e5cc42b1b13bfb2': 709,\n",
       " '6555c4504b13023f9348d80a': 480,\n",
       " '6555c8014b13023f9348f975': 524,\n",
       " '6555c81b4b13023f93490a7a': 933,\n",
       " '653168721e5cc42b1b13ac3b': 648,\n",
       " '6555c0674b13023f9348cb27': 423,\n",
       " '6555b4b54b13023f9348b75e': 7126,\n",
       " '653169031e5cc42b1b13c919': 967,\n",
       " '65316f071e5cc42b1b13eccb': 1264,\n",
       " '65316f6d1e5cc42b1b13ff47': 509,\n",
       " '6555ccc44b13023f934acd61': 1075,\n",
       " '651e0c73a662d76276b86a15': 444,\n",
       " '651dd71da662d76276b7e3c2': 935,\n",
       " '6555cb9f4b13023f9349fa71': 1998,\n",
       " '6555cbe74b13023f934a2c6c': 395,\n",
       " '651ddbada662d76276b7ef88': 684,\n",
       " '6555cba44b13023f9349fdd1': 1048,\n",
       " '653168901e5cc42b1b13afe1': 1123,\n",
       " '6531690b1e5cc42b1b13cb89': 663,\n",
       " '6555cab84b13023f93495f7c': 1777,\n",
       " '651df5b2a662d76276b833f3': 485,\n",
       " '6555c7ec4b13023f9348ee4a': 737,\n",
       " '654cbbcadc4fa72a6c403e10': 662,\n",
       " '6555bf514b13023f9348c6ee': 406,\n",
       " '652fc9321e5cc42b1b13a04d': 1876,\n",
       " '6555cb4f4b13023f9349c363': 848,\n",
       " '6555c0084b13023f9348c97b': 496,\n",
       " '6555c82b4b13023f9349150e': 3502,\n",
       " '6555ccd14b13023f934ad8ab': 666,\n",
       " '651df9b1a662d76276b83c5d': 235,\n",
       " '653169611e5cc42b1b13d9b2': 410,\n",
       " '6555cbf64b13023f934a3718': 655,\n",
       " '65316f1c1e5cc42b1b13efc1': 982,\n",
       " '651e088ba662d76276b86119': 427,\n",
       " '653168bd1e5cc42b1b13b78d': 803,\n",
       " '655c5595eee55a44e0ac1e51': 292,\n",
       " '6555c3af4b13023f9348d540': 463,\n",
       " '6555c7e94b13023f9348ed19': 1673,\n",
       " '652ebb8c1e5cc42b1b139900': 2678,\n",
       " '6555c84a4b13023f934924ce': 2113,\n",
       " '651dca7ca662d76276b7c1c9': 416,\n",
       " '651de65da662d76276b80c15': 688,\n",
       " '652fcb701e5cc42b1b13a173': 1249,\n",
       " '6531687e1e5cc42b1b13ad8c': 795,\n",
       " '651df878a662d76276b83979': 781,\n",
       " '6555c80e4b13023f934901a6': 1263}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1348a0fe-53eb-44bb-8518-1bc5b3127214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3531"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set['6555b5a44b13023f9348bdef']['content'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41450dfc-18c7-461e-8223-af27c45c77d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_TOKEN_LIMIT = 700\n",
    "INSTRUCTION_TOKENS = len(tokenizer.encode(new_system_prompt))\n",
    "\n",
    "BUFFER_TOKENS = 21\n",
    "\n",
    "ARTICLE_TOKEN_LIMIT = 4096 - OUTPUT_TOKEN_LIMIT - INSTRUCTION_TOKENS - BUFFER_TOKENS\n",
    "\n",
    "ARTICLE_TOKEN_LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9aa6b8e5-a4d7-4138-b12f-9d1b95137b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making dataset it ready for training\n",
    "modified_train_set = {}\n",
    "for art_id in train_set:\n",
    "    modified_train_set[art_id] = {}\n",
    "    # train_set[key]['text'] = format_prompt(train_set[key])\n",
    "    # modified_train_set[art_id]['text'] = train_set[key]['text']\n",
    "    # train_set[key]['article_id'] = key\n",
    "    modified_train_set[art_id]['article_id'] = art_id\n",
    "    modified_train_set[art_id]['content'] = train_set[art_id]['content']\n",
    "    modified_train_set[art_id]['response'] = json.dumps(train_set[art_id]['unified_response'])\n",
    "    # train_set[key]['unified_response'] = json.dumps(train_set[key]['unified_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e3a324-5761-47da-a5f7-cabba097b73d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # making dataset it ready for training\n",
    "# for key in train_set:\n",
    "#     train_set[key]['text'] = format_prompt(train_set[key])\n",
    "#     train_set[key]['article_id'] = key\n",
    "#     train_set[key]['response'] = json.dumps(train_set[key]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d7b8d29-b7bc-4f86-bfd6-6b822822fe0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_dict(modified_train_set).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "705ed650-a113-4414-a7e4-b39024eb4373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67c030-895d-4c7d-bbe8-6ea32104ce76",
   "metadata": {},
   "source": [
    "### Setting up prompt in ChatML Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33af3259-0675-4233-b63c-6484ea3aa555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_text_response_as_prompt(train_row):\n",
    "    truncated_content = truncate_text_to_token_limit(text=train_row['content'], encoder=tokenizer, token_limit=ARTICLE_TOKEN_LIMIT)\n",
    "    messages = [{\"role\": \"system\", \"content\": new_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"|article_start|\\n {truncated_content}\\n|article_end|\\n\"}]\n",
    "    context_prompt = tokenizer.decode(tokenizer.apply_chat_template(messages, add_generation_prompt=False))\n",
    "    prompt = context_prompt + train_row['response']\n",
    "    prompt = re.sub(r'\\n+','\\n',prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd130f74-56ca-45b7-8205-4ded2c693a29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3753"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(format_text_response_as_prompt(train_df[train_df.article_id == '6555b54e4b13023f9348ba8b'].iloc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8976a6-b82f-4312-a525-07006a113bcc",
   "metadata": {},
   "source": [
    "### Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "38004f2f-94ed-4b9a-8c99-332d1ba0843d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e7e2ba02-cea3-4573-9161-068fac132a74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289d4d7a3f544c2cbf9e82ae42819322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b3f7f73942459c9fd175d9a8aaf7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking dataset into chunks of 4096 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f74861e51174c0d999d2b14ad623085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 407\n",
      "Total number of samples: 407\n"
     ]
    }
   ],
   "source": [
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    # sample[\"text\"] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    sample[\"text\"] = f\"{format_text_response_as_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(template_dataset)\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "# new_column = dataset['input_ids']\n",
    "# dataset = dataset.add_column(\"labels\", new_column)\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=4096) # We use 4096 as the maximum length for packing\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e7fac9a8-e26a-4402-af41-cd3ffab29849",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 32002, 1, 32001, 28705, 1587, 13, 1976, 460, 272, 9209, 7546, 354, 264, 5374, 6735, 5593, 304, 1955, 4231, 4400, 28723, 995, 15627, 7276, 9623, 302, 10437, 298, 9862, 1840, 3036, 4045, 28723, 1263, 1287, 9623, 28725, 368, 622, 907, 3084, 264, 6817, 5643, 302, 28705, 28740, 28782, 298, 28705, 28770, 28734, 3085, 28725, 4961, 486, 15081, 28723, 13, 28740, 28723, 5643, 28730, 278, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 325, 10046, 2245, 28731, 714, 523, 28149, 331, 513, 5447, 10795, 1606, 298, 15978, 28748, 25443, 442, 459, 28723, 3058, 10086, 5090, 5088, 288, 1176, 753, 24171, 442, 15104, 460, 3614, 28725, 562, 459, 513, 5405, 28742, 28707, 28767, 13, 28750, 28723, 349, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 325, 4365, 442, 8250, 28731, 714, 523, 4365, 442, 8250, 2818, 356, 3454, 6836, 28767, 13, 28770, 28723, 5643, 28730, 1009, 28730, 267, 7122, 28730, 1392, 28730, 507, 515, 325, 10046, 2245, 28731, 714, 523, 28149, 331, 513, 5447, 349, 8598, 354, 1176, 3693, 28723, 354, 2757, 10437, 684, 28705, 28781, 28734, 28740, 28729, 442, 1741, 7223, 4799, 1747, 28742, 28707, 347, 8598, 354, 1176, 515, 28723, 3545, 4435, 298, 15441, 2145, 7978, 442, 12472, 1705, 28775, 442, 2475, 2531, 262, 1249, 2278, 4231, 622, 347, 8598, 28767, 13, 28781, 28723, 8598, 28730, 1392, 28730, 507, 515, 325, 4365, 442, 8250, 28731, 714, 523, 4365, 442, 8250, 2818, 356, 3454, 6836, 28767, 13, 28782, 28723, 5643, 28730, 1009, 28730, 15461, 28730, 2398, 472, 28730, 15458, 325, 10046, 2245, 28731, 714, 523, 27554, 331, 26340, 617, 11964, 28747, 11548, 27951, 28725, 28705, 28740, 1370, 28745, 5864, 4920, 4435, 28725, 4587, 28745, 14165, 3036, 349, 5104, 6199, 6417, 378, 15654, 298, 707, 3947, 442, 799, 15885, 297, 690, 1222, 865, 28705, 28770, 28734, 2202, 28723, 5440, 4231, 297, 5558, 659, 19367, 5678, 274, 3420, 28723, 4387, 13817, 460, 4312, 459, 5104, 6199, 28745, 8249, 346, 5643, 349, 3716, 354, 264, 1819, 28725, 879, 346, 354, 264, 4308, 302, 4587, 304, 264, 1188, 3774, 624, 354, 264, 2102, 28767, 13, 28784, 28723, 5447, 28730, 2398, 472, 28730, 15458, 325, 538, 302, 28705, 28740, 28725, 28705, 28770, 28725, 28705, 28787, 28725, 28705, 28740, 28781, 28725, 28705, 28770, 28734, 28725, 387, 28740, 28731, 714, 523, 1391, 16914, 1474, 302, 2202, 2818, 356, 3454, 6836, 28723, 387, 28740, 28747, 5104, 6199, 28723, 28705, 28740, 28747, 5447, 349, 8598, 865, 354, 369, 1370, 28723, 28705, 28770, 28747, 354, 264, 4308, 302, 2202, 28723, 28705, 28787, 28747, 354, 264, 1819, 28723, 28705, 28740, 28781, 28747, 354, 264, 4308, 302, 4587, 28723, 28705, 28770, 28734, 28747, 354, 264, 2102, 28767, 13, 28787, 28723, 5643, 28730, 1009, 28730, 5951, 1098, 472, 325, 10046, 2245, 28731, 714, 523, 28149, 331, 3917, 20646, 302, 5447, 387, 513, 871, 354, 307, 5680, 7938, 28725, 22651, 28730, 5951, 1098, 472, 442, 1023, 347, 744, 302, 11313, 28730, 14290, 4211, 28725, 10085, 356, 1474, 302, 905, 693, 622, 347, 5088, 286, 486, 272, 4231, 304, 272, 5657, 302, 272, 1951, 28723, 7223, 19810, 2651, 297, 1176, 515, 562, 459, 1215, 4387, 622, 347, 7444, 307, 5680, 442, 13977, 11323, 1999, 4387, 28723, 10437, 20279, 298, 1215, 2948, 1955, 442, 724, 270, 1214, 622, 347, 307, 5680, 28723, 4319, 322, 426, 466, 1955, 304, 5593, 10437, 395, 741, 13792, 460, 3917, 298, 347, 680, 4387, 28723, 10437, 395, 264, 1274, 302, 5879, 1671, 26295, 2838, 28733, 28707, 3572, 622, 347, 354, 307, 5680, 7938, 28767, 13, 28783, 28723, 20646, 325, 538, 302, 307, 5680, 28725, 11323, 1999, 28730, 5951, 1098, 28725, 11313, 28730, 14290, 28731, 714, 523, 5527, 356, 3454, 6836, 28767, 13, 28774, 28723, 5643, 28730, 1009, 28730, 15461, 28730, 1123, 325, 10046, 2245, 28731, 714, 523, 28149, 331, 513, 272, 5447, 349, 3014, 346, 1639, 840, 28725, 349, 396, 7382, 5511, 28725, 5643, 28725, 14165, 442, 3917, 28332, 28723, 1639, 840, 10437, 24517, 3926, 28723, 7382, 7769, 506, 20596, 2477, 477, 272, 3227, 442, 477, 12908, 1671, 1178, 28723, 5643, 7769, 506, 15045, 1178, 298, 16205, 28723, 513, 396, 5447, 349, 754, 346, 6797, 14990, 356, 2552, 5756, 304, 3969, 737, 396, 616, 28725, 868, 378, 349, 28332, 28767, 13, 28740, 28734, 28723, 5447, 28730, 1123, 325, 538, 302, 1639, 28725, 7382, 28725, 5643, 28725, 14165, 28725, 28332, 28731, 714, 523, 5527, 356, 3454, 6836, 28767, 13, 28740, 28740, 28723, 5643, 28730, 1009, 28730, 15461, 28730, 16788, 2487, 325, 10046, 28730, 772, 1329, 523, 28149, 331, 513, 272, 21790, 302, 272, 5447, 349, 10386, 789, 28725, 9135, 789, 442, 20258, 28723, 19971, 349, 20258, 28767, 13, 28740, 28750, 28723, 5447, 28730, 16788, 2487, 325, 538, 302, 10386, 28725, 9135, 28725, 20258, 1329, 523, 5527, 356, 3454, 6836, 28767, 13, 28740, 28770, 28723, 1335, 1081, 28730, 28713, 16939, 296, 325, 10046, 2245, 28731, 714, 523, 5238, 264, 1335, 1081, 2818, 356, 272, 3036, 302, 272, 5447, 28767, 13, 28740, 28781, 28723, 14060, 325, 772, 302, 28705, 28784, 28734, 3085, 28731, 714, 523, 23342, 3078, 864, 28725, 9040, 28733, 28715, 1058, 14060, 28723, 415, 14060, 1023, 2727, 6416, 19319, 562, 5061, 8988, 1671, 272, 20108, 28723, 3189, 28742, 28707, 1840, 272, 14060, 1368, 2485, 28725, 562, 3607, 378, 298, 708, 680, 821, 28705, 28784, 28734, 3085, 28767, 13, 19262, 2899, 1023, 347, 264, 7611, 4693, 395, 544, 272, 28705, 28740, 28781, 2747, 8148, 1671, 6925, 707, 1945, 28723, 661, 349, 1215, 2278, 369, 272, 2899, 349, 5090, 1220, 522, 395, 7611, 28723, 28516, 1546, 708, 710, 314, 982, 442, 1704, 314, 982, 28723, 9421, 297, 272, 3459, 2296, 4693, 28747, 13, 28751, 13, 28739, 23206, 28730, 278, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 1264, 7889, 13, 28739, 278, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 1264, 7889, 13, 28739, 23206, 28730, 1009, 28730, 267, 7122, 28730, 1392, 28730, 507, 515, 1264, 7889, 13, 28739, 267, 7122, 28730, 1392, 28730, 507, 515, 1264, 7889, 13, 28739, 23206, 28730, 1009, 28730, 15461, 28730, 2398, 472, 28730, 15458, 1264, 7889, 13, 28739, 15461, 28730, 2398, 472, 28730, 15458, 1264, 7889, 13, 28739, 23206, 28730, 1009, 28730, 5951, 1098, 472, 1264, 7889, 13, 28739, 5951, 1098, 472, 1264, 7889, 13, 28739, 23206, 28730, 1009, 28730, 15461, 28730, 1123, 1264, 7889, 13, 28739, 15461, 28730, 1123, 1264, 7889, 13, 28739, 23206, 28730, 1009, 28730, 15461, 28730, 16788, 2487, 1264, 7889, 13, 28739, 15461, 28730, 16788, 2487, 1264, 7889, 13, 28739, 1811, 1081, 28730, 28713, 16939, 296, 1264, 7889, 13, 28739, 3499, 1264, 7482, 13, 28752, 13, 32000, 259, 13, 32001, 28705, 2188, 13, 28766, 15461, 28730, 2521, 28766, 13, 8463, 7059, 7179, 515, 24414, 5365, 13048, 298, 6556, 2940, 298, 22041, 4644, 11311, 28747, 8463, 15490, 628, 6636, 15487, 9701, 7179, 515, 24414, 5365, 659, 750, 13048, 298, 5526, 16355, 8351, 420, 602, 762, 12754, 395, 12380, 302, 16583, 22041, 356, 7729, 28723, 6086, 346, 7179, 515, 24414, 5365, 349, 916, 13500, 15379, 304, 349, 5489, 11311, 7291, 773, 28723, 28749, 5265, 272, 8463, 7059, 403, 13048, 298, 272, 6556, 354, 5827, 302, 264, 7692, 282, 10840, 361, 5377, 18197, 28723, 1014, 4494, 8463, 1258, 6640, 5391, 9293, 403, 438, 272, 14218, 26162, 2693, 3284, 28741, 5283, 3336, 297, 351, 2915, 1585, 356, 4074, 28705, 28740, 28723, 28705, 13, 28766, 15461, 28730, 416, 28766, 13, 32000, 259, 13, 6799, 23206, 28730, 278, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 1264, 345, 1227, 900, 684, 264, 4355, 7059, 28725, 378, 5502, 28713, 5593, 442, 1955, 6563, 9191, 345, 278, 28730, 3013, 14497, 28730, 271, 28730, 25443, 28730, 14290, 1264, 1341, 28725, 345, 23206, 28730, 1009, 28730, 267, 7122, 28730, 1392, 28730, 507, 515, 1264, 345, 1313, 10795, 1606, 298, 264, 5864, 4355, 13355, 297, 5558, 28725, 12211, 349, 8598, 9191, 345, 267, 7122, 28730, 1392, 28730, 507, 515, 1264, 1132, 28725, 345, 23206, 28730, 1009, 28730, 15461, 28730, 2398, 472, 28730, 15458, 1264, 345, 3260, 349, 264, 2528, 4009, 369, 349, 3917, 298, 347, 9109, 1024, 624, 1370, 9191, 345, 15461, 28730, 2398, 472, 28730, 15458, 1264, 28705, 28740, 28725, 345, 23206, 28730, 1009, 28730, 5951, 1098, 472, 1264, 345, 28735, 9578, 24414, 5365, 349, 264, 15574, 5248, 562, 272, 4231, 349, 264, 2528, 4009, 690, 993, 6842, 22651, 4501, 9191, 345, 5951, 1098, 472, 1264, 345, 1398, 263, 1999, 28730, 5951, 1098, 548, 345, 23206, 28730, 1009, 28730, 15461, 28730, 1123, 1264, 345, 1014, 5447, 7959, 3926, 304, 5312, 708, 7382, 28725, 5643, 442, 14165, 1192, 28723, 661, 28742, 28713, 264, 1639, 840, 5511, 9191, 345, 15461, 28730, 1123, 1264, 345, 22313, 548, 345, 23206, 28730, 1009, 28730, 15461, 28730, 16788, 2487, 1264, 345, 1014, 5447, 1235, 459, 2169, 707, 2668, 5202, 10403, 28725, 12211, 21790, 349, 20258, 9191, 345, 15461, 28730, 16788, 2487, 1264, 345, 3384, 548, 345, 1811, 1081, 28730, 28713, 16939, 296, 1264, 345, 28743, 566, 638, 26144, 7179, 515, 24414, 5365, 12754, 2458, 16043, 298, 351, 666, 401, 1548, 548, 345, 3499, 1264, 345, 28743, 566, 638, 15490, 628, 6636, 15487, 9701, 7179, 515, 24414, 5365, 403, 13048, 298, 21548, 28742, 28713, 8351, 420, 602, 762, 12754, 2940, 298, 16583, 22041, 28723, 2584, 4644, 349, 11311, 916, 15379, 611, 28752, 32000], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'],max_length=4096, truncation=True,padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4d2d8f0a-8101-4621-93b8-dac89d702b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 407\n",
       "})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57ca5f8c-4128-4a35-ae50-ec1dcb80eed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x14eaf1890>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cc531edb-7179-46f1-bdb3-07e4affa59fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-ap-south-1-005418323977'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e655928c-2388-4afc-a034-b83a3836bf96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9433667-f5f6-4c9a-a6a3-e19c9f50cd26",
   "metadata": {},
   "source": [
    "### Saving training data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "02d91b51-a693-47eb-b24d-3f3c715e7785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3231de77857047a8a1fb489e882b90bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-ap-south-1-005418323977/fine_tuning_datasets/gpt4-samples/train-full-29-nov-packed\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/fine_tuning_datasets/gpt4-samples/train-full-29-nov-packed'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2d38214b-3b12-4628-a7e7-e8ae50e7e7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 407\n",
       "})"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a2533-b2d1-476e-bad8-1b51ecb3bdb1",
   "metadata": {},
   "source": [
    "### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "44304c47-94f8-48db-ad95-a6a66ffb35a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 2,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 5,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 3,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 5e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"cosine_with_restarts\",                   # learning rate scheduler\n",
    "    'weight_decay': 0.1,\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6908d813-bb59-4f10-9b5a-79c8830e3f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}-full-29-Nov-unpacked'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = './utils/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 6*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe723b0-2ef5-4f09-a59a-21162e453b6a",
   "metadata": {},
   "source": [
    "### Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "16ae277e-1468-4629-af41-63d3400280a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-11-29 17:18:01 Starting - Starting the training job...\n",
      "2023-11-29 17:18:28 Starting - Preparing the instances for training......\n",
      "2023-11-29 17:19:31 Downloading - Downloading input data...\n",
      "2023-11-29 17:19:51 Training - Downloading the training image..............................\n",
      "2023-11-29 17:24:43 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:37,512 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:37,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:37,574 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:37,576 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:38,898 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m 7.9/7.9 MB 100.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting optimum==1.14.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl (398 kB)\u001b[0m\n",
      "\u001b[34m 398.9/398.9 kB 57.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m 258.1/258.1 kB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m 92.6/92.6 MB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 98.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m 311.7/311.7 kB 50.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m 3.8/3.8 MB 110.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m 46.0/46.0 kB 12.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m 86.8/86.8 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, humanfriendly, huggingface-hub, coloredlogs, tokenizers, accelerate, transformers, peft, optimum\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 bitsandbytes-0.41.2.post2 coloredlogs-15.0.1 huggingface-hub-0.19.4 humanfriendly-10.0 optimum-1.14.0 peft-0.5.0 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:51,894 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:51,894 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:51,970 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:52,036 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:52,099 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:52,110 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 3,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0005,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 5,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03,\n",
      "        \"weight_decay\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":3,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0005,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine_with_restarts\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":5,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03,\"weight_decay\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":3,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0005,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine_with_restarts\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":5,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03,\"weight_decay\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"3\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0005\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"cosine_with_restarts\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"teknium/OpenHermes-2.5-Mistral-7B\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"5\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\",\"--weight_decay\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=3\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0005\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine_with_restarts\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=5\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 3 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0005 --logging_steps 10 --lr_scheduler_type cosine_with_restarts --max_grad_norm 0.3 --merge_adapters True --model_id teknium/OpenHermes-2.5-Mistral-7B --num_train_epochs 2 --output_dir /tmp/run --per_device_train_batch_size 5 --save_strategy epoch --tf32 True --use_flash_attn True --warmup_ratio 0.03 --weight_decay 0.1\u001b[0m\n",
      "\u001b[34m2023-11-29 17:25:52,137 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.3.6.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m 2.3/2.3 MB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56556525 sha256=1c918d84ff042582d547e4c9c7ec24ec9f3975589ca28cc4e6cbd15486b74f2a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_NjVkEqg...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/624 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|| 624/624 [00:00<00:00, 5.82MB/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json: 100%|| 23.9k/23.9k [00:00<00:00, 160MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 31.5M/9.94G [00:00<00:39, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|          | 73.4M/9.94G [00:00<00:31, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|         | 126M/9.94G [00:00<00:25, 379MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|         | 168M/9.94G [00:00<00:30, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|         | 210M/9.94G [00:00<00:33, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|         | 241M/9.94G [00:00<00:34, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|         | 273M/9.94G [00:00<00:34, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|         | 304M/9.94G [00:01<00:35, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|         | 336M/9.94G [00:01<00:34, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|         | 398M/9.94G [00:01<00:26, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|         | 451M/9.94G [00:01<00:25, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|         | 493M/9.94G [00:01<00:28, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|         | 535M/9.94G [00:01<00:30, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|         | 587M/9.94G [00:01<00:25, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|         | 650M/9.94G [00:01<00:22, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|         | 713M/9.94G [00:02<00:19, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|         | 776M/9.94G [00:02<00:18, 489MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|         | 839M/9.94G [00:02<00:17, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|         | 902M/9.94G [00:02<00:17, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|         | 965M/9.94G [00:02<00:16, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|         | 1.03G/9.94G [00:02<00:16, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|         | 1.09G/9.94G [00:02<00:16, 548MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|        | 1.15G/9.94G [00:02<00:15, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|        | 1.22G/9.94G [00:03<00:20, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|        | 1.27G/9.94G [00:03<00:23, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|        | 1.33G/9.94G [00:03<00:20, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|        | 1.39G/9.94G [00:03<00:18, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|        | 1.46G/9.94G [00:03<00:18, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|        | 1.51G/9.94G [00:03<00:22, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|        | 1.55G/9.94G [00:03<00:22, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|        | 1.61G/9.94G [00:04<00:20, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|        | 1.67G/9.94G [00:04<00:21, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|        | 1.72G/9.94G [00:04<00:21, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|        | 1.76G/9.94G [00:04<00:21, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|        | 1.82G/9.94G [00:04<00:18, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|        | 1.88G/9.94G [00:04<00:22, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|        | 1.93G/9.94G [00:04<00:20, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|        | 1.99G/9.94G [00:04<00:18, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|        | 2.06G/9.94G [00:05<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|        | 2.11G/9.94G [00:05<00:18, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|       | 2.16G/9.94G [00:05<00:21, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|       | 2.20G/9.94G [00:05<00:23, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|       | 2.25G/9.94G [00:05<00:21, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|       | 2.32G/9.94G [00:05<00:18, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|       | 2.37G/9.94G [00:06<00:20, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|       | 2.41G/9.94G [00:06<00:21, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|       | 2.46G/9.94G [00:06<00:19, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|       | 2.52G/9.94G [00:06<00:18, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|       | 2.56G/9.94G [00:06<00:19, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|       | 2.61G/9.94G [00:06<00:18, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|       | 2.66G/9.94G [00:06<00:16, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|       | 2.72G/9.94G [00:06<00:15, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|       | 2.77G/9.94G [00:06<00:15, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|       | 2.83G/9.94G [00:07<00:14, 489MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|       | 2.88G/9.94G [00:07<00:14, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|       | 2.94G/9.94G [00:07<00:16, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|       | 2.99G/9.94G [00:07<00:16, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|       | 3.05G/9.94G [00:07<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|      | 3.11G/9.94G [00:07<00:14, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|      | 3.17G/9.94G [00:07<00:15, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|      | 3.22G/9.94G [00:08<00:18, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|      | 3.26G/9.94G [00:08<00:20, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|      | 3.30G/9.94G [00:08<00:20, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|      | 3.36G/9.94G [00:08<00:17, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|      | 3.42G/9.94G [00:08<00:15, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|      | 3.47G/9.94G [00:08<00:14, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|      | 3.52G/9.94G [00:08<00:14, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|      | 3.59G/9.94G [00:08<00:13, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|      | 3.65G/9.94G [00:08<00:12, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|      | 3.71G/9.94G [00:09<00:12, 506MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|      | 3.77G/9.94G [00:09<00:11, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|      | 3.84G/9.94G [00:09<00:11, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|      | 3.89G/9.94G [00:09<00:12, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|      | 3.94G/9.94G [00:09<00:12, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|      | 4.00G/9.94G [00:09<00:12, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|      | 4.06G/9.94G [00:09<00:11, 507MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|     | 4.12G/9.94G [00:09<00:11, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|     | 4.17G/9.94G [00:10<00:11, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|     | 4.24G/9.94G [00:10<00:11, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|     | 4.29G/9.94G [00:10<00:11, 489MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|     | 4.34G/9.94G [00:10<00:18, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|     | 4.40G/9.94G [00:10<00:15, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|     | 4.47G/9.94G [00:10<00:13, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|     | 4.53G/9.94G [00:10<00:12, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|     | 4.58G/9.94G [00:11<00:13, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|     | 4.63G/9.94G [00:11<00:14, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|     | 4.69G/9.94G [00:11<00:13, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|     | 4.75G/9.94G [00:11<00:11, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|     | 4.81G/9.94G [00:11<00:11, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|     | 4.87G/9.94G [00:11<00:13, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|     | 4.92G/9.94G [00:12<00:14, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|     | 4.96G/9.94G [00:12<00:16, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|     | 5.01G/9.94G [00:12<00:14, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|     | 5.08G/9.94G [00:12<00:12, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|    | 5.14G/9.94G [00:12<00:10, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|    | 5.20G/9.94G [00:12<00:10, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|    | 5.26G/9.94G [00:12<00:09, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|    | 5.33G/9.94G [00:12<00:09, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|    | 5.39G/9.94G [00:12<00:08, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|    | 5.45G/9.94G [00:13<00:08, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|    | 5.52G/9.94G [00:13<00:08, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|    | 5.58G/9.94G [00:13<00:08, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|    | 5.64G/9.94G [00:13<00:08, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|    | 5.70G/9.94G [00:13<00:07, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|    | 5.77G/9.94G [00:13<00:07, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|    | 5.83G/9.94G [00:13<00:10, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|    | 5.88G/9.94G [00:14<00:11, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|    | 5.92G/9.94G [00:14<00:12, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|    | 5.97G/9.94G [00:14<00:13, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|    | 6.01G/9.94G [00:14<00:13, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|    | 6.04G/9.94G [00:14<00:13, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|   | 6.09G/9.94G [00:14<00:11, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|   | 6.16G/9.94G [00:14<00:09, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|   | 6.22G/9.94G [00:15<00:08, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|   | 6.28G/9.94G [00:15<00:07, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|   | 6.34G/9.94G [00:15<00:07, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|   | 6.41G/9.94G [00:15<00:07, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|   | 6.46G/9.94G [00:15<00:07, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|   | 6.51G/9.94G [00:15<00:08, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  66%|   | 6.56G/9.94G [00:15<00:08, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|   | 6.63G/9.94G [00:15<00:07, 449MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|   | 6.69G/9.94G [00:16<00:06, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|   | 6.75G/9.94G [00:16<00:06, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|   | 6.82G/9.94G [00:16<00:06, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|   | 6.88G/9.94G [00:16<00:05, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|   | 6.94G/9.94G [00:16<00:05, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|   | 7.00G/9.94G [00:16<00:05, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|   | 7.07G/9.94G [00:16<00:05, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|  | 7.13G/9.94G [00:16<00:05, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|  | 7.18G/9.94G [00:17<00:05, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  73%|  | 7.25G/9.94G [00:17<00:05, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|  | 7.31G/9.94G [00:17<00:05, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|  | 7.37G/9.94G [00:17<00:05, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|  | 7.42G/9.94G [00:17<00:05, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|  | 7.49G/9.94G [00:17<00:05, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|  | 7.54G/9.94G [00:17<00:05, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|  | 7.59G/9.94G [00:18<00:06, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|  | 7.63G/9.94G [00:18<00:06, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|  | 7.68G/9.94G [00:25<01:46, 21.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|  | 7.71G/9.94G [00:25<01:23, 26.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|  | 7.76G/9.94G [00:25<00:55, 39.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|  | 7.80G/9.94G [00:25<00:41, 51.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|  | 7.83G/9.94G [00:25<00:33, 63.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|  | 7.90G/9.94G [00:26<00:20, 97.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|  | 7.96G/9.94G [00:26<00:14, 139MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|  | 8.02G/9.94G [00:26<00:10, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%| | 8.08G/9.94G [00:26<00:07, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%| | 8.14G/9.94G [00:26<00:06, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%| | 8.19G/9.94G [00:26<00:06, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%| | 8.24G/9.94G [00:26<00:05, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%| | 8.30G/9.94G [00:26<00:04, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%| | 8.36G/9.94G [00:27<00:04, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%| | 8.41G/9.94G [00:27<00:04, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%| | 8.47G/9.94G [00:27<00:03, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%| | 8.54G/9.94G [00:27<00:03, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%| | 8.59G/9.94G [00:27<00:03, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%| | 8.65G/9.94G [00:27<00:02, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%| | 8.70G/9.94G [00:27<00:02, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%| | 8.76G/9.94G [00:28<00:03, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%| | 8.80G/9.94G [00:28<00:03, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%| | 8.84G/9.94G [00:28<00:03, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%| | 8.90G/9.94G [00:28<00:02, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%| | 8.97G/9.94G [00:28<00:02, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%| | 9.03G/9.94G [00:28<00:01, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%|| 9.09G/9.94G [00:28<00:01, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|| 9.14G/9.94G [00:28<00:01, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|| 9.21G/9.94G [00:29<00:01, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|| 9.26G/9.94G [00:29<00:01, 474MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|| 9.31G/9.94G [00:29<00:01, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|| 9.35G/9.94G [00:29<00:01, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|| 9.42G/9.94G [00:29<00:01, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|| 9.48G/9.94G [00:29<00:00, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|| 9.54G/9.94G [00:29<00:00, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|| 9.60G/9.94G [00:29<00:00, 506MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|| 9.66G/9.94G [00:30<00:00, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|| 9.72G/9.94G [00:30<00:00, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|| 9.78G/9.94G [00:30<00:00, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|| 9.84G/9.94G [00:30<00:00, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|| 9.89G/9.94G [00:30<00:00, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|| 9.94G/9.94G [00:30<00:00, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|| 9.94G/9.94G [00:30<00:00, 323MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|     | 1/2 [00:30<00:30, 30.99s/it]\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   1%|         | 62.9M/4.54G [00:00<00:08, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   3%|         | 126M/4.54G [00:00<00:08, 548MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   4%|         | 189M/4.54G [00:00<00:08, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   6%|         | 252M/4.54G [00:00<00:07, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   7%|         | 315M/4.54G [00:00<00:07, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   8%|         | 377M/4.54G [00:00<00:07, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  10%|         | 440M/4.54G [00:00<00:09, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  11%|         | 503M/4.54G [00:01<00:08, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  12%|        | 566M/4.54G [00:01<00:08, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  14%|        | 629M/4.54G [00:01<00:07, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  15%|        | 692M/4.54G [00:01<00:07, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  17%|        | 755M/4.54G [00:01<00:07, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  18%|        | 818M/4.54G [00:01<00:06, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  19%|        | 881M/4.54G [00:01<00:06, 548MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  21%|        | 944M/4.54G [00:01<00:07, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  22%|       | 996M/4.54G [00:02<00:09, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  23%|       | 1.05G/4.54G [00:02<00:08, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  24%|       | 1.11G/4.54G [00:02<00:07, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  26%|       | 1.17G/4.54G [00:02<00:07, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  27%|       | 1.24G/4.54G [00:02<00:06, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  29%|       | 1.30G/4.54G [00:02<00:06, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  30%|       | 1.36G/4.54G [00:02<00:07, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  31%|       | 1.42G/4.54G [00:02<00:07, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  33%|      | 1.48G/4.54G [00:03<00:06, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  34%|      | 1.53G/4.54G [00:03<00:06, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  35%|      | 1.58G/4.54G [00:03<00:07, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  36%|      | 1.64G/4.54G [00:03<00:07, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  37%|      | 1.70G/4.54G [00:03<00:06, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  39%|      | 1.76G/4.54G [00:03<00:05, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  40%|      | 1.81G/4.54G [00:03<00:05, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  41%|      | 1.87G/4.54G [00:04<00:07, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  42%|     | 1.91G/4.54G [00:04<00:07, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  43%|     | 1.97G/4.54G [00:04<00:06, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  45%|     | 2.02G/4.54G [00:04<00:08, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  46%|     | 2.09G/4.54G [00:04<00:06, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  47%|     | 2.15G/4.54G [00:04<00:05, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  48%|     | 2.20G/4.54G [00:04<00:06, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  50%|     | 2.25G/4.54G [00:05<00:05, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  51%|     | 2.32G/4.54G [00:05<00:05, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  52%|    | 2.38G/4.54G [00:05<00:04, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  54%|    | 2.44G/4.54G [00:05<00:04, 489MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  55%|    | 2.51G/4.54G [00:05<00:04, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  56%|    | 2.56G/4.54G [00:05<00:04, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  58%|    | 2.62G/4.54G [00:05<00:04, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  59%|    | 2.68G/4.54G [00:05<00:04, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  60%|    | 2.74G/4.54G [00:06<00:04, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  61%|    | 2.78G/4.54G [00:06<00:05, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  63%|   | 2.84G/4.54G [00:06<00:04, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  64%|   | 2.90G/4.54G [00:06<00:03, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  65%|   | 2.97G/4.54G [00:06<00:03, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  67%|   | 3.02G/4.54G [00:06<00:03, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  68%|   | 3.07G/4.54G [00:07<00:04, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  69%|   | 3.11G/4.54G [00:07<00:04, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  70%|   | 3.16G/4.54G [00:07<00:04, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  70%|   | 3.20G/4.54G [00:07<00:04, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  72%|  | 3.25G/4.54G [00:07<00:03, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  73%|  | 3.31G/4.54G [00:07<00:03, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  74%|  | 3.38G/4.54G [00:07<00:02, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  76%|  | 3.44G/4.54G [00:07<00:02, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  77%|  | 3.50G/4.54G [00:08<00:02, 490MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  79%|  | 3.57G/4.54G [00:08<00:01, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  80%|  | 3.63G/4.54G [00:08<00:01, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  81%| | 3.69G/4.54G [00:08<00:01, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  82%| | 3.74G/4.54G [00:08<00:02, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  84%| | 3.80G/4.54G [00:08<00:02, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  85%| | 3.85G/4.54G [00:09<00:01, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  86%| | 3.91G/4.54G [00:09<00:01, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  88%| | 3.97G/4.54G [00:09<00:01, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  89%| | 4.03G/4.54G [00:09<00:01, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  90%| | 4.09G/4.54G [00:09<00:00, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  91%| | 4.14G/4.54G [00:09<00:00, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  92%|| 4.19G/4.54G [00:09<00:00, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  94%|| 4.26G/4.54G [00:09<00:00, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  95%|| 4.32G/4.54G [00:10<00:00, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  96%|| 4.37G/4.54G [00:10<00:00, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  97%|| 4.42G/4.54G [00:10<00:00, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  99%|| 4.49G/4.54G [00:10<00:00, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|| 4.54G/4.54G [00:10<00:00, 430MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|| 2/2 [00:41<00:00, 19.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|| 2/2 [00:41<00:00, 20.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:05<00:05,  5.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:07<00:00,  3.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:07<00:00,  3.86s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|| 120/120 [00:00<00:00, 966kB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['k_proj', 'down_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj', 'o_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 167,772,160 || all params: 7,409,520,640 || trainable%: 2.2642781922259414\u001b[0m\n",
      "\u001b[34m0%|          | 0/54 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34m2%|         | 1/54 [01:28<1:18:33, 88.93s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 2/54 [02:57<1:16:53, 88.72s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 3/54 [04:26<1:15:21, 88.66s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 4/54 [05:54<1:13:51, 88.63s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 5/54 [07:23<1:12:21, 88.61s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 6/54 [08:51<1:10:52, 88.60s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 7/54 [10:20<1:09:23, 88.59s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 8/54 [11:48<1:07:55, 88.59s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 9/54 [13:17<1:06:26, 88.59s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 10/54 [14:46<1:04:57, 88.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6971, 'learning_rate': 0.00047136400641330245, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m19%|        | 10/54 [14:46<1:04:57, 88.58s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 11/54 [16:14<1:03:29, 88.58s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 12/54 [17:43<1:02:00, 88.58s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 13/54 [19:11<1:00:31, 88.58s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 14/54 [20:40<59:03, 88.58s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 15/54 [22:09<57:34, 88.58s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 16/54 [23:37<56:06, 88.58s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 17/54 [25:06<54:37, 88.58s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 18/54 [26:34<53:08, 88.58s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 19/54 [28:03<51:40, 88.58s/it]\u001b[0m\n",
      "\u001b[34m37%|      | 20/54 [29:31<50:11, 88.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3249, 'learning_rate': 0.00036618079301094214, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m37%|      | 20/54 [29:31<50:11, 88.58s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 21/54 [31:00<48:43, 88.58s/it]\u001b[0m\n",
      "\u001b[34m41%|      | 22/54 [32:29<47:14, 88.58s/it]\u001b[0m\n",
      "\u001b[34m43%|     | 23/54 [33:57<45:46, 88.58s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 24/54 [35:26<44:17, 88.58s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 25/54 [36:54<42:48, 88.58s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 26/54 [38:23<41:20, 88.58s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 27/54 [39:52<39:51, 88.58s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 28/54 [41:04<36:16, 83.70s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 29/54 [42:32<35:29, 85.17s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 30/54 [44:01<34:28, 86.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8796, 'learning_rate': 0.00021986582993616926, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m56%|    | 30/54 [44:01<34:28, 86.19s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 31/54 [45:30<33:18, 86.91s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 32/54 [46:58<32:02, 87.41s/it]\u001b[0m\n",
      "\u001b[34m61%|    | 33/54 [48:27<30:42, 87.76s/it]\u001b[0m\n",
      "\u001b[34m63%|   | 34/54 [49:55<29:20, 88.00s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 35/54 [51:24<27:55, 88.18s/it]\u001b[0m\n",
      "\u001b[34m67%|   | 36/54 [52:52<26:29, 88.30s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 37/54 [54:21<25:02, 88.38s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 38/54 [55:50<23:35, 88.44s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 39/54 [57:18<22:07, 88.48s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 40/54 [58:47<20:39, 88.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7603, 'learning_rate': 8.421933543980126e-05, 'epoch': 1.46}\u001b[0m\n",
      "\u001b[34m74%|  | 40/54 [58:47<20:39, 88.51s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 41/54 [1:00:15<19:10, 88.53s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 42/54 [1:01:44<17:42, 88.54s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 43/54 [1:03:12<16:14, 88.55s/it]\u001b[0m\n",
      "\u001b[34m81%| | 44/54 [1:04:41<14:45, 88.56s/it]\u001b[0m\n",
      "\u001b[34m83%| | 45/54 [1:06:10<13:17, 88.56s/it]\u001b[0m\n",
      "\u001b[34m85%| | 46/54 [1:07:38<11:48, 88.57s/it]\u001b[0m\n",
      "\u001b[34m87%| | 47/54 [1:09:07<10:19, 88.57s/it]\u001b[0m\n",
      "\u001b[34m89%| | 48/54 [1:10:35<08:51, 88.57s/it]\u001b[0m\n",
      "\u001b[34m91%| | 49/54 [1:12:04<07:22, 88.57s/it]\u001b[0m\n",
      "\u001b[34m93%|| 50/54 [1:13:33<05:54, 88.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7691, 'learning_rate': 7.2645456434869975e-06, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m93%|| 50/54 [1:13:33<05:54, 88.57s/it]\u001b[0m\n",
      "\u001b[34m94%|| 51/54 [1:15:01<04:25, 88.57s/it]\u001b[0m\n",
      "\u001b[34m96%|| 52/54 [1:16:30<02:57, 88.57s/it]\u001b[0m\n",
      "\u001b[34m98%|| 53/54 [1:17:58<01:28, 88.57s/it]\u001b[0m\n",
      "\u001b[34m100%|| 54/54 [1:19:27<00:00, 88.57s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 4768.2568, 'train_samples_per_second': 0.171, 'train_steps_per_second': 0.011, 'train_loss': 1.0634425260402538, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m100%|| 54/54 [1:19:28<00:00, 88.57s/it]\u001b[0m\n",
      "\u001b[34m100%|| 54/54 [1:19:28<00:00, 88.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:04<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:06<00:00,  2.86s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.04s/it]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|| 1.60k/1.60k [00:00<00:00, 13.1MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|| 493k/493k [00:00<00:00, 86.9MB/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json: 100%|| 51.0/51.0 [00:00<00:00, 604kB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|| 101/101 [00:00<00:00, 1.28MB/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m2023-11-29 18:50:20,013 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-29 18:50:20,013 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-29 18:50:20,013 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-29 18:50:26 Uploading - Uploading generated training model\n",
      "2023-11-29 18:51:01 Completed - Training job completed\n",
      "Training seconds: 5490\n",
      "Billable seconds: 5490\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8da6f9-cae4-41d6-8577-3ce9c8b6f954",
   "metadata": {},
   "source": [
    "### Old Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29232bcb-99fa-4c2d-adaf-90ed668da129",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-11-29 09:07:52 Starting - Starting the training job......\n",
      "2023-11-29 09:08:31 Starting - Preparing the instances for training...\n",
      "2023-11-29 09:09:25 Downloading - Downloading input data...\n",
      "2023-11-29 09:09:40 Training - Downloading the training image........................\n",
      "2023-11-29 09:13:52 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-29 09:14:48,195 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-29 09:14:48,249 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 09:14:48,258 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-29 09:14:48,260 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-29 09:14:49,583 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m 7.9/7.9 MB 106.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 22.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting optimum==1.14.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl (398 kB)\u001b[0m\n",
      "\u001b[34m 398.9/398.9 kB 69.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m 258.1/258.1 kB 54.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m 92.6/92.6 MB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 111.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m 311.7/311.7 kB 58.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m 3.8/3.8 MB 79.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m 46.0/46.0 kB 15.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m 86.8/86.8 kB 28.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, humanfriendly, huggingface-hub, coloredlogs, tokenizers, accelerate, transformers, peft, optimum\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 bitsandbytes-0.41.2.post2 coloredlogs-15.0.1 huggingface-hub-0.19.4 humanfriendly-10.0 optimum-1.14.0 peft-0.5.0 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,264 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,265 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,338 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,401 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,464 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,475 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 5,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":5,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":5,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-09-07-51-205/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"teknium/OpenHermes-2.5-Mistral-7B\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"5\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=5\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 2 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type cosine --max_grad_norm 0.3 --merge_adapters True --model_id teknium/OpenHermes-2.5-Mistral-7B --num_train_epochs 2 --output_dir /tmp/run --per_device_train_batch_size 5 --save_strategy epoch --tf32 True --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2023-11-29 09:15:02,502 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.3.6.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m 2.3/2.3 MB 42.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56556525 sha256=1c918d84ff042582d547e4c9c7ec24ec9f3975589ca28cc4e6cbd15486b74f2a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_NjVkEqg...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/624 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|| 624/624 [00:00<00:00, 6.07MB/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json: 100%|| 23.9k/23.9k [00:00<00:00, 68.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|          | 52.4M/9.94G [00:00<00:19, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|          | 105M/9.94G [00:00<00:19, 499MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|         | 157M/9.94G [00:00<00:19, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|         | 210M/9.94G [00:00<00:19, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|         | 262M/9.94G [00:00<00:19, 507MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|         | 315M/9.94G [00:00<00:18, 511MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|         | 377M/9.94G [00:00<00:18, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|         | 440M/9.94G [00:00<00:17, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|         | 503M/9.94G [00:00<00:17, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|         | 566M/9.94G [00:01<00:17, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|         | 629M/9.94G [00:01<00:17, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|         | 692M/9.94G [00:01<00:17, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|         | 755M/9.94G [00:01<00:17, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|         | 818M/9.94G [00:01<00:17, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|         | 881M/9.94G [00:01<00:17, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|         | 944M/9.94G [00:01<00:16, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|         | 1.01G/9.94G [00:01<00:18, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|         | 1.07G/9.94G [00:02<00:17, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|        | 1.13G/9.94G [00:02<00:17, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|        | 1.20G/9.94G [00:02<00:16, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|        | 1.26G/9.94G [00:02<00:16, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|        | 1.32G/9.94G [00:02<00:16, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|        | 1.38G/9.94G [00:02<00:15, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|        | 1.45G/9.94G [00:02<00:15, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|        | 1.51G/9.94G [00:02<00:15, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|        | 1.57G/9.94G [00:02<00:15, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|        | 1.64G/9.94G [00:03<00:15, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|        | 1.70G/9.94G [00:03<00:15, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|        | 1.75G/9.94G [00:03<00:15, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|        | 1.81G/9.94G [00:03<00:15, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|        | 1.88G/9.94G [00:03<00:15, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|        | 1.94G/9.94G [00:03<00:14, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|        | 2.00G/9.94G [00:03<00:14, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|        | 2.07G/9.94G [00:03<00:14, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|       | 2.13G/9.94G [00:04<00:14, 548MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|       | 2.19G/9.94G [00:04<00:14, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|       | 2.25G/9.94G [00:04<00:13, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|       | 2.32G/9.94G [00:04<00:13, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|       | 2.38G/9.94G [00:04<00:13, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|       | 2.44G/9.94G [00:04<00:14, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|       | 2.50G/9.94G [00:04<00:14, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|       | 2.55G/9.94G [00:04<00:14, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|       | 2.60G/9.94G [00:04<00:14, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|       | 2.66G/9.94G [00:05<00:14, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|       | 2.73G/9.94G [00:05<00:13, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|       | 2.79G/9.94G [00:05<00:13, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|       | 2.85G/9.94G [00:05<00:13, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|       | 2.92G/9.94G [00:05<00:13, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|       | 2.98G/9.94G [00:05<00:12, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|       | 3.04G/9.94G [00:05<00:12, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|       | 3.10G/9.94G [00:05<00:12, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|      | 3.17G/9.94G [00:05<00:12, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|      | 3.23G/9.94G [00:06<00:12, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|      | 3.29G/9.94G [00:06<00:12, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|      | 3.36G/9.94G [00:06<00:12, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|      | 3.42G/9.94G [00:06<00:12, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|      | 3.48G/9.94G [00:06<00:12, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|      | 3.54G/9.94G [00:06<00:12, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|      | 3.60G/9.94G [00:06<00:13, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|      | 3.66G/9.94G [00:06<00:12, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|      | 3.72G/9.94G [00:07<00:12, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|      | 3.79G/9.94G [00:07<00:11, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|      | 3.85G/9.94G [00:07<00:11, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|      | 3.91G/9.94G [00:07<00:11, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|      | 3.97G/9.94G [00:07<00:10, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|      | 4.04G/9.94G [00:07<00:10, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|      | 4.10G/9.94G [00:07<00:10, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|     | 4.16G/9.94G [00:07<00:10, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|     | 4.23G/9.94G [00:08<00:10, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|     | 4.28G/9.94G [00:08<00:10, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|     | 4.34G/9.94G [00:08<00:10, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|     | 4.40G/9.94G [00:08<00:10, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|     | 4.47G/9.94G [00:08<00:10, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|     | 4.52G/9.94G [00:08<00:12, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|     | 4.57G/9.94G [00:08<00:11, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|     | 4.63G/9.94G [00:08<00:11, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|     | 4.69G/9.94G [00:08<00:10, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|     | 4.74G/9.94G [00:09<00:12, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|     | 4.80G/9.94G [00:09<00:11, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|     | 4.87G/9.94G [00:09<00:10, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|     | 4.92G/9.94G [00:09<00:10, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|     | 4.98G/9.94G [00:09<00:09, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|     | 5.04G/9.94G [00:09<00:09, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|    | 5.11G/9.94G [00:09<00:09, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|    | 5.17G/9.94G [00:09<00:08, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|    | 5.23G/9.94G [00:10<00:08, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|    | 5.30G/9.94G [00:10<00:08, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|    | 5.36G/9.94G [00:10<00:08, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|    | 5.41G/9.94G [00:10<00:09, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|    | 5.46G/9.94G [00:10<00:11, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|    | 5.52G/9.94G [00:10<00:10, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|    | 5.57G/9.94G [00:10<00:11, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|    | 5.62G/9.94G [00:11<00:10, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|    | 5.67G/9.94G [00:11<00:10, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|    | 5.73G/9.94G [00:11<00:09, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|    | 5.78G/9.94G [00:11<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|    | 5.84G/9.94G [00:11<00:08, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|    | 5.90G/9.94G [00:11<00:08, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|    | 5.97G/9.94G [00:11<00:07, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|    | 6.03G/9.94G [00:11<00:07, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|    | 6.08G/9.94G [00:11<00:09, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|   | 6.14G/9.94G [00:12<00:08, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|   | 6.21G/9.94G [00:12<00:07, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|   | 6.27G/9.94G [00:12<00:07, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|   | 6.32G/9.94G [00:12<00:07, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|   | 6.39G/9.94G [00:12<00:06, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|   | 6.45G/9.94G [00:12<00:06, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|   | 6.51G/9.94G [00:12<00:09, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  66%|   | 6.57G/9.94G [00:13<00:08, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|   | 6.63G/9.94G [00:13<00:07, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|   | 6.68G/9.94G [00:13<00:07, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|   | 6.73G/9.94G [00:13<00:07, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|   | 6.78G/9.94G [00:13<00:07, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|   | 6.84G/9.94G [00:13<00:06, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|   | 6.90G/9.94G [00:13<00:06, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|   | 6.96G/9.94G [00:13<00:05, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|   | 7.03G/9.94G [00:13<00:05, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|  | 7.09G/9.94G [00:14<00:05, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|  | 7.15G/9.94G [00:14<00:05, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|  | 7.20G/9.94G [00:14<00:05, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  73%|  | 7.26G/9.94G [00:14<00:05, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|  | 7.31G/9.94G [00:14<00:06, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|  | 7.37G/9.94G [00:14<00:05, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|  | 7.43G/9.94G [00:14<00:05, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|  | 7.50G/9.94G [00:14<00:04, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|  | 7.56G/9.94G [00:15<00:04, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|  | 7.61G/9.94G [00:15<00:04, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|  | 7.68G/9.94G [00:15<00:04, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|  | 7.74G/9.94G [00:15<00:04, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|  | 7.80G/9.94G [00:15<00:04, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|  | 7.86G/9.94G [00:15<00:03, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|  | 7.93G/9.94G [00:15<00:03, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|  | 7.99G/9.94G [00:15<00:03, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|  | 8.05G/9.94G [00:16<00:03, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%| | 8.12G/9.94G [00:16<00:03, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%| | 8.18G/9.94G [00:16<00:03, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%| | 8.24G/9.94G [00:16<00:03, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%| | 8.30G/9.94G [00:16<00:03, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%| | 8.37G/9.94G [00:16<00:03, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%| | 8.43G/9.94G [00:16<00:03, 474MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%| | 8.49G/9.94G [00:16<00:02, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%| | 8.56G/9.94G [00:17<00:02, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%| | 8.62G/9.94G [00:17<00:02, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%| | 8.68G/9.94G [00:17<00:02, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%| | 8.75G/9.94G [00:17<00:02, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%| | 8.81G/9.94G [00:17<00:02, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%| | 8.87G/9.94G [00:17<00:01, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%| | 8.93G/9.94G [00:17<00:01, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%| | 9.00G/9.94G [00:17<00:01, 562MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%| | 9.06G/9.94G [00:18<00:01, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|| 9.11G/9.94G [00:18<00:01, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|| 9.18G/9.94G [00:18<00:01, 506MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|| 9.24G/9.94G [00:18<00:01, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|| 9.30G/9.94G [00:18<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|| 9.36G/9.94G [00:18<00:01, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|| 9.43G/9.94G [00:18<00:00, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|| 9.49G/9.94G [00:18<00:00, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|| 9.55G/9.94G [00:18<00:00, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|| 9.62G/9.94G [00:19<00:00, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|| 9.68G/9.94G [00:19<00:00, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|| 9.74G/9.94G [00:19<00:00, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|| 9.80G/9.94G [00:19<00:00, 564MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|| 9.87G/9.94G [00:19<00:00, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|| 9.93G/9.94G [00:19<00:00, 561MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|| 9.94G/9.94G [00:19<00:00, 507MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|     | 1/2 [00:19<00:19, 19.86s/it]\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   1%|          | 31.5M/4.54G [00:00<00:20, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   2%|         | 73.4M/4.54G [00:00<00:14, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   3%|         | 136M/4.54G [00:00<00:10, 428MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   4%|         | 199M/4.54G [00:00<00:08, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   6%|         | 262M/4.54G [00:00<00:08, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   7%|         | 325M/4.54G [00:00<00:10, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   9%|         | 388M/4.54G [00:00<00:09, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  10%|         | 451M/4.54G [00:01<00:08, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  11%|        | 514M/4.54G [00:01<00:08, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  13%|        | 577M/4.54G [00:01<00:07, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  14%|        | 640M/4.54G [00:01<00:07, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  15%|        | 692M/4.54G [00:01<00:07, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  17%|        | 755M/4.54G [00:01<00:07, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  18%|        | 818M/4.54G [00:01<00:07, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  19%|        | 881M/4.54G [00:01<00:06, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  21%|        | 944M/4.54G [00:01<00:06, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  22%|       | 1.01G/4.54G [00:02<00:06, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  24%|       | 1.07G/4.54G [00:02<00:06, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  25%|       | 1.13G/4.54G [00:02<00:06, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  26%|       | 1.20G/4.54G [00:02<00:06, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  28%|       | 1.26G/4.54G [00:02<00:06, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  29%|       | 1.32G/4.54G [00:02<00:06, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  30%|       | 1.38G/4.54G [00:02<00:06, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  32%|      | 1.45G/4.54G [00:02<00:06, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  33%|      | 1.50G/4.54G [00:03<00:06, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  34%|      | 1.55G/4.54G [00:03<00:06, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  35%|      | 1.60G/4.54G [00:03<00:06, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  37%|      | 1.67G/4.54G [00:03<00:05, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  38%|      | 1.73G/4.54G [00:03<00:05, 511MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  39%|      | 1.79G/4.54G [00:03<00:05, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  41%|      | 1.86G/4.54G [00:03<00:05, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  42%|     | 1.92G/4.54G [00:03<00:04, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  44%|     | 1.98G/4.54G [00:03<00:04, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  45%|     | 2.04G/4.54G [00:04<00:06, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  46%|     | 2.10G/4.54G [00:04<00:06, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  47%|     | 2.15G/4.54G [00:04<00:06, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  49%|     | 2.21G/4.54G [00:04<00:05, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  50%|     | 2.28G/4.54G [00:04<00:04, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  51%|    | 2.34G/4.54G [00:04<00:04, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  53%|    | 2.40G/4.54G [00:05<00:04, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  54%|    | 2.46G/4.54G [00:05<00:03, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  56%|    | 2.53G/4.54G [00:05<00:03, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  57%|    | 2.59G/4.54G [00:05<00:03, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  58%|    | 2.65G/4.54G [00:05<00:03, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  60%|    | 2.72G/4.54G [00:05<00:03, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  61%|    | 2.78G/4.54G [00:05<00:03, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  63%|   | 2.84G/4.54G [00:05<00:03, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  64%|   | 2.90G/4.54G [00:05<00:02, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  65%|   | 2.97G/4.54G [00:06<00:02, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  67%|   | 3.03G/4.54G [00:06<00:03, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  68%|   | 3.08G/4.54G [00:06<00:03, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  69%|   | 3.15G/4.54G [00:06<00:02, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  71%|   | 3.21G/4.54G [00:06<00:02, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  72%|  | 3.27G/4.54G [00:06<00:02, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  73%|  | 3.33G/4.54G [00:06<00:02, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  75%|  | 3.40G/4.54G [00:06<00:02, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  76%|  | 3.46G/4.54G [00:07<00:01, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  78%|  | 3.52G/4.54G [00:07<00:01, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  79%|  | 3.59G/4.54G [00:07<00:01, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  80%|  | 3.65G/4.54G [00:07<00:01, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  82%| | 3.71G/4.54G [00:07<00:01, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  83%| | 3.77G/4.54G [00:07<00:01, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  85%| | 3.84G/4.54G [00:07<00:01, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  86%| | 3.90G/4.54G [00:07<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  87%| | 3.96G/4.54G [00:07<00:01, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  89%| | 4.03G/4.54G [00:08<00:00, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  90%| | 4.09G/4.54G [00:08<00:00, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  91%|| 4.15G/4.54G [00:08<00:00, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  93%|| 4.22G/4.54G [00:08<00:00, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  94%|| 4.27G/4.54G [00:08<00:00, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  95%|| 4.32G/4.54G [00:08<00:00, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  97%|| 4.38G/4.54G [00:08<00:00, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  98%|| 4.45G/4.54G [00:08<00:00, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  99%|| 4.51G/4.54G [00:09<00:00, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|| 4.54G/4.54G [00:09<00:00, 497MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|| 2/2 [00:29<00:00, 13.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|| 2/2 [00:29<00:00, 14.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:05<00:05,  5.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:07<00:00,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:07<00:00,  3.85s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|| 120/120 [00:00<00:00, 993kB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['up_proj', 'q_proj', 'v_proj', 'down_proj', 'gate_proj', 'k_proj', 'o_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 167,772,160 || all params: 7,409,520,640 || trainable%: 2.2642781922259414\u001b[0m\n",
      "\u001b[34m0%|          | 0/80 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34m1%|         | 1/80 [01:01<1:20:38, 61.25s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 2/80 [02:02<1:19:21, 61.05s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 3/80 [03:03<1:18:15, 60.98s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 4/80 [04:03<1:17:12, 60.95s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 5/80 [05:04<1:16:10, 60.93s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 6/80 [06:05<1:15:08, 60.92s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 7/80 [07:06<1:14:07, 60.92s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 8/80 [08:07<1:13:05, 60.91s/it]\u001b[0m\n",
      "\u001b[34m11%|        | 9/80 [09:08<1:12:04, 60.91s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 10/80 [10:09<1:11:03, 60.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0871, 'learning_rate': 0.00019594929736144976, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m12%|        | 10/80 [10:09<1:11:03, 60.91s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 11/80 [11:10<1:10:02, 60.91s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 12/80 [12:11<1:09:01, 60.91s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 13/80 [13:12<1:08:00, 60.91s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 14/80 [14:13<1:06:59, 60.91s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 15/80 [15:13<1:05:58, 60.91s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 16/80 [16:14<1:04:58, 60.91s/it]\u001b[0m\n",
      "\u001b[34m21%|       | 17/80 [17:15<1:03:57, 60.91s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 18/80 [18:16<1:02:56, 60.91s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 19/80 [19:17<1:01:55, 60.91s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 20/80 [20:18<1:00:54, 60.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8185, 'learning_rate': 0.0001768950525339362, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m25%|       | 20/80 [20:18<1:00:54, 60.91s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 21/80 [21:19<59:53, 60.91s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 22/80 [22:20<58:52, 60.91s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 23/80 [23:21<57:51, 60.91s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 24/80 [24:22<56:50, 60.91s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 25/80 [25:23<55:49, 60.91s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 26/80 [26:23<54:49, 60.91s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 27/80 [27:24<53:48, 60.91s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 28/80 [28:25<52:47, 60.91s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 29/80 [29:26<51:46, 60.91s/it]\u001b[0m\n",
      "\u001b[34m38%|      | 30/80 [30:27<50:45, 60.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.846, 'learning_rate': 0.00014521719072826858, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m38%|      | 30/80 [30:27<50:45, 60.91s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 31/80 [31:28<49:44, 60.91s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 32/80 [32:29<48:43, 60.91s/it]\u001b[0m\n",
      "\u001b[34m41%|     | 33/80 [33:30<47:42, 60.91s/it]\u001b[0m\n",
      "\u001b[34m42%|     | 34/80 [34:31<46:41, 60.91s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 35/80 [35:32<45:40, 60.91s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 36/80 [36:33<44:39, 60.91s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 37/80 [37:33<43:39, 60.91s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 38/80 [38:34<42:38, 60.91s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 39/80 [39:35<41:37, 60.91s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 40/80 [40:36<40:36, 60.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8661, 'learning_rate': 0.00010611616608218429, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m50%|     | 40/80 [40:36<40:36, 60.90s/it]\u001b[0m\n",
      "\u001b[34m51%|    | 41/80 [41:20<36:19, 55.89s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 42/80 [42:21<36:21, 57.40s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 43/80 [43:22<36:02, 58.45s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 44/80 [44:23<35:30, 59.19s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 45/80 [45:24<34:49, 59.70s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 46/80 [46:25<34:02, 60.07s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 47/80 [47:26<33:10, 60.32s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 48/80 [48:27<32:15, 60.50s/it]\u001b[0m\n",
      "\u001b[34m61%|   | 49/80 [49:28<31:19, 60.62s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 50/80 [50:28<30:21, 60.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.69, 'learning_rate': 6.601106984173835e-05, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m62%|   | 50/80 [50:28<30:21, 60.71s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 51/80 [51:29<29:22, 60.77s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 52/80 [52:30<28:22, 60.81s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 53/80 [53:31<27:22, 60.84s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 54/80 [54:32<26:22, 60.86s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 55/80 [55:33<25:21, 60.87s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 56/80 [56:34<24:21, 60.88s/it]\u001b[0m\n",
      "\u001b[34m71%|  | 57/80 [57:35<23:20, 60.89s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 58/80 [58:36<22:19, 60.89s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 59/80 [59:37<21:18, 60.90s/it]\u001b[0m\n",
      "\u001b[34m75%|  | 60/80 [1:00:38<20:17, 60.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6647, 'learning_rate': 3.1485828503215585e-05, 'epoch': 1.48}\u001b[0m\n",
      "\u001b[34m75%|  | 60/80 [1:00:38<20:17, 60.90s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 61/80 [1:01:38<19:17, 60.90s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 62/80 [1:02:39<18:16, 60.90s/it]\u001b[0m\n",
      "\u001b[34m79%|  | 63/80 [1:03:40<17:15, 60.90s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 64/80 [1:04:41<16:14, 60.90s/it]\u001b[0m\n",
      "\u001b[34m81%| | 65/80 [1:05:42<15:13, 60.90s/it]\u001b[0m\n",
      "\u001b[34m82%| | 66/80 [1:06:43<14:12, 60.90s/it]\u001b[0m\n",
      "\u001b[34m84%| | 67/80 [1:07:44<13:11, 60.90s/it]\u001b[0m\n",
      "\u001b[34m85%| | 68/80 [1:08:45<12:10, 60.90s/it]\u001b[0m\n",
      "\u001b[34m86%| | 69/80 [1:09:46<11:09, 60.90s/it]\u001b[0m\n",
      "\u001b[34m88%| | 70/80 [1:10:47<10:09, 60.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6986, 'learning_rate': 8.208341474624071e-06, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m88%| | 70/80 [1:10:47<10:09, 60.91s/it]\u001b[0m\n",
      "\u001b[34m89%| | 71/80 [1:11:48<09:08, 60.91s/it]\u001b[0m\n",
      "\u001b[34m90%| | 72/80 [1:12:48<08:07, 60.91s/it]\u001b[0m\n",
      "\u001b[34m91%|| 73/80 [1:13:49<07:06, 60.91s/it]\u001b[0m\n",
      "\u001b[34m92%|| 74/80 [1:14:50<06:05, 60.91s/it]\u001b[0m\n",
      "\u001b[34m94%|| 75/80 [1:15:51<05:04, 60.91s/it]\u001b[0m\n",
      "\u001b[34m95%|| 76/80 [1:16:52<04:03, 60.91s/it]\u001b[0m\n",
      "\u001b[34m96%|| 77/80 [1:17:53<03:02, 60.90s/it]\u001b[0m\n",
      "\u001b[34m98%|| 78/80 [1:18:54<02:01, 60.90s/it]\u001b[0m\n",
      "\u001b[34m99%|| 79/80 [1:19:55<01:00, 60.90s/it]\u001b[0m\n",
      "\u001b[34m100%|| 80/80 [1:20:56<00:00, 60.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6878, 'learning_rate': 0.0, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m100%|| 80/80 [1:20:56<00:00, 60.90s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 4857.1073, 'train_samples_per_second': 0.166, 'train_steps_per_second': 0.016, 'train_loss': 0.7948538959026337, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m100%|| 80/80 [1:20:57<00:00, 60.90s/it]\u001b[0m\n",
      "\u001b[34m100%|| 80/80 [1:20:57<00:00, 60.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:04<00:04,  4.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:06<00:00,  2.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.06s/it]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|| 1.60k/1.60k [00:00<00:00, 11.8MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|| 493k/493k [00:00<00:00, 46.8MB/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json: 100%|| 51.0/51.0 [00:00<00:00, 475kB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|| 101/101 [00:00<00:00, 1.14MB/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m2023-11-29 10:40:45,504 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-29 10:40:45,505 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-29 10:40:45,505 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-29 10:40:52 Uploading - Uploading generated training model\n",
      "2023-11-29 10:41:28 Completed - Training job completed\n",
      "Training seconds: 5522\n",
      "Billable seconds: 5522\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cb916-6469-4746-b97b-6c2ba4e96922",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7a185fae-6b17-45bd-96aa-73e005c40e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7b3444f4-9c5d-45c6-9cb6-a01b7fa21c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e457d6f3-2023-43c5-81ac-6b294aef5182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::005418323977:role/service-role/AmazonSageMaker-ExecutionRole-20231030T210397'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "66728320-2091-4c02-807c-17f110e468c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "26f0598d-6446-472b-83b0-fa786ba014fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e4e7857d-c4aa-450a-b377-57e860014a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-29-17-17-59-738/output/model/'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0863cbb-9c81-429e-bf67-3a42eb21d49d",
   "metadata": {},
   "source": [
    "#### Download model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f77dc39d-8448-4486-b99f-cf004d2c3238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize a boto3 S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "442df457-92f1-4714-80c5-5c63a8af3a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/debug-output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m s3_file_path \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m local_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_folder, s3_file_path[\u001b[38;5;28mlen\u001b[39m(s3_folder):])\n\u001b[0;32m---> 21\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(local_file_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m s3\u001b[38;5;241m.\u001b[39mdownload_file(bucket_name, s3_file_path, local_file_path)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/debug-output'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize a boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and folder details\n",
    "bucket_name = 'sagemaker-ap-south-1-005418323977'\n",
    "s3_folder = 'huggingface-qlora-teknium-OpenHermes-2--2023-11-29-05-23-47-562'\n",
    "\n",
    "# Local directory to save files\n",
    "local_folder = './hermes_full_finetuned_model/'\n",
    "\n",
    "# List objects within the specified S3 folder\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder)\n",
    "\n",
    "# Download each file in the folder\n",
    "for obj in objects.get('Contents', []):\n",
    "    s3_file_path = obj['Key']\n",
    "    local_file_path = os.path.join(local_folder, s3_file_path[len(s3_folder):])\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    s3.download_file(bucket_name, s3_file_path, local_file_path)\n",
    "    print(f'Downloaded {s3_file_path} to {local_file_path}')\n",
    "\n",
    "# Remember to replace 'your-bucket-name', 'your-folder-name/', and 'path/to/local/folder/' with your actual bucket name, S3 folder, and local folder path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0987420a-1b54-4a7f-9e9c-a6d47040dc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dadea77f-af0e-4af4-86b1-fe89bdab5460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "from huggingface_hub import HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a50e7d2-750a-4a5e-a290-e9af74556410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4404f88-4c84-44bf-9a97-3bca77d85a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39327ad-1171-49c8-9044-e7e270735244",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-req-build-fvqwwsub\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-req-build-fvqwwsub\n",
      "  Resolved https://github.com/huggingface/transformers to commit 3bc50d81e6c70d63e59d635106bac6a561b47681\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8048947 sha256=38109c4d4a4b05baf5d143f483a397e952fdded8c2a4b4dc9c36e75e3483b40d\n",
      "  Stored in directory: /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-ephem-wheel-cache-pab9gusl/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed transformers-4.36.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134d41c8-83a2-4c58-bb55-6aec92acf4a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88278256-a9cd-4662-84db-439006947434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea3aa33-dc5d-414b-92c6-fc9dd5b9237b",
   "metadata": {},
   "source": [
    "### Upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee7ff3fb-d610-4cbb-9328-ab06b8ed2c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05414f7ba934c368ebd1b36467ab559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('./hermes_finetuned_model/', local_files_only = True, torch_dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8bcbba1-e789-4e44-8d4b-e03010095a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7564bd269a534e2cabc63a556841bb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520568007bb848f68e0013ca617b4686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b4c8cc9f33444192f555c5c3ee3582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f431aa46fd455c89e1f588953abedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/WintWealth/partial_finetuned_open_hermes_2.5/commit/e48ca5f4ad711b31dfa6ea52d45a0e548e5a9adb', commit_message='Upload model', commit_description='', oid='e48ca5f4ad711b31dfa6ea52d45a0e548e5a9adb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('WintWealth/partial_finetuned_open_hermes_2.5', token = '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', private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a11fb8-3e67-4687-9583-23b9383c4fe3",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ed934755-bf78-48db-a105-7650556ea5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "# model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(3072), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "85bad177-3c5d-443c-8a96-c01483c77b26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-12-01-00-40-04-949\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-12-01-00-40-05-774\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-12-01-00-40-05-774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c25e7a3d-368a-4fe9-835b-f7cd3ee34092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = llm.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "69eeb1a5-4e6a-4665-9134-564d77f76e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-tgi-inference-2023-12-01-00-40-05-774'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0b5c259-76aa-4b58-bcb7-092213d613b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = llm.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc4dd5da-517c-4f6c-b4e8-ab416b9907e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-tgi-inference-2023-11-29-10-43-04-737'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dead63c4-0929-40e1-9888-97addf34644b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = 'huggingface-pytorch-tgi-inference-2023-11-28-06-16-30-408'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "562d8821-3da7-4c60-9f7a-b1950acceccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system_prompt = df.iloc[0].system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31d280be-9338-4ca3-90e3-a681b3038830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_article_for_prompt(article_text):\n",
    "    instruction = f\"<|im_start|>system\\n{new_system_prompt}<|im_end|>\\n\"\n",
    "    # not adding context as instruction ends with |actual_article|\n",
    "    context = f\"### <|im_start|>user\\n{article_text}|im_end|>\\n\"\n",
    "    response = f\"### <|im_start|>assistant\\n\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    prompt = re.sub(r'\\n+','\\n',prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a831fe67-6f4e-4401-9cf7-60675244c074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "parent_folder = '/Users/ravi.tej/Desktop/ML/Recommendations/arcane/'\n",
    "from hydra import compose, initialize\n",
    "import os\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('../../conf/application.run.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "envs_element = root.find('./configuration/envs')\n",
    "for variable in envs_element.findall('env'):\n",
    "    name = variable.get('name')\n",
    "    value = variable.get('value')\n",
    "    os.environ[name] = value\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ravi.tej/Desktop/ML/Recommendations/arcane/')\n",
    "\n",
    "from src._utils import load_bertopic_model_from_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d9220b-4ccf-4593-a333-58ef589b3041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.articles.ArticleService import ArticleService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acfdf3b-192f-49e6-a98f-4f4815977070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art = ArticleService._get_article_json_from_s3_and_api('652a045b50af0e25a9122fd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e30584-3114-42bb-9af4-ba832b7626f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Parasitic, blood-sucking 'alien-like' wasp found in Amazon, eats host from inside outNEW DELHI: Scientists have identified a horrifying and dangerous new species of parasitic wasp that feeds on the blood of its host and devours it from the inside out. The Daily Star reports, this alien-like insect, named Capitojoppa amazonica, was discovered in the Amazon, specifically within the Allpahuayo-Mishana National Reserve in Peru. The parasitic wasp reaches a size of approximately 1.7cm and possesses a tube-like organ that injects an egg into the host's body. It usually targets caterpillars, beetles, and spiders.The wasp was discovered as part of an extensive, ongoing research project. The research team used specialized tent-like traps to capture flying insects in the rainforest. Capitojoppa amazonica is just one out of 109 newly discovered species.Brandon Claridge from Utah State University, who is also the lead author of the study that describes Capitojoppa amazonica, explained to Live Science  Once the host is located and mounted, the female will frantically stroke it with her antennae. If acceptable, the female will deposit a single egg inside the host by piercing it with her ovipositor. The larvae of the wasp consumes the host from the inside out, after the eggs are hatched. They subsequently develop within the deceased host's body before growing into adult wasps.Later, several other wasps gather to feed on the haemolymph, a fluid similar to blood, present inside the host after the expectant mother punctures it with her tube.Claridge stated,  Females will even stab the host with the ovipositor and feed without laying an egg as it helps with gaining nutrients for egg maturation.Study co-author Ilari Sksjrvi, from the University of Turku, said,  The species biodiversity of many organisms is highest on the whole planet at Allpahuayo-Mishana.  Allpahuayo-Mishana is a part of the Amazon that has an unprecedented abundance of species, due to the region's complex geological history, he added.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art['title'] + art['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe49eea-05f0-4a19-a66d-586abb6754c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smr = sess.boto_session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff32630e-f97c-45a7-a2b8-ace8fa765707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"###\", \"</s>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9bcf0c-05d1-46ed-8c25-51f512d0762a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = '''\n",
    "World economic issues cast shadow on Indian stock market, FIIs continue to sell\n",
    "The Indian stock market is facing challenges amid a contracting global landscape. Despite high valuations, factors like high inflation, bond yields, and geopolitical tensions are affecting market sustainability.\n",
    "\n",
    " Indian stock market under pressure due to FII selling (File photo)\n",
    "Indian stock market under pressure due to FII selling (File photo)\n",
    "Its been a long journey for the Indian stock market. It has been growing well, generating superior wealth in the ongoing century. The Nifty 500, the broader equity index, has provided a decent CAGR of 13.6% over about 23 years. A one-time investment of 1 lakh in December 2000 would have been 21 lakh today. From the latest intra high of 17,754.05, dated September 12, it went down by 7.25% on October 26, and on November 3, Nifty 50 closed at 17,000.95, which is 4.25% lower. A short-term break in the long-term growing market.\n",
    "\n",
    "At the onset of the 21st century, the big picture for India was as a rising emerging market as the domestic economy had opened for world business. Initially, the focus was on infrastructure development, particularly in the areas of roads, power and realty, seen as basis fundamental to the new economy. However, it was often characterized as an elephant economy, substantial and steady, driven mainly by domestic demand, and not swiftly adapting to global opportunities. This placid perception has since shifted in the present decade, with the economy now recognized as a burgeoning force poised to become a global supply hub in the future. The multiplier effect is seen in varied sectors like Digital, Renewables, Electronics, Technology, Pharma to Chemical, while efficient working of government expenditure is also uplifting rural and domestic demand.\n",
    "\n",
    "The Indian economy & fiscal situation are as strong as they have ever been. Projections indicate a stable 6.5% YoY GDP growth from FY24 to FY26, alongside a 5.25% fiscal deficit, even amid global economic deceleration. H1FY24 corporate earnings growth has been bumper, with PAT growth of top 100 large cap estimates at 35% YoY. While no intrinsic structural issues have been identified within India, global circumstances have instigated fluctuations in the stock market, leading to currency volatility. INR has depreciated against USD, 83.270 Friday closing from 82.140 at March-end.\n",
    "\n",
    "The recent decline in the Indian stock market is predominantly driven by global factors. Notably, there is a conspicuous deceleration in the global economy, as evidenced by Europe's recession, with Germany, the region's foremost manufacturing hub, recording negative GDP growth for the past three quarters. In Asia, the engine growth of China is decelerating. Annual taker of 7% GDP growth, during pre-covid is forecast to settle to 4.5% in the future. It is leading the government to consider implementing a significant stimulus package to regain traction.\n",
    "\n",
    "Amid a contracting global landscape, two nations, the US and India, are decoupling. In 2022, the US was projected to enter a recession in the latter part of 2023, however, it managed to avert this scenario through the implementation of a comprehensive $8 trillion COVID assistance package, along with fiscal and monetary stimulus measures introduced by the government between 2020 and 2023. These initiatives had far-reaching benefits, extending support to households, states, healthcare, businesses, and other institutions. Consequently, the likelihood of a recession has now significantly diminished. However, a slowdown is forecast, the annual GDP growth is estimated to reduce from 2.3% in CY23 to 1% in CY24 due to high fiscal deficit, interest rate and quantitative tightening by the US FED.\n",
    "\n",
    "This is the primary issue of the global stock market, and the fallout of the world economy. The current global economic landscape stands in contrast to the elevated trajectory of the Indian stock market. It is becoming a challenge to hold the gains due to high FIIs selling in the last 3-4months. Even the optimistic H1 results are not supporting the market to sustain the momentum strong.\n",
    "\n",
    "Despite long-standing imbalances in the economy, including high inflation, elevated bond yields, geopolitical tensions, and supply constraints, the Indian stock market, like the main indices, has maintained a high valuation. For example, the MSCI India Index has been trading at an average one-year forward P/E of 20.5x above the long-term of 18x. Today at 19.6x, a dichotomy in context to dollar terms with elevated bond yield trading at decades high of 5%. FIIs are cautious as interest rates are expected to stay high, in-stroke to the hawkish central bank view, and economic slowdown and moderation in future earnings are warranting a consolidation in prices and valuations.\n",
    "\n",
    "The author, Vinod Nair is Head of Research at Geojit Financial Services\n",
    "\n",
    "Disclaimer: The views and recommendations made above are those of individual analysts or broking companies, and not of Mint. We advise investors to check with certified experts before taking any investment decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56c31b1b-cc2e-4e66-bb51-aa05d640d065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request = {\"inputs\": format_article_for_prompt(art.full_content), \"parameters\": parameters, \"stream\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a06151fc-7aa9-4c4b-a4b3-a6133e4d9925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21e7476-4f65-4891-b5de-88b4e58c403d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = {\"inputs\": format_article_for_prompt(content), \"parameters\": parameters, \"stream\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f4bf468-a28d-4d75-afd5-d7847bb534ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(request),\n",
    "    ContentType=\"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f6aa10-92e3-4c31-b393-29807fa29c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = resp['Body'].read()\n",
    "\n",
    "json.loads(json.loads(k)[0]['generated_text'])['summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c1184-2a04-4a59-968b-de37be3b02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6c9c5b46-d656-4174-a2e6-aaae0bf2ae89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = '''\n",
    "The article highlights the challenges faced by the Indian stock market in light of global economic issues. Despite India's strong economy and positive corporate earnings growth, foreign institutional investors (FIIs) are selling their shares due to the global economic deceleration. The contraction in the global economy, led by Europe's recession and China's decelerating economy, is affecting the Indian market. The recent fall in the stock market is attributed to global factors, not intrinsic structural issues within India. The article concludes by stating that despite the challenges, the Indian stock market has maintained a high valuation, which is leading to consolidation in prices and valuations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f76610c8-966d-42c9-a6ba-1a9b9e2c2c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3a504dc-a766-422f-b970-ddc85df8a3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sanghi Industries share price Today Live Updates : Sanghi Industries sees upward trend in trading  Mint:  On the last day Sanghi Industries stock opened at  123 and closed at  122.65. The stock reached its highest point of  123 and lowest point of  120.45 during the day. The market capitalization of the company is  3129.62 crore. The 52-week high and low for the stock are  131.9 and  51.55 respectively. The BSE volume for the stock was 6082 shares. Disclaimer: This is an AI-generated live blog and has not been edited by LiveMint staff. The current days low price of Sanghi Industries stock is  120.45 and the high price is  123. The current data for Sanghi Industries stock shows that the stock price is  122.7. There has been a 0.04 percent change in the stock price with a net change of 0.05. The current data for Sanghi Industries stock shows that the price is  121.3 with a percent change of -1.1 and a net change of -1.35. This indicates that the stock has decreased in value by 1.1% and has decreased by  1.35. The stock of Sanghi Industries reached a low of  120.45 and a high of  123 on the current day. On the last day Sanghi Industries had a trading volume of 6082 shares on the Bombay Stock Exchange (BSE). The closing price for the day was  122.65. Download the App to get 14 days of unlimited access to Mint Premium absolutely free '"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art.full_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138da96c-85c5-4475-b1d0-dc8cb99379c4",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71437896-8e59-44ae-9d73-7b3e9b5644cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoenv",
   "language": "python",
   "name": "recoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
