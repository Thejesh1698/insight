{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a0f310-7a5a-4e2b-9359-fdc7111e8130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Role name fetched using \"aws iam list-roles --query 'Roles[?contains(RoleName, `SageMaker`)]'\" in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73511d8-7e56-416e-a29d-8488101717ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: aws\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-roles --query 'Roles[?contains(RoleName, `SageMaker`)]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bc8e0f-2ba5-4e0c-8ed3-c238c3de48c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an IAM client\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# List roles\n",
    "response = iam.list_roles()\n",
    "\n",
    "# If you want to filter for SageMaker roles, you can do it in Python\n",
    "sagemaker_roles = [role for role in response['Roles'] if 'SageMaker' in role['RoleName']]\n",
    "\n",
    "# `sagemaker_roles` now contains a list of SageMaker roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bcddd2-87a2-46d9-a255-7493c713bb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AmazonSageMaker-ExecutionRole-20231030T210397'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_roles[0]['RoleName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c85d3a-5038-4664-8233-54f715e2ebfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name ravi_tej to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::005418323977:role/service-role/AmazonSageMaker-ExecutionRole-20231030T210397\n",
      "sagemaker bucket: sagemaker-ap-south-1-005418323977\n",
      "sagemaker session region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20231030T210397')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04101810-0801-4a8f-b539-f3eacab7b8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba60830-f907-4d3d-a05e-90a0581ef691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('train_set_full_27_nov.json', 'r') as f:\n",
    "    train_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76aa1b2-3afd-450e-b719-097907da563f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for art_id in train_set:\n",
    "    res = train_set[art_id]['response']\n",
    "    train_set[art_id] = {'content': train_set[art_id]['content'], 'response': {}}\n",
    "    train_set[art_id]['response']['attributes'] = res[0]\n",
    "    train_set[art_id]['response']['summaries'] = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22714b52-5ba9-410a-8491-f2f41d117d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('gpt-responses-nov-7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a438bd3b-a6cc-4a42-ac3b-d3db141a8313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe510a0-d742-44e4-8213-fe37b9cb20f3",
   "metadata": {},
   "source": [
    "<|im_start|>system\n",
    "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819457ef-10f2-41f2-b3ed-5d208c0f797d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt =  '''\n",
    "You are the chief editor for a leading Indian financial and business news website. You evaluate critical attributes of articles to gate keep content quality. For many attributes, you will first provide a brief analysis of less than 30 words, followed by assessment.\n",
    "\n",
    "1. analysis_is_financial_or_business_news (short text) : <analyse if article pertains to finance/business or not. government policies directly impacting indian corporations or investors are ok, but not if aren't>\n",
    "2. is_financial_or_business_news (1/0) : <1 or 0 based on previous attribute>\n",
    "3. analysis_of_relevant_for_india (short text) : <analyse if article is relevant for indians. for example articles about 401k or small foreign companies won't be relevant for india. however changes to fed interest rates or nasdaq or large multinational important news will be relevant>\n",
    "4. relevant_for_india (1/0) : <1 or 0 based on previous attribute>\n",
    "5. analysis_of_article_validity_duration (short text) : <Analyse relevance duration: Stock fluctuations, 1 day; significant policy changes, weeks; educational content is timeless unless it refers to any tax or other regulations in which case only 30 days. International news in India has shorter lifespan. popular topics are usually not timeless; quarterly analysis is valid for a week, yearly for a couple of weeks and a much longer one for a month>\n",
    "6. article_validity_duration (one of 1, 3, 7, 14, 30, -1) : <calculate number of days based on previous attribute. -1: timeless. 1: article is relevant only for that day. 3: for a couple of days. 7: for a week. 14: for a couple of weeks. 30: for a month>\n",
    "7. analysis_of_popularity (short text) : <analyse likely popularity of article - if its for niche audience, moderate_popularity or should be part of breaking_news section, depending on number of people who will be impacted by the news and the scale of the event. foreign entities known in india but not very popular will be mostly niche or rarely moderately popular. articles targeted to very specific business or pratices will be niche. infotainment business and financial articles with some drama are likely to be more popular. articles with a list of rules without compelling story-telling will be for niche audience>\n",
    "8. popularity (one of niche, moderately_popular, breaking_news) : <based on previous attribute>\n",
    "9. analysis_of_article_type (short text) : <analyse if the article is majorly factual, is an opinion piece, analysis, educational or likely sponsored. factual articles relay events. opinion pieces have predictions either from the author or from statements without data. analysis pieces have substantial data to justify. if an article is overly zealous on certain stock and seems like an ad, then it is sponsored>\n",
    "10. article_type (one of fact, opinion, analysis, educational, sponsored) : <based on previous attribute>\n",
    "11. analysis_of_article_sentiment (short_text): <analyse if the sentiment of the article is bullish, bearish or NA. balanced is NA>\n",
    "12. article_sentiment (one of bull, bear, NA): <based on previous attribute>\n",
    "13. summary_list (text of 60 words) : <Generate increasingly concise, entity-dense summaries. Repeat the following steps 3 times. \n",
    "Step a. Identify 1-3 informative Entities (semicolon delimited) from the Article which are missing from the previously generated summary. \n",
    "Step b. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. \n",
    "\n",
    "A Missing Entity is: \n",
    "Relevant to the main story: Specific, descriptive yet concise (5 words or fewer); \n",
    "Novel: not in the previous summary; \n",
    "Faithful: present in the Article; \n",
    "\n",
    "Guidelines: Don't refer to the article in third person, but summarize the contents. The first summary should be long (4-5 sentences, ~60 words) yet highly non-specific. Subsequently rewrite the previous summary to improve flow and make space for additional entities. Make space with compression and removal of uninformative phrases like 'the article discusses'. The summaries should become highly dense but easily understood without the Article. Never drop entities from the previous summary. If space cannot be made, add fewer new entities. Remember, use the exact same number of words for each summary. generate a list of 3 dictionaries whose keys are 'missing_entities' and 'denser_summary'. and put the entire list in double quotes (\") for safe overall json decoding>\n",
    "14. headline_suggestion (short text) : <Write a headline based on the content of the article>\n",
    "\n",
    "your response should be a list of 2 json structures with all the 14 above keys without missing any key. It is very important that the response is directly readable with json.loads(). no preamble or postamble. respond in the exact following structure:\n",
    "{\"attributes\":\n",
    "{\n",
    "\"analysis_is_financial_or_business_news\": \"\",\n",
    "\"is_financial_or_business_news\": \"\",\n",
    "\"analysis_of_relevant_for_india\": \"\",\n",
    "\"relevant_for_india\": \"\",\n",
    "\"analysis_of_article_validity_duration\": \"\",\n",
    "\"article_validity_duration\": \"\",\n",
    "\"analysis_of_popularity\": \"\",\n",
    "\"popularity\": \"\",\n",
    "\"analysis_of_article_type\": \"\",\n",
    "\"article_type\": \"\",\n",
    "\"analysis_of_article_sentiment\": \"\",\n",
    "\"article_sentiment\": \"\",\n",
    "\"headline_suggestion\": \"\"},\n",
    "\"summaries\": [{\"missing_entities\":\"\", \"denser_summary\": \"\"}, {\"missing_entities\":\"\", \"denser_summary\": \"\"}, {\"missing_entities\":\"\", \"denser_summary\": \"\"}]\n",
    "}\n",
    "\n",
    "|article|\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f82069a-65ac-4ec0-8397-bffbdb750c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(train_row):\n",
    "    instruction = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "    # not adding context as instruction ends with |actual_article|\n",
    "    context = f\"### <|im_start|>user\\n{train_row['content']}|im_end|>\\n\"\n",
    "    response = f\"### <|im_start|>assistant\\n{train_row['response']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    prompt = re.sub(r'\\n+','\\n',prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8601942e-b63c-4f8b-9769-105673556e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1675fbf1-df97-4a4b-b9af-c5d2f5228d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ec0900-ca14-4e82-8563-017cf8520d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"../scripts/utils\")\n",
    "from pack_dataset import pack_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3ef4b8-4d3e-4faf-8bf7-b561f8d46204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3e3a324-5761-47da-a5f7-cabba097b73d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['text'] = df.apply(format_prompt, axis=1)\n",
    "for key in train_set:\n",
    "    train_set[key]['text'] = format_prompt(train_set[key])\n",
    "    train_set[key]['article_id'] = key\n",
    "    train_set[key]['response'] = json.dumps(train_set[key]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d7b8d29-b7bc-4f86-bfd6-6b822822fe0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_dict(train_set).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "705ed650-a113-4414-a7e4-b39024eb4373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7e2ba02-cea3-4573-9161-068fac132a74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3bb5cb27fb40928a9ecaa63f1da8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e0127518b849b7b1e947fc6783af5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking dataset into chunks of 4096 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea79ea94e924d8aa0c81d811c6326c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 345\n",
      "Total number of samples: 345\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(template_dataset)\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=4096) # We use 2048 as the maximum length for packing\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d2d8f0a-8101-4621-93b8-dac89d702b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 345\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc531edb-7179-46f1-bdb3-07e4affa59fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-ap-south-1-005418323977'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e655928c-2388-4afc-a034-b83a3836bf96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting s3fs\n",
      "  Obtaining dependency information for s3fs from https://files.pythonhosted.org/packages/55/d1/7d6279cf80ed186a35a884fa2c22b5423611609bcca9296e47bfed0da27b/s3fs-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.7.0 (from s3fs)\n",
      "  Obtaining dependency information for aiobotocore~=2.7.0 from https://files.pythonhosted.org/packages/d0/bc/6a96a686845c9f5958fac0ecafa6050ed77d0e71553b3cfe69acbaa70191/aiobotocore-2.7.0-py3-none-any.whl.metadata\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec==2023.10.0 (from s3fs)\n",
      "  Obtaining dependency information for fsspec==2023.10.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from s3fs) (3.8.5)\n",
      "Collecting botocore<1.31.65,>=1.31.16 (from aiobotocore~=2.7.0->s3fs)\n",
      "  Obtaining dependency information for botocore<1.31.65,>=1.31.16 from https://files.pythonhosted.org/packages/d0/68/6a9c405bc6c6e7d832743a458c87f21cee393ef2cf32437a0a3a07cd0ae9/botocore-1.31.64-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.64-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore~=2.7.0->s3fs)\n",
      "  Downloading wrapt-1.15.0-cp311-cp311-macosx_11_0_arm64.whl (36 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.7.0->s3fs)\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (1.26.16)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (1.16.0)\n",
      "Downloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, fsspec, aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.73\n",
      "    Uninstalling botocore-1.31.73:\n",
      "      Successfully uninstalled botocore-1.31.73\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.28.73 requires botocore<1.32.0,>=1.31.73, but you have botocore 1.31.64 which is incompatible.\n",
      "awscli 1.29.73 requires botocore==1.31.73, but you have botocore 1.31.64 which is incompatible.\n",
      "datasets 2.14.5 requires fsspec[http]<2023.9.0,>=2023.1.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.7.0 aioitertools-0.11.0 botocore-1.31.64 fsspec-2023.10.0 s3fs-2023.10.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02d91b51-a693-47eb-b24d-3f3c715e7785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dacfdd57bf440b7ad73f3c98ed26f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-ap-south-1-005418323977/fine_tuning_datasets/OpenHermes-2.5-mistral/gpt4-samples/train-full-27-nov\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/fine_tuning_datasets/OpenHermes-2.5-mistral/gpt4-samples/train-full-27-nov'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44304c47-94f8-48db-ad95-a6a66ffb35a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 2,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 3,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': False,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"cosine\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6908d813-bb59-4f10-9b5a-79c8830e3f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}-full-27-Nov'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = './utils/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 4*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29232bcb-99fa-4c2d-adaf-90ed668da129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-11-27 14:47:22 Starting - Starting the training job...\n",
      "2023-11-27 14:47:38 Starting - Preparing the instances for training......\n",
      "2023-11-27 14:48:52 Downloading - Downloading input data\n",
      "2023-11-27 14:48:52 Training - Downloading the training image...........................\n",
      "2023-11-27 14:53:24 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:17,480 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:17,499 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:17,508 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:17,509 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:18,828 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 80.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting optimum==1.14.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl (398 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 398.9/398.9 kB 59.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 56.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 27.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 110.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.7/311.7 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 115.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 26.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, humanfriendly, huggingface-hub, coloredlogs, tokenizers, accelerate, transformers, peft, optimum\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 bitsandbytes-0.41.2.post2 coloredlogs-15.0.1 huggingface-hub-0.19.4 humanfriendly-10.0 optimum-1.14.0 peft-0.5.0 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:31,938 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:31,938 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:31,980 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:32,008 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:32,036 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:32,047 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 3,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": false,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":false,\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":false,\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-14-47-20-918/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"teknium/OpenHermes-2.5-Mistral-7B\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"3\",\"--save_strategy\",\"epoch\",\"--tf32\",\"False\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=3\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 2 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type cosine --max_grad_norm 0.3 --merge_adapters True --model_id teknium/OpenHermes-2.5-Mistral-7B --num_train_epochs 2 --output_dir /tmp/run --per_device_train_batch_size 3 --save_strategy epoch --tf32 False --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2023-11-27 14:54:32,074 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.3.5.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 72.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.3.5-cp310-cp310-linux_x86_64.whl size=56481052 sha256=229e0dd36323b56e9a91e8a5852a77e736d0258e96f484a0b612fcf29b4c1b72\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1a/b5/5d/88757297debfc54c2132aa58c582ee7915660ae4db55119b98\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.3.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_NjVkEqg...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/624 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 624/624 [00:00<00:00, 4.38MB/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 118MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 41.9M/9.94G [00:00<00:26, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|          | 83.9M/9.94G [00:00<00:37, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|▏         | 136M/9.94G [00:00<00:29, 330MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|▏         | 178M/9.94G [00:00<00:30, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|▏         | 220M/9.94G [00:00<00:28, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|▎         | 262M/9.94G [00:00<00:27, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|▎         | 304M/9.94G [00:00<00:31, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|▎         | 346M/9.94G [00:01<00:30, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|▍         | 388M/9.94G [00:01<00:33, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|▍         | 430M/9.94G [00:01<00:29, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|▍         | 482M/9.94G [00:01<00:26, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|▌         | 535M/9.94G [00:01<00:24, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|▌         | 587M/9.94G [00:01<00:23, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|▋         | 640M/9.94G [00:01<00:22, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|▋         | 692M/9.94G [00:01<00:21, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|▋         | 744M/9.94G [00:02<00:20, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|▊         | 797M/9.94G [00:02<00:25, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|▊         | 839M/9.94G [00:02<00:24, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|▉         | 881M/9.94G [00:02<00:24, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|▉         | 923M/9.94G [00:02<00:26, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|▉         | 965M/9.94G [00:02<00:24, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|█         | 1.02G/9.94G [00:02<00:22, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|█         | 1.07G/9.94G [00:02<00:21, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|█▏        | 1.12G/9.94G [00:03<00:20, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|█▏        | 1.17G/9.94G [00:03<00:20, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|█▏        | 1.23G/9.94G [00:03<00:20, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|█▎        | 1.28G/9.94G [00:03<00:21, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|█▎        | 1.33G/9.94G [00:03<00:20, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|█▍        | 1.38G/9.94G [00:03<00:19, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|█▍        | 1.44G/9.94G [00:03<00:19, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|█▍        | 1.49G/9.94G [00:03<00:19, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|█▌        | 1.54G/9.94G [00:04<00:18, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|█▌        | 1.59G/9.94G [00:04<00:18, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|█▋        | 1.65G/9.94G [00:04<00:19, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|█▋        | 1.70G/9.94G [00:04<00:22, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|█▊        | 1.75G/9.94G [00:04<00:21, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|█▊        | 1.80G/9.94G [00:04<00:20, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|█▊        | 1.86G/9.94G [00:04<00:19, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|█▉        | 1.91G/9.94G [00:04<00:20, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|█▉        | 1.95G/9.94G [00:05<00:20, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|██        | 1.99G/9.94G [00:05<00:22, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|██        | 2.03G/9.94G [00:05<00:21, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|██        | 2.09G/9.94G [00:05<00:20, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|██▏       | 2.13G/9.94G [00:05<00:22, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|██▏       | 2.17G/9.94G [00:05<00:21, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|██▏       | 2.22G/9.94G [00:05<00:19, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|██▎       | 2.26G/9.94G [00:05<00:19, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|██▎       | 2.31G/9.94G [00:06<00:19, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|██▎       | 2.36G/9.94G [00:06<00:18, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|██▍       | 2.41G/9.94G [00:06<00:17, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|██▍       | 2.46G/9.94G [00:06<00:17, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|██▌       | 2.52G/9.94G [00:06<00:17, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|██▌       | 2.57G/9.94G [00:06<00:17, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|██▋       | 2.62G/9.94G [00:06<00:16, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|██▋       | 2.67G/9.94G [00:06<00:16, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|██▋       | 2.73G/9.94G [00:06<00:16, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|██▊       | 2.78G/9.94G [00:07<00:18, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|██▊       | 2.82G/9.94G [00:07<00:19, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|██▉       | 2.86G/9.94G [00:07<00:20, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|██▉       | 2.90G/9.94G [00:07<00:21, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|██▉       | 2.95G/9.94G [00:07<00:22, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|███       | 3.00G/9.94G [00:07<00:19, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|███       | 3.04G/9.94G [00:07<00:21, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|███       | 3.09G/9.94G [00:08<00:19, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|███▏      | 3.15G/9.94G [00:08<00:17, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|███▏      | 3.20G/9.94G [00:08<00:17, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|███▎      | 3.24G/9.94G [00:08<00:18, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|███▎      | 3.28G/9.94G [00:08<00:19, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|███▎      | 3.32G/9.94G [00:08<00:18, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|███▍      | 3.38G/9.94G [00:08<00:17, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|███▍      | 3.42G/9.94G [00:09<00:18, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|███▍      | 3.47G/9.94G [00:09<00:17, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|███▌      | 3.52G/9.94G [00:09<00:16, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|███▌      | 3.58G/9.94G [00:09<00:15, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|███▋      | 3.63G/9.94G [00:09<00:15, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|███▋      | 3.68G/9.94G [00:09<00:14, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|███▊      | 3.73G/9.94G [00:09<00:17, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|███▊      | 3.77G/9.94G [00:09<00:17, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|███▊      | 3.82G/9.94G [00:10<00:24, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|███▉      | 3.86G/9.94G [00:10<00:21, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|███▉      | 3.91G/9.94G [00:10<00:19, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|███▉      | 3.95G/9.94G [00:10<00:20, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|████      | 4.00G/9.94G [00:10<00:18, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|████      | 4.05G/9.94G [00:10<00:16, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|████      | 4.09G/9.94G [00:10<00:18, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|████▏     | 4.14G/9.94G [00:11<00:16, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|████▏     | 4.19G/9.94G [00:11<00:15, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|████▎     | 4.25G/9.94G [00:11<00:14, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|████▎     | 4.30G/9.94G [00:11<00:13, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|████▎     | 4.34G/9.94G [00:11<00:13, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|████▍     | 4.38G/9.94G [00:11<00:14, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|████▍     | 4.44G/9.94G [00:11<00:14, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|████▌     | 4.48G/9.94G [00:11<00:15, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|████▌     | 4.52G/9.94G [00:12<00:15, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|████▌     | 4.56G/9.94G [00:12<00:16, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|████▋     | 4.60G/9.94G [00:12<00:16, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|████▋     | 4.65G/9.94G [00:12<00:16, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|████▋     | 4.70G/9.94G [00:12<00:14, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|████▊     | 4.75G/9.94G [00:12<00:13, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|████▊     | 4.80G/9.94G [00:12<00:12, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|████▉     | 4.85G/9.94G [00:12<00:12, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|████▉     | 4.91G/9.94G [00:13<00:11, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|████▉     | 4.96G/9.94G [00:13<00:11, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|█████     | 5.01G/9.94G [00:13<00:11, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|█████     | 5.06G/9.94G [00:13<00:11, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|█████▏    | 5.12G/9.94G [00:13<00:11, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.17G/9.94G [00:13<00:11, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.22G/9.94G [00:13<00:10, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.27G/9.94G [00:13<00:10, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|█████▎    | 5.33G/9.94G [00:14<00:11, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.37G/9.94G [00:14<00:12, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.41G/9.94G [00:14<00:12, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|█████▍    | 5.46G/9.94G [00:14<00:11, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|█████▌    | 5.51G/9.94G [00:14<00:12, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.55G/9.94G [00:14<00:13, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.59G/9.94G [00:14<00:13, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.64G/9.94G [00:15<00:12, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.68G/9.94G [00:15<00:13, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.73G/9.94G [00:15<00:12, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.77G/9.94G [00:15<00:13, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.80G/9.94G [00:15<00:13, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|█████▊    | 5.83G/9.94G [00:15<00:13, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.86G/9.94G [00:15<00:14, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.90G/9.94G [00:15<00:13, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|█████▉    | 5.96G/9.94G [00:16<00:11, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|██████    | 6.00G/9.94G [00:16<00:10, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|██████    | 6.05G/9.94G [00:16<00:10, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|██████▏   | 6.09G/9.94G [00:16<00:12, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.13G/9.94G [00:16<00:11, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.19G/9.94G [00:16<00:10, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.23G/9.94G [00:16<00:09, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.27G/9.94G [00:16<00:11, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.31G/9.94G [00:17<00:11, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|██████▍   | 6.35G/9.94G [00:17<00:12, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|██████▍   | 6.40G/9.94G [00:17<00:11, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|██████▍   | 6.44G/9.94G [00:17<00:10, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|██████▌   | 6.48G/9.94G [00:17<00:10, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.52G/9.94G [00:17<00:11, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.56G/9.94G [00:17<00:10, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.62G/9.94G [00:18<00:09, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.67G/9.94G [00:18<00:08, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.72G/9.94G [00:18<00:08, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.76G/9.94G [00:18<00:09, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|██████▊   | 6.82G/9.94G [00:18<00:08, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|██████▉   | 6.87G/9.94G [00:18<00:07, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|██████▉   | 6.92G/9.94G [00:18<00:08, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|███████   | 6.96G/9.94G [00:18<00:07, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|███████   | 7.01G/9.94G [00:19<00:07, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|███████   | 7.07G/9.94G [00:19<00:06, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.12G/9.94G [00:19<00:06, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.17G/9.94G [00:19<00:06, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.22G/9.94G [00:19<00:06, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.28G/9.94G [00:19<00:06, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|███████▎  | 7.33G/9.94G [00:19<00:06, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|███████▍  | 7.38G/9.94G [00:19<00:06, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.42G/9.94G [00:20<00:06, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|███████▌  | 7.47G/9.94G [00:20<00:10, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.51G/9.94G [00:20<00:09, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.55G/9.94G [00:20<00:08, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|███████▋  | 7.60G/9.94G [00:20<00:07, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.65G/9.94G [00:20<00:06, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.71G/9.94G [00:21<00:05, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.76G/9.94G [00:21<00:05, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|███████▊  | 7.81G/9.94G [00:21<00:05, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.85G/9.94G [00:21<00:05, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.91G/9.94G [00:21<00:05, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.95G/9.94G [00:21<00:06, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|████████  | 7.99G/9.94G [00:21<00:06, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|████████  | 8.03G/9.94G [00:21<00:05, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|████████▏ | 8.08G/9.94G [00:22<00:05, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.13G/9.94G [00:22<00:05, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.17G/9.94G [00:22<00:05, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.21G/9.94G [00:22<00:04, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.26G/9.94G [00:22<00:04, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%|████████▎ | 8.30G/9.94G [00:22<00:04, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.35G/9.94G [00:22<00:04, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.39G/9.94G [00:22<00:04, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%|████████▍ | 8.43G/9.94G [00:23<00:04, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%|████████▌ | 8.47G/9.94G [00:23<00:04, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%|████████▌ | 8.51G/9.94G [00:23<00:04, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%|████████▌ | 8.57G/9.94G [00:23<00:03, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.61G/9.94G [00:23<00:04, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.65G/9.94G [00:23<00:03, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.69G/9.94G [00:23<00:04, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.73G/9.94G [00:24<00:04, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.78G/9.94G [00:24<00:03, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%|████████▊ | 8.82G/9.94G [00:24<00:03, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%|████████▉ | 8.87G/9.94G [00:24<00:03, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%|████████▉ | 8.91G/9.94G [00:24<00:03, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%|█████████ | 8.95G/9.94G [00:24<00:03, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%|█████████ | 9.00G/9.94G [00:24<00:02, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%|█████████ | 9.05G/9.94G [00:24<00:02, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.10G/9.94G [00:25<00:02, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.15G/9.94G [00:25<00:01, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.21G/9.94G [00:25<00:01, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.26G/9.94G [00:25<00:01, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|█████████▎| 9.31G/9.94G [00:25<00:01, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.36G/9.94G [00:25<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|█████████▍| 9.41G/9.94G [00:25<00:01, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|█████████▌| 9.45G/9.94G [00:25<00:01, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|█████████▌| 9.49G/9.94G [00:26<00:01, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.53G/9.94G [00:26<00:01, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|█████████▋| 9.57G/9.94G [00:26<00:01, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.63G/9.94G [00:26<00:00, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.68G/9.94G [00:26<00:00, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.72G/9.94G [00:26<00:00, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.76G/9.94G [00:26<00:00, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|█████████▊| 9.80G/9.94G [00:26<00:00, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.85G/9.94G [00:27<00:00, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.89G/9.94G [00:27<00:00, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|█████████▉| 9.93G/9.94G [00:27<00:00, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|██████████| 9.94G/9.94G [00:27<00:00, 363MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:27<00:27, 27.86s/it]\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   1%|          | 41.9M/4.54G [00:00<00:11, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   2%|▏         | 83.9M/4.54G [00:00<00:12, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   3%|▎         | 136M/4.54G [00:00<00:11, 393MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   4%|▍         | 189M/4.54G [00:00<00:10, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   5%|▌         | 241M/4.54G [00:00<00:10, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   6%|▋         | 294M/4.54G [00:00<00:09, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   8%|▊         | 346M/4.54G [00:00<00:09, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   9%|▉         | 398M/4.54G [00:01<00:11, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  10%|▉         | 440M/4.54G [00:01<00:12, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  11%|█         | 482M/4.54G [00:01<00:11, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  12%|█▏        | 524M/4.54G [00:01<00:12, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  12%|█▏        | 566M/4.54G [00:01<00:13, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  13%|█▎        | 608M/4.54G [00:01<00:12, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  14%|█▍        | 650M/4.54G [00:01<00:13, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  15%|█▌        | 692M/4.54G [00:01<00:12, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  16%|█▋        | 744M/4.54G [00:02<00:10, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  18%|█▊        | 797M/4.54G [00:02<00:09, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  19%|█▊        | 849M/4.54G [00:02<00:09, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  20%|█▉        | 891M/4.54G [00:02<00:10, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  21%|██        | 933M/4.54G [00:02<00:15, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  21%|██▏       | 975M/4.54G [00:02<00:14, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  22%|██▏       | 1.01G/4.54G [00:03<00:14, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  23%|██▎       | 1.05G/4.54G [00:03<00:12, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  24%|██▍       | 1.10G/4.54G [00:03<00:11, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  25%|██▌       | 1.14G/4.54G [00:03<00:10, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  26%|██▋       | 1.20G/4.54G [00:03<00:09, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  27%|██▋       | 1.25G/4.54G [00:03<00:08, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  29%|██▊       | 1.30G/4.54G [00:03<00:08, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  30%|██▉       | 1.34G/4.54G [00:04<00:10, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  30%|███       | 1.38G/4.54G [00:04<00:09, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  32%|███▏      | 1.44G/4.54G [00:04<00:08, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  33%|███▎      | 1.49G/4.54G [00:04<00:08, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  34%|███▎      | 1.53G/4.54G [00:04<00:08, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  35%|███▍      | 1.57G/4.54G [00:04<00:08, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  36%|███▌      | 1.63G/4.54G [00:04<00:07, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  37%|███▋      | 1.68G/4.54G [00:04<00:07, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  38%|███▊      | 1.73G/4.54G [00:05<00:06, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  39%|███▉      | 1.78G/4.54G [00:05<00:06, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  40%|████      | 1.84G/4.54G [00:05<00:06, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  42%|████▏     | 1.89G/4.54G [00:05<00:06, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  43%|████▎     | 1.94G/4.54G [00:05<00:05, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  44%|████▍     | 1.99G/4.54G [00:05<00:05, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  45%|████▌     | 2.04G/4.54G [00:05<00:05, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  46%|████▌     | 2.10G/4.54G [00:05<00:07, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  47%|████▋     | 2.14G/4.54G [00:06<00:06, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  48%|████▊     | 2.19G/4.54G [00:06<00:06, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  49%|████▉     | 2.23G/4.54G [00:06<00:07, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  50%|█████     | 2.28G/4.54G [00:06<00:07, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  51%|█████▏    | 2.33G/4.54G [00:06<00:06, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  52%|█████▏    | 2.37G/4.54G [00:06<00:05, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  53%|█████▎    | 2.42G/4.54G [00:06<00:05, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  54%|█████▍    | 2.46G/4.54G [00:07<00:05, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  55%|█████▌    | 2.51G/4.54G [00:07<00:05, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  56%|█████▌    | 2.55G/4.54G [00:07<00:06, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  57%|█████▋    | 2.59G/4.54G [00:07<00:06, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  58%|█████▊    | 2.63G/4.54G [00:07<00:05, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  59%|█████▉    | 2.68G/4.54G [00:07<00:05, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  60%|██████    | 2.73G/4.54G [00:07<00:05, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  61%|██████    | 2.77G/4.54G [00:07<00:05, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  62%|██████▏   | 2.81G/4.54G [00:08<00:06, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  63%|██████▎   | 2.85G/4.54G [00:08<00:05, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  64%|██████▍   | 2.90G/4.54G [00:08<00:04, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  65%|██████▍   | 2.95G/4.54G [00:08<00:05, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  66%|██████▌   | 2.99G/4.54G [00:08<00:05, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  67%|██████▋   | 3.03G/4.54G [00:08<00:04, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  68%|██████▊   | 3.07G/4.54G [00:09<00:05, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  69%|██████▊   | 3.11G/4.54G [00:09<00:04, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  70%|██████▉   | 3.16G/4.54G [00:09<00:04, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  70%|███████   | 3.20G/4.54G [00:09<00:04, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  71%|███████   | 3.23G/4.54G [00:09<00:04, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  72%|███████▏  | 3.26G/4.54G [00:09<00:04, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  73%|███████▎  | 3.31G/4.54G [00:09<00:03, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  74%|███████▍  | 3.37G/4.54G [00:09<00:03, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  75%|███████▌  | 3.41G/4.54G [00:10<00:03, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  76%|███████▌  | 3.46G/4.54G [00:10<00:03, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  77%|███████▋  | 3.50G/4.54G [00:10<00:03, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  78%|███████▊  | 3.54G/4.54G [00:10<00:03, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  79%|███████▉  | 3.59G/4.54G [00:10<00:02, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  80%|████████  | 3.64G/4.54G [00:10<00:02, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  81%|████████▏ | 3.69G/4.54G [00:10<00:02, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  82%|████████▏ | 3.74G/4.54G [00:10<00:01, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  84%|████████▎ | 3.80G/4.54G [00:11<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  85%|████████▍ | 3.84G/4.54G [00:11<00:01, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  86%|████████▌ | 3.89G/4.54G [00:11<00:01, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  87%|████████▋ | 3.94G/4.54G [00:11<00:01, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  88%|████████▊ | 4.00G/4.54G [00:11<00:01, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  89%|████████▉ | 4.05G/4.54G [00:11<00:01, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  90%|█████████ | 4.10G/4.54G [00:11<00:01, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  91%|█████████▏| 4.15G/4.54G [00:11<00:00, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  93%|█████████▎| 4.20G/4.54G [00:12<00:00, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  94%|█████████▍| 4.26G/4.54G [00:12<00:00, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  95%|█████████▍| 4.31G/4.54G [00:12<00:00, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  96%|█████████▌| 4.36G/4.54G [00:12<00:00, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  97%|█████████▋| 4.41G/4.54G [00:12<00:00, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  98%|█████████▊| 4.46G/4.54G [00:12<00:00, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  99%|█████████▉| 4.50G/4.54G [00:13<00:00, 250MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|██████████| 4.54G/4.54G [00:13<00:00, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|██████████| 4.54G/4.54G [00:13<00:00, 345MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:41<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:41<00:00, 20.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 120/120 [00:00<00:00, 1.33MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['up_proj', 'k_proj', 'gate_proj', 'o_proj', 'down_proj', 'q_proj', 'v_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 83,886,080 || all params: 7,325,634,560 || trainable%: 1.1451032577852205\u001b[0m\n",
      "\u001b[34m0%|          | 0/114 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34m1%|          | 1/114 [00:35<1:07:41, 35.94s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/114 [01:11<1:06:51, 35.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/114 [01:47<1:06:11, 35.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/114 [02:23<1:05:34, 35.77s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/114 [02:58<1:04:57, 35.76s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/114 [03:34<1:04:21, 35.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 7/114 [04:10<1:03:44, 35.75s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/114 [04:46<1:03:08, 35.74s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/114 [05:21<1:02:32, 35.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/114 [05:57<1:01:57, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2004, 'learning_rate': 0.00019853538358476932, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/114 [05:57<1:01:57, 35.74s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/114 [06:33<1:01:21, 35.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 12/114 [07:09<1:00:45, 35.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 13/114 [07:44<1:00:09, 35.74s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 14/114 [08:20<59:34, 35.74s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/114 [08:56<58:58, 35.74s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/114 [09:32<58:22, 35.74s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 17/114 [10:07<57:46, 35.74s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/114 [10:43<57:10, 35.74s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/114 [11:19<56:35, 35.74s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/114 [11:54<55:59, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7389, 'learning_rate': 0.00018973984286913584, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/114 [11:54<55:59, 35.74s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 21/114 [12:30<55:23, 35.74s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 22/114 [13:06<54:47, 35.74s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 23/114 [13:42<54:12, 35.74s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 24/114 [14:17<53:36, 35.74s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/114 [14:53<53:00, 35.74s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/114 [15:29<52:25, 35.74s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 27/114 [16:05<51:49, 35.74s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 28/114 [16:40<51:13, 35.74s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 29/114 [17:16<50:37, 35.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 30/114 [17:52<50:02, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7393, 'learning_rate': 0.0001736741137876405, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 30/114 [17:52<50:02, 35.74s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 31/114 [18:28<49:26, 35.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 32/114 [19:03<48:50, 35.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 33/114 [19:39<48:14, 35.74s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 34/114 [20:15<47:39, 35.74s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 35/114 [20:51<47:03, 35.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 36/114 [21:26<46:27, 35.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 37/114 [22:02<45:51, 35.74s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 38/114 [22:38<45:16, 35.74s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 39/114 [23:14<44:40, 35.74s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 40/114 [23:49<44:04, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7742, 'learning_rate': 0.0001516397461638962, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 40/114 [23:49<44:04, 35.74s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 41/114 [24:25<43:29, 35.74s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 42/114 [25:01<42:53, 35.74s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 43/114 [25:36<42:17, 35.74s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 44/114 [26:12<41:41, 35.74s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 45/114 [26:48<41:06, 35.74s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 46/114 [27:24<40:30, 35.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 47/114 [27:59<39:54, 35.74s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 48/114 [28:35<39:18, 35.74s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 49/114 [29:11<38:43, 35.74s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 50/114 [29:47<38:07, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7029, 'learning_rate': 0.00012542183341934872, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 50/114 [29:47<38:07, 35.74s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 51/114 [30:22<37:31, 35.74s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 52/114 [30:58<36:55, 35.74s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 53/114 [31:34<36:20, 35.74s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 54/114 [32:10<35:44, 35.74s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 55/114 [32:45<35:08, 35.74s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 56/114 [33:21<34:32, 35.74s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 57/114 [33:57<33:57, 35.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 58/114 [34:33<33:30, 35.90s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 59/114 [35:09<32:51, 35.85s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 60/114 [35:45<32:14, 35.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7088, 'learning_rate': 9.71443949206304e-05, 'epoch': 1.04}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 60/114 [35:45<32:14, 35.82s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "902ddb2e-82c0-488e-885b-edc71d2e59f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:22:34 Starting - Starting the training job...\n",
      "2023-11-27 13:22:49 Starting - Preparing the instances for training......\n",
      "2023-11-27 13:24:05 Downloading - Downloading input data\n",
      "2023-11-27 13:24:05 Training - Downloading the training image..............................\n",
      "2023-11-27 13:28:51 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-27 13:29:49,223 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-27 13:29:49,242 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 13:29:49,251 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-27 13:29:49,252 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-27 13:29:50,612 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 115.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting optimum==1.14.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl (398 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 398.9/398.9 kB 63.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 56.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 30.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 108.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.7/311.7 kB 54.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 123.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 14.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.14.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.14.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, humanfriendly, huggingface-hub, coloredlogs, tokenizers, accelerate, transformers, peft, optimum\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 bitsandbytes-0.41.2.post2 coloredlogs-15.0.1 huggingface-hub-0.19.4 humanfriendly-10.0 optimum-1.14.0 peft-0.5.0 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:02,987 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:02,987 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:03,028 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:03,057 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:03,086 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:03,097 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 3,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": false,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":false,\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":false,\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-27-13-22-32-543/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"teknium/OpenHermes-2.5-Mistral-7B\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"3\",\"--save_strategy\",\"epoch\",\"--tf32\",\"False\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=3\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 2 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type cosine --max_grad_norm 0.3 --merge_adapters True --model_id teknium/OpenHermes-2.5-Mistral-7B --num_train_epochs 2 --output_dir /tmp/run --per_device_train_batch_size 3 --save_strategy epoch --tf32 False --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2023-11-27 13:30:03,125 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.3.5.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 44.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.3.5-cp310-cp310-linux_x86_64.whl size=56481052 sha256=229e0dd36323b56e9a91e8a5852a77e736d0258e96f484a0b612fcf29b4c1b72\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1a/b5/5d/88757297debfc54c2132aa58c582ee7915660ae4db55119b98\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.3.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_NjVkEqg...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/624 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 624/624 [00:00<00:00, 6.00MB/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mpytorch_model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 55.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   0%|          | 41.9M/9.94G [00:00<00:29, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|          | 83.9M/9.94G [00:00<00:31, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   1%|▏         | 136M/9.94G [00:00<00:24, 393MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|▏         | 189M/9.94G [00:00<00:24, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   2%|▏         | 231M/9.94G [00:00<00:28, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|▎         | 273M/9.94G [00:00<00:26, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   3%|▎         | 315M/9.94G [00:00<00:25, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|▎         | 367M/9.94G [00:00<00:23, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   4%|▍         | 419M/9.94G [00:01<00:22, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|▍         | 472M/9.94G [00:01<00:21, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   5%|▌         | 524M/9.94G [00:01<00:26, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|▌         | 566M/9.94G [00:01<00:26, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   6%|▌         | 619M/9.94G [00:01<00:23, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|▋         | 682M/9.94G [00:01<00:21, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   7%|▋         | 734M/9.94G [00:01<00:21, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|▊         | 786M/9.94G [00:02<00:26, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   8%|▊         | 828M/9.94G [00:02<00:26, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|▉         | 870M/9.94G [00:02<00:25, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:   9%|▉         | 923M/9.94G [00:02<00:23, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|▉         | 975M/9.94G [00:02<00:22, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  10%|█         | 1.03G/9.94G [00:02<00:20, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|█         | 1.08G/9.94G [00:02<00:21, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  11%|█▏        | 1.13G/9.94G [00:02<00:20, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|█▏        | 1.18G/9.94G [00:03<00:23, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  12%|█▏        | 1.23G/9.94G [00:03<00:23, 377MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|█▎        | 1.27G/9.94G [00:03<00:23, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  13%|█▎        | 1.31G/9.94G [00:03<00:25, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|█▎        | 1.35G/9.94G [00:03<00:27, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  14%|█▍        | 1.39G/9.94G [00:03<00:27, 314MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|█▍        | 1.45G/9.94G [00:03<00:24, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  15%|█▌        | 1.50G/9.94G [00:03<00:22, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|█▌        | 1.55G/9.94G [00:04<00:20, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|█▌        | 1.59G/9.94G [00:04<00:23, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  16%|█▋        | 1.64G/9.94G [00:04<00:22, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  17%|█▋        | 1.69G/9.94G [00:04<00:20, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|█▊        | 1.74G/9.94G [00:04<00:19, 427MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  18%|█▊        | 1.79G/9.94G [00:04<00:18, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|█▊        | 1.85G/9.94G [00:04<00:20, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|█▉        | 1.89G/9.94G [00:05<00:23, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  19%|█▉        | 1.93G/9.94G [00:05<00:24, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|█▉        | 1.98G/9.94G [00:05<00:22, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  20%|██        | 2.03G/9.94G [00:05<00:20, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  21%|██        | 2.09G/9.94G [00:05<00:19, 408MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|██▏       | 2.14G/9.94G [00:05<00:18, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  22%|██▏       | 2.19G/9.94G [00:05<00:18, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|██▎       | 2.24G/9.94G [00:05<00:17, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  23%|██▎       | 2.31G/9.94G [00:05<00:16, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|██▍       | 2.37G/9.94G [00:06<00:15, 490MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  24%|██▍       | 2.42G/9.94G [00:06<00:15, 490MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  25%|██▍       | 2.49G/9.94G [00:06<00:14, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|██▌       | 2.54G/9.94G [00:06<00:14, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  26%|██▌       | 2.59G/9.94G [00:06<00:14, 499MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|██▋       | 2.64G/9.94G [00:06<00:14, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  27%|██▋       | 2.69G/9.94G [00:06<00:14, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|██▊       | 2.75G/9.94G [00:06<00:14, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  28%|██▊       | 2.80G/9.94G [00:06<00:14, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|██▊       | 2.85G/9.94G [00:07<00:14, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  29%|██▉       | 2.90G/9.94G [00:07<00:14, 498MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|██▉       | 2.96G/9.94G [00:07<00:14, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  30%|███       | 3.02G/9.94G [00:07<00:13, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  31%|███       | 3.07G/9.94G [00:07<00:13, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|███▏      | 3.14G/9.94G [00:07<00:13, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  32%|███▏      | 3.19G/9.94G [00:07<00:13, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|███▎      | 3.24G/9.94G [00:07<00:13, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  33%|███▎      | 3.29G/9.94G [00:07<00:13, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|███▎      | 3.34G/9.94G [00:08<00:13, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  34%|███▍      | 3.41G/9.94G [00:08<00:12, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|███▍      | 3.47G/9.94G [00:08<00:12, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  35%|███▌      | 3.52G/9.94G [00:08<00:12, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  36%|███▌      | 3.58G/9.94G [00:08<00:12, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|███▋      | 3.64G/9.94G [00:08<00:12, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  37%|███▋      | 3.69G/9.94G [00:08<00:11, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|███▊      | 3.75G/9.94G [00:08<00:11, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  38%|███▊      | 3.81G/9.94G [00:08<00:12, 511MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|███▉      | 3.86G/9.94G [00:09<00:13, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  39%|███▉      | 3.91G/9.94G [00:09<00:14, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|███▉      | 3.96G/9.94G [00:09<00:14, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  40%|████      | 4.02G/9.94G [00:09<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|████      | 4.07G/9.94G [00:09<00:12, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  41%|████▏     | 4.12G/9.94G [00:09<00:12, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|████▏     | 4.17G/9.94G [00:09<00:12, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  42%|████▏     | 4.23G/9.94G [00:09<00:12, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  43%|████▎     | 4.28G/9.94G [00:10<00:14, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|████▎     | 4.33G/9.94G [00:10<00:13, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  44%|████▍     | 4.38G/9.94G [00:10<00:12, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|████▍     | 4.44G/9.94G [00:10<00:12, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  45%|████▌     | 4.49G/9.94G [00:10<00:17, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|████▌     | 4.55G/9.94G [00:10<00:15, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  46%|████▌     | 4.59G/9.94G [00:10<00:16, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|████▋     | 4.63G/9.94G [00:11<00:16, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  47%|████▋     | 4.69G/9.94G [00:11<00:14, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|████▊     | 4.74G/9.94G [00:11<00:13, 380MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  48%|████▊     | 4.79G/9.94G [00:11<00:12, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|████▊     | 4.84G/9.94G [00:11<00:12, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  49%|████▉     | 4.90G/9.94G [00:11<00:11, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|████▉     | 4.95G/9.94G [00:11<00:11, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  50%|█████     | 5.00G/9.94G [00:11<00:11, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|█████     | 5.05G/9.94G [00:12<00:11, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  51%|█████▏    | 5.11G/9.94G [00:12<00:11, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.16G/9.94G [00:12<00:10, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.21G/9.94G [00:12<00:12, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.25G/9.94G [00:12<00:13, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.30G/9.94G [00:12<00:13, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.35G/9.94G [00:12<00:12, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.39G/9.94G [00:13<00:14, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|█████▍    | 5.43G/9.94G [00:13<00:13, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  55%|█████▌    | 5.48G/9.94G [00:13<00:12, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.54G/9.94G [00:13<00:11, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.59G/9.94G [00:13<00:10, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.64G/9.94G [00:13<00:09, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.69G/9.94G [00:13<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.76G/9.94G [00:13<00:08, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|█████▊    | 5.82G/9.94G [00:13<00:08, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.87G/9.94G [00:14<00:08, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|█████▉    | 5.92G/9.94G [00:14<00:08, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  60%|██████    | 5.98G/9.94G [00:14<00:07, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|██████    | 6.03G/9.94G [00:14<00:07, 492MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  61%|██████    | 6.08G/9.94G [00:14<00:08, 472MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.14G/9.94G [00:14<00:07, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.21G/9.94G [00:14<00:07, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.27G/9.94G [00:14<00:07, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|██████▎   | 6.33G/9.94G [00:14<00:06, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  64%|██████▍   | 6.40G/9.94G [00:15<00:06, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|██████▍   | 6.46G/9.94G [00:15<00:06, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  65%|██████▌   | 6.51G/9.94G [00:15<00:06, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.57G/9.94G [00:15<00:06, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.63G/9.94G [00:15<00:06, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.68G/9.94G [00:15<00:06, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.73G/9.94G [00:15<00:06, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.78G/9.94G [00:15<00:06, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|██████▉   | 6.84G/9.94G [00:15<00:06, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  69%|██████▉   | 6.89G/9.94G [00:16<00:05, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|██████▉   | 6.94G/9.94G [00:16<00:05, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  70%|███████   | 6.99G/9.94G [00:16<00:05, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|███████   | 7.05G/9.94G [00:16<00:05, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  71%|███████▏  | 7.10G/9.94G [00:16<00:05, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.15G/9.94G [00:16<00:05, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.20G/9.94G [00:16<00:05, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.27G/9.94G [00:16<00:05, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|███████▎  | 7.32G/9.94G [00:16<00:05, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  74%|███████▍  | 7.37G/9.94G [00:16<00:05, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.42G/9.94G [00:17<00:05, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  75%|███████▌  | 7.48G/9.94G [00:17<00:06, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.54G/9.94G [00:17<00:05, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  76%|███████▋  | 7.60G/9.94G [00:17<00:05, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.67G/9.94G [00:17<00:04, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.73G/9.94G [00:17<00:04, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.78G/9.94G [00:17<00:04, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.83G/9.94G [00:17<00:04, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.89G/9.94G [00:18<00:04, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.94G/9.94G [00:18<00:04, 402MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  80%|████████  | 7.99G/9.94G [00:18<00:04, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|████████  | 8.04G/9.94G [00:18<00:04, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  81%|████████▏ | 8.10G/9.94G [00:18<00:04, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.15G/9.94G [00:18<00:03, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.21G/9.94G [00:18<00:03, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.27G/9.94G [00:18<00:03, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.34G/9.94G [00:19<00:03, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.39G/9.94G [00:19<00:03, 502MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%|████████▍ | 8.44G/9.94G [00:19<00:03, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  85%|████████▌ | 8.49G/9.94G [00:19<00:02, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%|████████▌ | 8.55G/9.94G [00:19<00:02, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  86%|████████▋ | 8.60G/9.94G [00:19<00:02, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.65G/9.94G [00:19<00:02, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.70G/9.94G [00:19<00:02, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.76G/9.94G [00:19<00:02, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%|████████▊ | 8.81G/9.94G [00:20<00:02, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  89%|████████▉ | 8.86G/9.94G [00:20<00:02, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%|████████▉ | 8.91G/9.94G [00:20<00:02, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  90%|█████████ | 8.97G/9.94G [00:20<00:02, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%|█████████ | 9.02G/9.94G [00:20<00:01, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  91%|█████████ | 9.07G/9.94G [00:20<00:01, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.12G/9.94G [00:20<00:01, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.18G/9.94G [00:20<00:01, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.23G/9.94G [00:20<00:01, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.29G/9.94G [00:21<00:01, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.34G/9.94G [00:21<00:01, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.40G/9.94G [00:21<00:01, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  95%|█████████▌| 9.45G/9.94G [00:21<00:01, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.50G/9.94G [00:21<00:01, 412MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.55G/9.94G [00:21<00:00, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.60G/9.94G [00:21<00:00, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.65G/9.94G [00:21<00:00, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.69G/9.94G [00:22<00:00, 390MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.73G/9.94G [00:22<00:00, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.78G/9.94G [00:22<00:00, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.84G/9.94G [00:22<00:00, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.89G/9.94G [00:22<00:00, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|█████████▉| 9.93G/9.94G [00:22<00:00, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00001-of-00002.bin: 100%|██████████| 9.94G/9.94G [00:22<00:00, 437MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:23<00:23, 23.25s/it]\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   1%|          | 52.4M/4.54G [00:00<00:09, 482MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   2%|▏         | 105M/4.54G [00:00<00:08, 498MB/s] #033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   3%|▎         | 157M/4.54G [00:00<00:08, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   5%|▍         | 210M/4.54G [00:00<00:08, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   6%|▌         | 262M/4.54G [00:00<00:08, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   7%|▋         | 325M/4.54G [00:00<00:08, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:   9%|▊         | 388M/4.54G [00:00<00:07, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  10%|▉         | 451M/4.54G [00:00<00:07, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  11%|█▏        | 514M/4.54G [00:01<00:07, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  13%|█▎        | 577M/4.54G [00:01<00:07, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  14%|█▍        | 640M/4.54G [00:01<00:07, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  15%|█▌        | 692M/4.54G [00:01<00:07, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  16%|█▋        | 744M/4.54G [00:01<00:07, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  18%|█▊        | 797M/4.54G [00:01<00:07, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  19%|█▉        | 860M/4.54G [00:01<00:07, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  20%|██        | 923M/4.54G [00:01<00:07, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  21%|██▏       | 975M/4.54G [00:01<00:06, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  23%|██▎       | 1.03G/4.54G [00:02<00:07, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  24%|██▍       | 1.08G/4.54G [00:02<00:06, 507MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  25%|██▍       | 1.13G/4.54G [00:02<00:06, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  26%|██▌       | 1.18G/4.54G [00:02<00:06, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  27%|██▋       | 1.24G/4.54G [00:02<00:06, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  28%|██▊       | 1.29G/4.54G [00:02<00:06, 502MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  30%|██▉       | 1.34G/4.54G [00:02<00:06, 504MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  31%|███       | 1.39G/4.54G [00:02<00:06, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  32%|███▏      | 1.45G/4.54G [00:02<00:06, 501MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  33%|███▎      | 1.51G/4.54G [00:02<00:05, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  34%|███▍      | 1.56G/4.54G [00:03<00:06, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  36%|███▌      | 1.61G/4.54G [00:03<00:06, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  37%|███▋      | 1.67G/4.54G [00:03<00:06, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  38%|███▊      | 1.72G/4.54G [00:03<00:06, 425MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  39%|███▉      | 1.77G/4.54G [00:03<00:06, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  40%|████      | 1.82G/4.54G [00:03<00:06, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  41%|████▏     | 1.88G/4.54G [00:03<00:06, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  42%|████▏     | 1.93G/4.54G [00:03<00:06, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  44%|████▍     | 1.99G/4.54G [00:04<00:05, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  45%|████▌     | 2.04G/4.54G [00:04<00:05, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  46%|████▌     | 2.10G/4.54G [00:04<00:05, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  47%|████▋     | 2.14G/4.54G [00:04<00:05, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  48%|████▊     | 2.19G/4.54G [00:04<00:05, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  49%|████▉     | 2.24G/4.54G [00:04<00:07, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  51%|█████     | 2.31G/4.54G [00:05<00:06, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  52%|█████▏    | 2.36G/4.54G [00:05<00:05, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  53%|█████▎    | 2.41G/4.54G [00:05<00:05, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  55%|█████▍    | 2.47G/4.54G [00:05<00:04, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  56%|█████▌    | 2.54G/4.54G [00:05<00:04, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  57%|█████▋    | 2.59G/4.54G [00:05<00:04, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  58%|█████▊    | 2.64G/4.54G [00:05<00:04, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  59%|█████▉    | 2.69G/4.54G [00:05<00:04, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  61%|██████    | 2.75G/4.54G [00:06<00:05, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  62%|██████▏   | 2.80G/4.54G [00:06<00:04, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  63%|██████▎   | 2.84G/4.54G [00:06<00:05, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  64%|██████▎   | 2.89G/4.54G [00:06<00:04, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  65%|██████▍   | 2.95G/4.54G [00:06<00:04, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  66%|██████▌   | 3.00G/4.54G [00:06<00:03, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  67%|██████▋   | 3.05G/4.54G [00:06<00:03, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  69%|██████▊   | 3.11G/4.54G [00:06<00:03, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  70%|██████▉   | 3.17G/4.54G [00:07<00:03, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  71%|███████   | 3.21G/4.54G [00:07<00:04, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  72%|███████▏  | 3.26G/4.54G [00:07<00:03, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  73%|███████▎  | 3.31G/4.54G [00:07<00:03, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  74%|███████▍  | 3.37G/4.54G [00:07<00:02, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  75%|███████▌  | 3.42G/4.54G [00:07<00:02, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  76%|███████▋  | 3.47G/4.54G [00:07<00:02, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  78%|███████▊  | 3.52G/4.54G [00:08<00:02, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  79%|███████▊  | 3.58G/4.54G [00:08<00:02, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  80%|███████▉  | 3.62G/4.54G [00:08<00:02, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  81%|████████  | 3.67G/4.54G [00:08<00:02, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  82%|████████▏ | 3.71G/4.54G [00:08<00:02, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  83%|████████▎ | 3.76G/4.54G [00:08<00:01, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  84%|████████▍ | 3.82G/4.54G [00:08<00:01, 422MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  85%|████████▌ | 3.87G/4.54G [00:08<00:01, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  86%|████████▋ | 3.92G/4.54G [00:09<00:01, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  88%|████████▊ | 3.97G/4.54G [00:09<00:01, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  89%|████████▊ | 4.03G/4.54G [00:09<00:01, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  90%|█████████ | 4.09G/4.54G [00:09<00:00, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  91%|█████████▏| 4.15G/4.54G [00:09<00:00, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  93%|█████████▎| 4.22G/4.54G [00:09<00:00, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  94%|█████████▍| 4.28G/4.54G [00:09<00:00, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  96%|█████████▌| 4.34G/4.54G [00:09<00:00, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  97%|█████████▋| 4.40G/4.54G [00:09<00:00, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin:  98%|█████████▊| 4.47G/4.54G [00:10<00:00, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|█████████▉| 4.53G/4.54G [00:10<00:00, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mpytorch_model-00002-of-00002.bin: 100%|██████████| 4.54G/4.54G [00:10<00:00, 441MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:34<00:00, 15.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:34<00:00, 17.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 120/120 [00:00<00:00, 1.02MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['q_proj', 'k_proj', 'up_proj', 'v_proj', 'o_proj', 'down_proj', 'gate_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 83,886,080 || all params: 7,325,634,560 || trainable%: 1.1451032577852205\u001b[0m\n",
      "\u001b[34m0%|          | 0/116 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34m1%|          | 1/116 [00:35<1:08:55, 35.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/116 [01:11<1:08:05, 35.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/116 [01:47<1:07:24, 35.79s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/116 [02:23<1:06:46, 35.77s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/116 [02:58<1:06:09, 35.76s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/116 [03:34<1:05:33, 35.76s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 7/116 [04:10<1:04:57, 35.75s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/116 [04:46<1:04:21, 35.75s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/116 [05:21<1:03:45, 35.75s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 10/116 [05:57<1:03:09, 35.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1922, 'learning_rate': 0.0001985871018518236, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m9%|▊         | 10/116 [05:57<1:03:09, 35.75s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 11/116 [06:33<1:02:33, 35.75s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 12/116 [07:09<1:01:57, 35.75s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 13/116 [07:44<1:01:21, 35.75s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 14/116 [08:20<1:00:46, 35.75s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/116 [08:56<1:00:10, 35.75s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/116 [09:32<59:34, 35.75s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 17/116 [10:07<58:58, 35.75s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/116 [10:43<58:23, 35.75s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 19/116 [11:19<57:47, 35.75s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 20/116 [11:55<57:11, 35.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7552, 'learning_rate': 0.0001900968867902419, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 20/116 [11:55<57:11, 35.75s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 21/116 [12:30<56:35, 35.75s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 22/116 [13:06<56:00, 35.74s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 23/116 [13:42<55:24, 35.74s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 24/116 [14:18<54:48, 35.74s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/116 [14:53<54:12, 35.74s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 26/116 [15:29<53:36, 35.74s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 27/116 [16:05<53:01, 35.74s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 28/116 [16:41<52:25, 35.74s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 29/116 [17:16<51:49, 35.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 30/116 [17:52<51:13, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.716, 'learning_rate': 0.00017456421648831655, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 30/116 [17:52<51:13, 35.74s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 31/116 [18:28<50:38, 35.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 32/116 [19:04<50:02, 35.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 33/116 [19:39<49:26, 35.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 34/116 [20:15<48:51, 35.74s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 35/116 [20:51<48:15, 35.74s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 36/116 [21:27<47:39, 35.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 37/116 [22:02<47:03, 35.74s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 38/116 [22:38<46:27, 35.74s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 39/116 [23:14<45:52, 35.74s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 40/116 [23:49<45:16, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.757, 'learning_rate': 0.00015320320765153367, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 40/116 [23:49<45:16, 35.74s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 41/116 [24:25<44:40, 35.74s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 42/116 [25:01<44:05, 35.74s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 43/116 [25:37<43:29, 35.74s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 44/116 [26:12<42:53, 35.74s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 45/116 [26:48<42:17, 35.74s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 46/116 [27:24<41:42, 35.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 47/116 [28:00<41:06, 35.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 48/116 [28:35<40:30, 35.74s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 49/116 [29:11<39:54, 35.74s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 50/116 [29:47<39:19, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7622, 'learning_rate': 0.00012768355114248494, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 50/116 [29:47<39:19, 35.74s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 51/116 [30:23<38:43, 35.75s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 52/116 [30:58<38:07, 35.74s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 53/116 [31:34<37:31, 35.75s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 54/116 [32:10<36:56, 35.75s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 55/116 [32:46<36:20, 35.75s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 56/116 [33:21<35:44, 35.75s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 57/116 [33:57<35:08, 35.75s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 58/116 [34:21<31:13, 32.30s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 59/116 [34:58<31:49, 33.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 60/116 [35:33<31:53, 34.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.743, 'learning_rate': 0.0001, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 60/116 [35:33<31:53, 34.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 61/116 [36:09<31:45, 34.65s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 62/116 [36:45<31:28, 34.98s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 63/116 [37:21<31:06, 35.21s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 64/116 [37:56<30:39, 35.37s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 65/116 [38:32<30:09, 35.48s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 66/116 [39:08<29:38, 35.56s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 67/116 [39:44<29:05, 35.62s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 68/116 [40:19<28:31, 35.66s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 69/116 [40:55<27:57, 35.68s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 70/116 [41:31<27:22, 35.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6809, 'learning_rate': 7.231644885751507e-05, 'epoch': 1.21}\u001b[0m\n",
      "\u001b[34m60%|██████    | 70/116 [41:31<27:22, 35.70s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 71/116 [42:07<26:47, 35.72s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 72/116 [42:42<26:11, 35.72s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 73/116 [43:18<25:36, 35.73s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 74/116 [43:54<25:00, 35.73s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 75/116 [44:30<24:25, 35.74s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 76/116 [45:05<23:49, 35.74s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 77/116 [45:41<23:13, 35.74s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 78/116 [46:17<22:38, 35.74s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 79/116 [46:53<22:02, 35.74s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 80/116 [47:28<21:26, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6135, 'learning_rate': 4.6796792348466356e-05, 'epoch': 1.38}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 80/116 [47:28<21:26, 35.74s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 81/116 [48:04<20:51, 35.74s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 82/116 [48:40<20:15, 35.74s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 83/116 [49:16<19:39, 35.74s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 84/116 [49:51<19:03, 35.74s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 85/116 [50:27<18:28, 35.74s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 86/116 [51:03<17:52, 35.74s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 87/116 [51:39<17:16, 35.74s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 88/116 [52:14<16:40, 35.74s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 89/116 [52:50<16:05, 35.74s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 90/116 [53:26<15:29, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.622, 'learning_rate': 2.5435783511683443e-05, 'epoch': 1.55}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 90/116 [53:26<15:29, 35.74s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 91/116 [54:02<14:53, 35.74s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 92/116 [54:37<14:17, 35.74s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 93/116 [55:13<13:42, 35.74s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 94/116 [55:49<13:06, 35.74s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 95/116 [56:25<12:30, 35.74s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 96/116 [57:00<11:54, 35.74s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 97/116 [57:36<11:19, 35.74s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 98/116 [58:12<10:43, 35.74s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 99/116 [58:48<10:07, 35.74s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 100/116 [59:23<09:31, 35.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5491, 'learning_rate': 9.903113209758096e-06, 'epoch': 1.72}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 100/116 [59:23<09:31, 35.74s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 101/116 [59:59<08:56, 35.74s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 102/116 [1:00:35<08:20, 35.74s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 103/116 [1:01:10<07:44, 35.74s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 104/116 [1:01:46<07:08, 35.74s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 105/116 [1:02:22<06:33, 35.74s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 106/116 [1:02:58<05:57, 35.74s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 107/116 [1:03:33<05:21, 35.74s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 108/116 [1:04:09<04:45, 35.74s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 109/116 [1:04:45<04:10, 35.75s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 110/116 [1:05:21<03:34, 35.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5976, 'learning_rate': 1.4128981481764115e-06, 'epoch': 1.9}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 110/116 [1:05:21<03:34, 35.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 111/116 [1:05:56<02:58, 35.75s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 112/116 [1:06:32<02:22, 35.75s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 113/116 [1:07:08<01:47, 35.75s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 114/116 [1:07:44<01:11, 35.75s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 115/116 [1:08:19<00:35, 35.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 116/116 [1:08:44<00:00, 32.30s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 4124.7526, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.028, 'train_loss': 0.7168279351859257, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 116/116 [1:08:44<00:00, 32.30s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 116/116 [1:08:44<00:00, 35.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 14.9MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 141MB/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 563kB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|██████████| 101/101 [00:00<00:00, 987kB/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:43:36,227 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:43:36,227 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-27 14:43:36,227 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-27 14:43:43 Uploading - Uploading generated training model\n",
      "2023-11-27 14:44:14 Completed - Training job completed\n",
      "Training seconds: 4829\n",
      "Billable seconds: 4829\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cb916-6469-4746-b97b-6c2ba4e96922",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a185fae-6b17-45bd-96aa-73e005c40e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7b3444f4-9c5d-45c6-9cb6-a01b7fa21c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e457d6f3-2023-43c5-81ac-6b294aef5182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::005418323977:role/service-role/AmazonSageMaker-ExecutionRole-20231030T210397'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "26f0598d-6446-472b-83b0-fa786ba014fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HF_MODEL_ID': '/opt/ml/model',\n",
       " 'SM_NUM_GPUS': '1',\n",
       " 'MAX_INPUT_LENGTH': '4096',\n",
       " 'MAX_TOTAL_TOKENS': '6144'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4e7857d-c4aa-450a-b377-57e860014a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2023-11-23-09-31-02-129/output/model/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0863cbb-9c81-429e-bf67-3a42eb21d49d",
   "metadata": {},
   "source": [
    "#### Download model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "442df457-92f1-4714-80c5-5c63a8af3a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/config.json to ./model/config.json\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/generation_config.json to ./model/generation_config.json\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00001-of-00008.safetensors to ./model/model-00001-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00002-of-00008.safetensors to ./model/model-00002-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00003-of-00008.safetensors to ./model/model-00003-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00004-of-00008.safetensors to ./model/model-00004-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00005-of-00008.safetensors to ./model/model-00005-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00006-of-00008.safetensors to ./model/model-00006-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00007-of-00008.safetensors to ./model/model-00007-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model-00008-of-00008.safetensors to ./model/model-00008-of-00008.safetensors\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/model.safetensors.index.json to ./model/model.safetensors.index.json\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/special_tokens_map.json to ./model/special_tokens_map.json\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/tokenizer.json to ./model/tokenizer.json\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/tokenizer.model to ./model/tokenizer.model\n",
      "Downloaded huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/tokenizer_config.json to ./model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize a boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and folder details\n",
    "bucket_name = 'sagemaker-ap-south-1-005418323977'\n",
    "s3_folder = 'huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/output/model/'\n",
    "\n",
    "# Local directory to save files\n",
    "local_folder = './model/'\n",
    "\n",
    "# List objects within the specified S3 folder\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder)\n",
    "\n",
    "# Download each file in the folder\n",
    "for obj in objects.get('Contents', []):\n",
    "    s3_file_path = obj['Key']\n",
    "    local_file_path = os.path.join(local_folder, s3_file_path[len(s3_folder):])\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    s3.download_file(bucket_name, s3_file_path, local_file_path)\n",
    "    print(f'Downloaded {s3_file_path} to {local_file_path}')\n",
    "\n",
    "# Remember to replace 'your-bucket-name', 'your-folder-name/', and 'path/to/local/folder/' with your actual bucket name, S3 folder, and local folder path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0987420a-1b54-4a7f-9e9c-a6d47040dc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadea77f-af0e-4af4-86b1-fe89bdab5460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "from huggingface_hub import HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a50e7d2-750a-4a5e-a290-e9af74556410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4404f88-4c84-44bf-9a97-3bca77d85a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39327ad-1171-49c8-9044-e7e270735244",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-req-build-fvqwwsub\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-req-build-fvqwwsub\n",
      "  Resolved https://github.com/huggingface/transformers to commit 3bc50d81e6c70d63e59d635106bac6a561b47681\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8048947 sha256=38109c4d4a4b05baf5d143f483a397e952fdded8c2a4b4dc9c36e75e3483b40d\n",
      "  Stored in directory: /private/var/folders/d4/cgyr_gnj7nn2wy_hq40gkq8c0000gq/T/pip-ephem-wheel-cache-pab9gusl/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed transformers-4.36.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134d41c8-83a2-4c58-bb55-6aec92acf4a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ravi.tej/anaconda3/envs/bertopicenv/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7ff3fb-d610-4cbb-9328-ab06b8ed2c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0976409d65ad45eeba97e56e4f2aab9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('./model', local_files_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bcbba1-e789-4e44-8d4b-e03010095a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494aaff271bc4396935d0fa6955a3126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629ff691724d44fc829fa794cc643692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a405d8d4b71421caae3415ee4809811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a2d40c6f274c228f0f9f14fc4803fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6eff990f74252b44a05dbb8521a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b58e4893324eb0907f188b940ae4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ccadd3a1d94aa6849b7f33ba0a0f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/WintWealth/sample_finetuned_zephyr_mistral/commit/5b9572a46a658b9c8d4d830c5ab885fbbc0eee6f', commit_message='Upload model', commit_description='', oid='5b9572a46a658b9c8d4d830c5ab885fbbc0eee6f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('WintWealth/sample_finetuned_zephyr_mistral', token = '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', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed934755-bf78-48db-a105-7650556ea5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "# model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_s3_path = 's3://sagemaker-ap-south-1-005418323977/huggingface-qlora-HuggingFaceH4-zephyr--2023-11-04-17-07-27-613/'\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(7000), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(8000), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85bad177-3c5d-443c-8a96-c01483c77b26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-11-23-12-03-20-094\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-11-23-12-03-20-985\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-11-23-12-03-20-985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2023-11-23-12-03-20-985: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deploy model to an endpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm_model\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m      4\u001b[0m   initial_instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m   instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m      6\u001b[0m   container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mhealth_check_timeout, \u001b[38;5;66;03m# 10 minutes to be able to load the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/recoenv/lib/python3.11/site-packages/sagemaker/huggingface/model.py:311\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     inference_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneuron\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.inf1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneuronx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserving_image_uri(\n\u001b[1;32m    306\u001b[0m         region_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mregion_name,\n\u001b[1;32m    307\u001b[0m         instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m    308\u001b[0m         inference_tool\u001b[38;5;241m=\u001b[39minference_tool,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(HuggingFaceModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m    312\u001b[0m     initial_instance_count,\n\u001b[1;32m    313\u001b[0m     instance_type,\n\u001b[1;32m    314\u001b[0m     serializer,\n\u001b[1;32m    315\u001b[0m     deserializer,\n\u001b[1;32m    316\u001b[0m     accelerator_type,\n\u001b[1;32m    317\u001b[0m     endpoint_name,\n\u001b[1;32m    318\u001b[0m     tags,\n\u001b[1;32m    319\u001b[0m     kms_key,\n\u001b[1;32m    320\u001b[0m     wait,\n\u001b[1;32m    321\u001b[0m     data_capture_config,\n\u001b[1;32m    322\u001b[0m     async_inference_config,\n\u001b[1;32m    323\u001b[0m     serverless_inference_config,\n\u001b[1;32m    324\u001b[0m     volume_size\u001b[38;5;241m=\u001b[39mvolume_size,\n\u001b[1;32m    325\u001b[0m     model_data_download_timeout\u001b[38;5;241m=\u001b[39mmodel_data_download_timeout,\n\u001b[1;32m    326\u001b[0m     container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m    327\u001b[0m     inference_recommendation_id\u001b[38;5;241m=\u001b[39minference_recommendation_id,\n\u001b[1;32m    328\u001b[0m     explainer_config\u001b[38;5;241m=\u001b[39mexplainer_config,\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/recoenv/lib/python3.11/site-packages/sagemaker/model.py:1495\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1493\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mendpoint_from_production_variants(\n\u001b[1;32m   1496\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name,\n\u001b[1;32m   1497\u001b[0m     production_variants\u001b[38;5;241m=\u001b[39m[production_variant],\n\u001b[1;32m   1498\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m   1499\u001b[0m     kms_key\u001b[38;5;241m=\u001b[39mkms_key,\n\u001b[1;32m   1500\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m   1501\u001b[0m     data_capture_config_dict\u001b[38;5;241m=\u001b[39mdata_capture_config_dict,\n\u001b[1;32m   1502\u001b[0m     explainer_config_dict\u001b[38;5;241m=\u001b[39mexplainer_config_dict,\n\u001b[1;32m   1503\u001b[0m     async_inference_config_dict\u001b[38;5;241m=\u001b[39masync_inference_config_dict,\n\u001b[1;32m   1504\u001b[0m )\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1507\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/envs/recoenv/lib/python3.11/site-packages/sagemaker/session.py:4851\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4848\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   4849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   4852\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mname, config_name\u001b[38;5;241m=\u001b[39mname, tags\u001b[38;5;241m=\u001b[39mendpoint_tags, wait\u001b[38;5;241m=\u001b[39mwait\n\u001b[1;32m   4853\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/recoenv/lib/python3.11/site-packages/sagemaker/session.py:4196\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   4192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   4193\u001b[0m     EndpointName\u001b[38;5;241m=\u001b[39mendpoint_name, EndpointConfigName\u001b[38;5;241m=\u001b[39mconfig_name, Tags\u001b[38;5;241m=\u001b[39mtags\n\u001b[1;32m   4194\u001b[0m )\n\u001b[1;32m   4195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_endpoint(endpoint_name)\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/anaconda3/envs/recoenv/lib/python3.11/site-packages/sagemaker/session.py:4548\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   4543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   4544\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4545\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4546\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4547\u001b[0m         )\n\u001b[0;32m-> 4548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4549\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4550\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4551\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4552\u001b[0m     )\n\u001b[1;32m   4553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-11-23-12-03-20-985: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b0b5c259-76aa-4b58-bcb7-092213d613b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = llm.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "562d8821-3da7-4c60-9f7a-b1950acceccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = df.iloc[0].system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31d280be-9338-4ca3-90e3-a681b3038830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_article_for_prompt(article_text):\n",
    "    instruction = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "    # not adding context as instruction ends with |actual_article|\n",
    "    context = f\"### <|im_start|>user\\n{article_text}|im_end|>\\n\"\n",
    "    response = f\"### <|im_start|>assistant\\n\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    prompt = re.sub(r'\\n+','\\n',prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a831fe67-6f4e-4401-9cf7-60675244c074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/ravi.tej/anaconda3/envs/recoenv/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "parent_folder = '/Users/ravi.tej/Desktop/ML/Recommendations/arcane/'\n",
    "from hydra import compose, initialize\n",
    "import os\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('../../conf/application.run.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "envs_element = root.find('./configuration/envs')\n",
    "for variable in envs_element.findall('env'):\n",
    "    name = variable.get('name')\n",
    "    value = variable.get('value')\n",
    "    os.environ[name] = value\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ravi.tej/Desktop/ML/Recommendations/arcane/')\n",
    "\n",
    "from src._utils import load_bertopic_model_from_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69d9220b-4ccf-4593-a333-58ef589b3041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.articles.ArticleService import ArticleService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5acfdf3b-192f-49e6-a98f-4f4815977070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art = ArticleService.get_Article('652a045b50af0e25a9122fd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bf1cec43-2084-4486-96f2-6ccae1617c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parasitic blood-sucking alien-like wasp found in Amazon eats host from inside out: NEW DELHI: Scientists have identified a horrifying and dangerous new species of parasitic wasp that feeds on the blood of its host and devours it from the inside out. The Daily Star reports this alien-like insect named Capitojoppa amazonica was discovered in the Amazon specifically within the Allpahuayo-Mishana National Reserve in Peru. The parasitic wasp reaches a size of approximately 1.7cm and possesses a tube-like organ that injects an egg into the hosts body. It usually targets caterpillars beetles and spiders.The wasp was discovered as part of an extensive ongoing research project. The research team used specialized tent-like traps to capture flying insects in the rainforest. Capitojoppa amazonica is just one out of 109 newly discovered species.Brandon Claridge from Utah State University who is also the lead author of the study that describes Capitojoppa amazonica explained to Live Science  Once the host is located and mounted the female will frantically stroke it with her antennae. If acceptable the female will deposit a single egg inside the host by piercing it with her ovipositor. The larvae of the wasp consumes the host from the inside out after the eggs are hatched. They subsequently develop within the deceased hosts body before growing into adult wasps.Later several other wasps gather to feed on the haemolymph a fluid similar to blood present inside the host after the expectant mother punctures it with her tube.Claridge stated  Females will even stab the host with the ovipositor and feed without laying an egg as it helps with gaining nutrients for egg maturation.Study co-author Ilari Sksjrvi from the University of Turku said  The species biodiversity of many organisms is highest on the whole planet at Allpahuayo-Mishana.  Allpahuayo-Mishana is a part of the Amazon that has an unprecedented abundance of species due to the regions complex geological history he added.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art.full_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8fe49eea-05f0-4a19-a66d-586abb6754c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smr = sess.boto_session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ff32630e-f97c-45a7-a2b8-ace8fa765707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"###\", \"</s>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ee9bcf0c-05d1-46ed-8c25-51f512d0762a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = '''\n",
    "World economic issues cast shadow on Indian stock market, FIIs continue to sell\n",
    "The Indian stock market is facing challenges amid a contracting global landscape. Despite high valuations, factors like high inflation, bond yields, and geopolitical tensions are affecting market sustainability.\n",
    "\n",
    " Indian stock market under pressure due to FII selling (File photo)\n",
    "Indian stock market under pressure due to FII selling (File photo)\n",
    "It’s been a long journey for the Indian stock market. It has been growing well, generating superior wealth in the ongoing century. The Nifty 500, the broader equity index, has provided a decent CAGR of 13.6% over about 23 years. A one-time investment of ₹1 lakh in December 2000 would have been ₹21 lakh today. From the latest intra high of 17,754.05, dated September 12, it went down by 7.25% on October 26, and on November 3, Nifty 50 closed at 17,000.95, which is 4.25% lower. A short-term break in the long-term growing market.\n",
    "\n",
    "At the onset of the 21st century, the big picture for India was as a rising emerging market as the domestic economy had opened for world business. Initially, the focus was on infrastructure development, particularly in the areas of roads, power and realty, seen as basis fundamental to the new economy. However, it was often characterized as an elephant economy, substantial and steady, driven mainly by domestic demand, and not swiftly adapting to global opportunities. This placid perception has since shifted in the present decade, with the economy now recognized as a burgeoning force poised to become a global supply hub in the future. The multiplier effect is seen in varied sectors like Digital, Renewables, Electronics, Technology, Pharma to Chemical, while efficient working of government expenditure is also uplifting rural and domestic demand.\n",
    "\n",
    "The Indian economy & fiscal situation are as strong as they have ever been. Projections indicate a stable 6.5% YoY GDP growth from FY24 to FY26, alongside a 5.25% fiscal deficit, even amid global economic deceleration. H1FY24 corporate earnings growth has been bumper, with PAT growth of top 100 large cap estimates at 35% YoY. While no intrinsic structural issues have been identified within India, global circumstances have instigated fluctuations in the stock market, leading to currency volatility. INR has depreciated against USD, 83.270 Friday closing from 82.140 at March-end.\n",
    "\n",
    "The recent decline in the Indian stock market is predominantly driven by global factors. Notably, there is a conspicuous deceleration in the global economy, as evidenced by Europe's recession, with Germany, the region's foremost manufacturing hub, recording negative GDP growth for the past three quarters. In Asia, the engine growth of China is decelerating. Annual taker of 7% GDP growth, during pre-covid is forecast to settle to 4.5% in the future. It is leading the government to consider implementing a significant stimulus package to regain traction.\n",
    "\n",
    "Amid a contracting global landscape, two nations, the US and India, are decoupling. In 2022, the US was projected to enter a recession in the latter part of 2023, however, it managed to avert this scenario through the implementation of a comprehensive $8 trillion COVID assistance package, along with fiscal and monetary stimulus measures introduced by the government between 2020 and 2023. These initiatives had far-reaching benefits, extending support to households, states, healthcare, businesses, and other institutions. Consequently, the likelihood of a recession has now significantly diminished. However, a slowdown is forecast, the annual GDP growth is estimated to reduce from 2.3% in CY23 to 1% in CY24 due to high fiscal deficit, interest rate and quantitative tightening by the US FED.\n",
    "\n",
    "This is the primary issue of the global stock market, and the fallout of the world economy. The current global economic landscape stands in contrast to the elevated trajectory of the Indian stock market. It is becoming a challenge to hold the gains due to high FIIs selling in the last 3-4months. Even the optimistic H1 results are not supporting the market to sustain the momentum strong.\n",
    "\n",
    "Despite long-standing imbalances in the economy, including high inflation, elevated bond yields, geopolitical tensions, and supply constraints, the Indian stock market, like the main indices, has maintained a high valuation. For example, the MSCI India Index has been trading at an average one-year forward P/E of 20.5x above the long-term of 18x. Today at 19.6x, a dichotomy in context to dollar terms with elevated bond yield trading at decades high of 5%. FIIs are cautious as interest rates are expected to stay high, in-stroke to the hawkish central bank view, and economic slowdown and moderation in future earnings are warranting a consolidation in prices and valuations.\n",
    "\n",
    "The author, Vinod Nair is Head of Research at Geojit Financial Services\n",
    "\n",
    "Disclaimer: The views and recommendations made above are those of individual analysts or broking companies, and not of Mint. We advise investors to check with certified experts before taking any investment decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56c31b1b-cc2e-4e66-bb51-aa05d640d065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request = {\"inputs\": format_article_for_prompt(art.full_content), \"parameters\": parameters, \"stream\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b21e7476-4f65-4891-b5de-88b4e58c403d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = {\"inputs\": format_article_for_prompt(art.full_content), \"parameters\": parameters, \"stream\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0f4bf468-a28d-4d75-afd5-d7847bb534ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(request),\n",
    "    ContentType=\"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e6199459-2ab6-43c0-a078-740d20893711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = resp['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "89930b4e-4630-4a99-a45d-06806f371b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_of_is_financial_news_or_not': 'Article is about a discovery in entomology, not finance.',\n",
       " 'assessment_is_financial_news': '0',\n",
       " 'evaluation_of_relevant_for_india': 'Although the discovery is globally relevant, it does not have relevance for Indian finance.',\n",
       " 'assessment_relevant_for_india': '0',\n",
       " 'evaluation_of_is_article_safe': \"The article is safe as it doesn't contain harmful, hateful, or biased content. It's a scientific discovery report.\",\n",
       " 'assessment_is_article_safe': '1',\n",
       " 'evaluation_of_article_validity_duration': 'The article is likely to remain relevant for a short period of time due to its nature as a discovery report.',\n",
       " 'assessment_article_validity_duration': '3',\n",
       " 'evaluation_article_has_advanced_concepts': \"The article uses some advanced entomological terms, such as 'ovipositor' and 'haemolymph', which may be unfamiliar to a general reader.\",\n",
       " 'assessment_article_has_advanced_concepts': '1',\n",
       " 'evaluation_of_likely_popularity': 'Given the unusual nature of the discovery and the potential for shock value, the article is likely to generate interest among a wide audience.',\n",
       " 'assessment_likely_popularity': '3',\n",
       " 'evaluation_of_article_type': 'The article is a factual report about a scientific discovery.',\n",
       " 'assessment_article_type': 'factual',\n",
       " 'summary': 'Scientists have identified a new species of parasitic wasp, Capitojoppa amazonica, found in the Amazon rainforest. This wasp feeds on the blood of its host and devours it from the inside out. The wasp is approximately 1.7cm in size and targets caterpillars, beetles, and spiders. The discovery was made during an ongoing research project, using specialized traps to capture flying insects.',\n",
       " 'headline_suggestion': \"New Parasitic Wasp Discovered in Amazon Feeds on Host's Blood\"}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json.loads(k)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c1184-2a04-4a59-968b-de37be3b02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6c9c5b46-d656-4174-a2e6-aaae0bf2ae89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = '''\n",
    "The article highlights the challenges faced by the Indian stock market in light of global economic issues. Despite India's strong economy and positive corporate earnings growth, foreign institutional investors (FIIs) are selling their shares due to the global economic deceleration. The contraction in the global economy, led by Europe's recession and China's decelerating economy, is affecting the Indian market. The recent fall in the stock market is attributed to global factors, not intrinsic structural issues within India. The article concludes by stating that despite the challenges, the Indian stock market has maintained a high valuation, which is leading to consolidation in prices and valuations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f76610c8-966d-42c9-a6ba-1a9b9e2c2c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3a504dc-a766-422f-b970-ddc85df8a3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sanghi Industries share price Today Live Updates : Sanghi Industries sees upward trend in trading  Mint:  On the last day Sanghi Industries stock opened at ₹ 123 and closed at ₹ 122.65. The stock reached its highest point of ₹ 123 and lowest point of ₹ 120.45 during the day. The market capitalization of the company is ₹ 3129.62 crore. The 52-week high and low for the stock are ₹ 131.9 and ₹ 51.55 respectively. The BSE volume for the stock was 6082 shares. Disclaimer: This is an AI-generated live blog and has not been edited by LiveMint staff. The current days low price of Sanghi Industries stock is ₹ 120.45 and the high price is ₹ 123. The current data for Sanghi Industries stock shows that the stock price is ₹ 122.7. There has been a 0.04 percent change in the stock price with a net change of 0.05. The current data for Sanghi Industries stock shows that the price is ₹ 121.3 with a percent change of -1.1 and a net change of -1.35. This indicates that the stock has decreased in value by 1.1% and has decreased by ₹ 1.35. The stock of Sanghi Industries reached a low of ₹ 120.45 and a high of ₹ 123 on the current day. On the last day Sanghi Industries had a trading volume of 6082 shares on the Bombay Stock Exchange (BSE). The closing price for the day was ₹ 122.65. Download the App to get 14 days of unlimited access to Mint Premium absolutely free '"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art.full_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8cc78-94f6-4706-83a9-ec1ef15efc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoenv",
   "language": "python",
   "name": "recoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
