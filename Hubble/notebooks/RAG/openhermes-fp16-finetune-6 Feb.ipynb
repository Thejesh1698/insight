{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7772f89a-518d-4cd8-aef7-d74adda8d329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ravi.tej/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name ravi_tej to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::005418323977:role/service-role/AmazonSageMaker-ExecutionRole-20231030T210397\n",
      "sagemaker bucket: sagemaker-ap-south-1-005418323977\n",
      "sagemaker session region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20231030T210397')['Role']['Arn']\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc1aee4-be38-49d2-86de-408c8f90fd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from random import randint\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from pack_dataset import pack_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28394ead-b1db-445b-90a4-4dfcd6f000e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'teknium/OpenHermes-2.5-Mistral-7B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933478db-3707-4c13-b9ae-cc0e3ce5b050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 2,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 3,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 4,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"cosine_with_restarts\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run'                         # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78ad92e-b94a-4b8e-8635-23e9b0f95323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b65cd63-f97e-4345-a1a8-03ac3d9a9222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_id = 'MangoPurpleValley'\n",
    "finetune_dataset_config = {'finetune_id': 'search_openhermes_fp16_' + finetune_id,\n",
    "                          'date': datetime.strftime(datetime.today(),'%Y-%m-%d'),\n",
    "                          'num_datapoints': 2480,\n",
    "                            'data_source': 'gpt4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923f7727-6d17-4e13-81ee-263ff4ef330b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}-{finetune_dataset_config[\"finetune_id\"]}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora-original.py',    # train script\n",
    "    source_dir           = '../utils/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 10*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 50,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6841ca87-6adc-4a94-95e7-c6b61254adbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_input_path = 's3://sagemaker-ap-south-1-005418323977/fine_tuning_datasets/2024-02-06-search_openhermes_fp16_MangoPurpleValley'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2efc4ed3-24a9-483b-96e8-81ff06cec50e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:57:43 Starting - Starting the training job...\n",
      "2024-02-06 11:57:59 Starting - Preparing the instances for training......\n",
      "2024-02-06 11:58:56 Downloading - Downloading input data...\n",
      "2024-02-06 11:59:20 Downloading - Downloading the training image..................\n",
      "2024-02-06 12:02:26 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:30,856 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:30,881 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:30,890 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:30,892 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:32,188 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.5/123.5 kB 8.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting optimum==1.14.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.15.0 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting auto-gptq (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 8.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.14.0->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0->-r requirements.txt (line 9)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r requirements.txt (line 9)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r requirements.txt (line 5)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq->-r requirements.txt (line 10)) (0.1.99)\u001b[0m\n",
      "\u001b[34mCollecting rouge (from auto-gptq->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting gekko (from auto-gptq->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 120.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 9)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements.txt (line 1)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0->-r requirements.txt (line 3)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r requirements.txt (line 9)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r requirements.txt (line 9)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r requirements.txt (line 9)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq->-r requirements.txt (line 10)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.14.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 123.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.14.0-py3-none-any.whl (398 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 398.9/398.9 kB 58.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 38.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 60.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.0/105.0 MB 30.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 92.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 120.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 122.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, rouge, humanfriendly, gekko, coloredlogs, bitsandbytes, tokenizers, accelerate, transformers, datasets, peft, optimum, auto-gptq\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 auto-gptq-0.6.0 bitsandbytes-0.42.0 coloredlogs-15.0.1 datasets-2.15.0 gekko-1.0.6 humanfriendly-10.0 optimum-1.14.0 peft-0.5.0 rouge-1.0.1 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.35.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,834 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,834 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,877 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,912 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,946 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,956 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 3,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora-original\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora-original.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine_with_restarts\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora-original.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora-original\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"cosine_with_restarts\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"teknium/OpenHermes-2.5-Mistral-7B\",\"num_train_epochs\":2,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":3,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora-original\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora-original.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"cosine_with_restarts\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"teknium/OpenHermes-2.5-Mistral-7B\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"3\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine_with_restarts\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=3\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora-original.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 4 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type cosine_with_restarts --max_grad_norm 0.3 --merge_adapters True --model_id teknium/OpenHermes-2.5-Mistral-7B --num_train_epochs 2 --output_dir /tmp/run --per_device_train_batch_size 3 --save_strategy epoch --tf32 True --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-02-06 12:03:45,985 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.4.2\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.4.2.tar.gz (2.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 53.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.4.2) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.4.2) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.4.2) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.4.2) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.4.2) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.4.2) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.4.2-cp310-cp310-linux_x86_64.whl size=113930372 sha256=2c7ddc942e0715ef4a7ab62e3404b519a7ac040b3b6eae8fedcdc08a36ced786\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9d/cf/7f/d14555553b5b30698dae0a4159fdd058157e7021cec565ecaa\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.4.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_NjVkEqg...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/624 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 624/624 [00:00<00:00, 5.24MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 131MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 52.4M/9.94G [00:00<00:19, 502MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 105M/9.94G [00:00<00:20, 472MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 168M/9.94G [00:00<00:19, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 231M/9.94G [00:00<00:18, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 283M/9.94G [00:00<00:18, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 336M/9.94G [00:00<00:19, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 388M/9.94G [00:00<00:19, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 451M/9.94G [00:00<00:18, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 514M/9.94G [00:01<00:18, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 566M/9.94G [00:01<00:18, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▋         | 629M/9.94G [00:01<00:17, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 692M/9.94G [00:01<00:17, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 755M/9.94G [00:01<00:16, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 818M/9.94G [00:01<00:16, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 881M/9.94G [00:01<00:16, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 944M/9.94G [00:01<00:17, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|█         | 1.01G/9.94G [00:01<00:16, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.07G/9.94G [00:02<00:16, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█▏        | 1.13G/9.94G [00:02<00:16, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.20G/9.94G [00:02<00:16, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.94G [00:02<00:16, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.32G/9.94G [00:02<00:15, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.38G/9.94G [00:02<00:15, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.45G/9.94G [00:02<00:15, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▌        | 1.51G/9.94G [00:02<00:15, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.57G/9.94G [00:02<00:15, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▋        | 1.64G/9.94G [00:03<00:14, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.70G/9.94G [00:03<00:14, 561MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.76G/9.94G [00:03<00:14, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.82G/9.94G [00:03<00:14, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.89G/9.94G [00:03<00:14, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|█▉        | 1.95G/9.94G [00:03<00:14, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 2.01G/9.94G [00:03<00:14, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.08G/9.94G [00:03<00:14, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.14G/9.94G [00:04<00:14, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.20G/9.94G [00:04<00:14, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.26G/9.94G [00:04<00:14, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.33G/9.94G [00:04<00:14, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.39G/9.94G [00:04<00:14, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.45G/9.94G [00:04<00:14, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.94G [00:04<00:14, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.57G/9.94G [00:04<00:14, 520MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▋       | 2.62G/9.94G [00:04<00:15, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.68G/9.94G [00:05<00:14, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.75G/9.94G [00:05<00:13, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.81G/9.94G [00:05<00:13, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.86G/9.94G [00:05<00:13, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.93G/9.94G [00:05<00:13, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 2.99G/9.94G [00:05<00:13, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.05G/9.94G [00:05<00:12, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███▏      | 3.11G/9.94G [00:05<00:12, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.18G/9.94G [00:05<00:12, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.24G/9.94G [00:06<00:12, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.30G/9.94G [00:06<00:12, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.37G/9.94G [00:06<00:12, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.43G/9.94G [00:06<00:11, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.49G/9.94G [00:06<00:11, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.55G/9.94G [00:06<00:11, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▋      | 3.62G/9.94G [00:06<00:11, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.68G/9.94G [00:06<00:11, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.74G/9.94G [00:07<00:11, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.81G/9.94G [00:07<00:11, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.87G/9.94G [00:07<00:10, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.93G/9.94G [00:07<00:10, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.00G/9.94G [00:07<00:10, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.06G/9.94G [00:07<00:10, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████▏     | 4.12G/9.94G [00:07<00:10, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.18G/9.94G [00:07<00:10, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.25G/9.94G [00:07<00:10, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.31G/9.94G [00:08<00:10, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.37G/9.94G [00:08<00:10, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.44G/9.94G [00:08<00:10, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.50G/9.94G [00:08<00:10, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.56G/9.94G [00:08<00:10, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▋     | 4.61G/9.94G [00:08<00:11, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.68G/9.94G [00:08<00:10, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.73G/9.94G [00:09<00:13, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.77G/9.94G [00:09<00:13, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▊     | 4.83G/9.94G [00:09<00:11, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.89G/9.94G [00:09<00:11, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.95G/9.94G [00:09<00:10, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.00G/9.94G [00:09<00:10, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.05G/9.94G [00:09<00:09, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████▏    | 5.11G/9.94G [00:09<00:09, 501MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.17G/9.94G [00:09<00:09, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.23G/9.94G [00:09<00:09, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.28G/9.94G [00:10<00:16, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▎    | 5.34G/9.94G [00:10<00:14, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.40G/9.94G [00:10<00:12, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.46G/9.94G [00:10<00:10, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.53G/9.94G [00:10<00:09, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.59G/9.94G [00:10<00:09, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.64G/9.94G [00:11<00:08, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.70G/9.94G [00:11<00:08, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.77G/9.94G [00:11<00:08, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▊    | 5.83G/9.94G [00:11<00:08, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.89G/9.94G [00:11<00:07, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|█████▉    | 5.96G/9.94G [00:11<00:07, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.02G/9.94G [00:11<00:07, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.08G/9.94G [00:11<00:07, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.14G/9.94G [00:12<00:07, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.21G/9.94G [00:12<00:06, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.27G/9.94G [00:12<00:06, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▎   | 6.33G/9.94G [00:12<00:06, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.40G/9.94G [00:12<00:06, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.46G/9.94G [00:12<00:06, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.52G/9.94G [00:12<00:06, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.59G/9.94G [00:12<00:06, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.94G [00:12<00:05, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.94G [00:13<00:05, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.94G [00:13<00:05, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.84G/9.94G [00:13<00:05, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.90G/9.94G [00:13<00:05, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 6.96G/9.94G [00:13<00:05, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.03G/9.94G [00:13<00:05, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████▏  | 7.09G/9.94G [00:13<00:05, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.15G/9.94G [00:13<00:05, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.21G/9.94G [00:13<00:05, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.28G/9.94G [00:14<00:04, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.34G/9.94G [00:14<00:04, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.94G [00:14<00:04, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.47G/9.94G [00:14<00:04, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.53G/9.94G [00:14<00:04, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▋  | 7.59G/9.94G [00:14<00:04, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.65G/9.94G [00:14<00:04, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.72G/9.94G [00:14<00:04, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.78G/9.94G [00:15<00:04, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.84G/9.94G [00:15<00:04, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.91G/9.94G [00:15<00:03, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 7.97G/9.94G [00:15<00:03, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.03G/9.94G [00:15<00:03, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████▏ | 8.10G/9.94G [00:15<00:03, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.16G/9.94G [00:15<00:03, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.22G/9.94G [00:15<00:03, 484MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.27G/9.94G [00:16<00:03, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.34G/9.94G [00:16<00:03, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.94G [00:16<00:03, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.45G/9.94G [00:16<00:02, 508MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.50G/9.94G [00:16<00:02, 511MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.57G/9.94G [00:16<00:02, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.63G/9.94G [00:16<00:02, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.69G/9.94G [00:16<00:02, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.76G/9.94G [00:16<00:02, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▊ | 8.82G/9.94G [00:17<00:02, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.88G/9.94G [00:17<00:01, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.94G/9.94G [00:17<00:01, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.01G/9.94G [00:17<00:01, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.07G/9.94G [00:17<00:01, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.13G/9.94G [00:17<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.20G/9.94G [00:17<00:01, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.26G/9.94G [00:17<00:01, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.32G/9.94G [00:17<00:01, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.38G/9.94G [00:18<00:01, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.45G/9.94G [00:18<00:00, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.51G/9.94G [00:18<00:00, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▋| 9.57G/9.94G [00:18<00:00, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.94G [00:18<00:00, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.70G/9.94G [00:18<00:00, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.76G/9.94G [00:18<00:00, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.83G/9.94G [00:18<00:00, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.89G/9.94G [00:19<00:00, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.94G/9.94G [00:19<00:00, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.94G/9.94G [00:19<00:00, 520MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:19<00:19, 19.60s/it]\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|▏         | 62.9M/4.54G [00:00<00:08, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   3%|▎         | 126M/4.54G [00:00<00:08, 531MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▍         | 189M/4.54G [00:00<00:08, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▌         | 252M/4.54G [00:00<00:07, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 315M/4.54G [00:00<00:07, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 377M/4.54G [00:00<00:07, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|▉         | 440M/4.54G [00:00<00:07, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 503M/4.54G [00:00<00:07, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  12%|█▏        | 566M/4.54G [00:01<00:07, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 629M/4.54G [00:01<00:07, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▌        | 692M/4.54G [00:01<00:06, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 755M/4.54G [00:01<00:06, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 818M/4.54G [00:01<00:10, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▉        | 870M/4.54G [00:01<00:09, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  21%|██        | 933M/4.54G [00:01<00:08, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 996M/4.54G [00:02<00:07, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 1.06G/4.54G [00:02<00:07, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▍       | 1.12G/4.54G [00:02<00:06, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▌       | 1.18G/4.54G [00:02<00:06, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 1.25G/4.54G [00:02<00:06, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.31G/4.54G [00:02<00:06, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  30%|███       | 1.37G/4.54G [00:02<00:05, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.44G/4.54G [00:02<00:05, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.50G/4.54G [00:02<00:06, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.56G/4.54G [00:03<00:05, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.63G/4.54G [00:03<00:05, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.69G/4.54G [00:03<00:05, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.74G/4.54G [00:03<00:05, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▉      | 1.79G/4.54G [00:03<00:05, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████      | 1.85G/4.54G [00:03<00:05, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.91G/4.54G [00:03<00:04, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.97G/4.54G [00:03<00:04, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▍     | 2.03G/4.54G [00:03<00:04, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▌     | 2.10G/4.54G [00:04<00:04, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 2.16G/4.54G [00:04<00:04, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  49%|████▉     | 2.22G/4.54G [00:04<00:04, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|█████     | 2.29G/4.54G [00:04<00:04, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 2.35G/4.54G [00:04<00:04, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 2.41G/4.54G [00:04<00:03, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▍    | 2.47G/4.54G [00:04<00:03, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 2.54G/4.54G [00:04<00:03, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 2.60G/4.54G [00:04<00:03, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▊    | 2.66G/4.54G [00:05<00:03, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|██████    | 2.73G/4.54G [00:05<00:03, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████▏   | 2.79G/4.54G [00:05<00:03, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  63%|██████▎   | 2.85G/4.54G [00:05<00:03, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▍   | 2.92G/4.54G [00:05<00:03, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▌   | 2.98G/4.54G [00:05<00:02, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 3.04G/4.54G [00:05<00:02, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 3.10G/4.54G [00:05<00:02, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  70%|██████▉   | 3.17G/4.54G [00:06<00:02, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████   | 3.23G/4.54G [00:06<00:02, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 3.29G/4.54G [00:06<00:02, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▍  | 3.36G/4.54G [00:06<00:02, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▌  | 3.42G/4.54G [00:06<00:02, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 3.48G/4.54G [00:06<00:01, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 3.54G/4.54G [00:06<00:01, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  79%|███████▉  | 3.61G/4.54G [00:06<00:01, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████  | 3.67G/4.54G [00:06<00:01, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  82%|████████▏ | 3.73G/4.54G [00:07<00:01, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▎ | 3.80G/4.54G [00:07<00:01, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▍ | 3.86G/4.54G [00:07<00:01, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▋ | 3.92G/4.54G [00:07<00:01, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  88%|████████▊ | 3.98G/4.54G [00:07<00:00, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▉ | 4.05G/4.54G [00:07<00:00, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  91%|█████████ | 4.11G/4.54G [00:07<00:00, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 4.17G/4.54G [00:07<00:00, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 4.24G/4.54G [00:07<00:00, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▍| 4.30G/4.54G [00:08<00:00, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 4.36G/4.54G [00:08<00:00, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 4.42G/4.54G [00:08<00:00, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▉| 4.49G/4.54G [00:08<00:00, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 4.54G/4.54G [00:08<00:00, 530MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:28<00:00, 13.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:28<00:00, 14.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 120/120 [00:00<00:00, 1.28MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['k_proj', 'down_proj', 'o_proj', 'q_proj', 'v_proj', 'up_proj', 'gate_proj']\u001b[0m\n",
      "\u001b[34mCUDA extension not installed.\u001b[0m\n",
      "\u001b[34mCUDA extension not installed.\u001b[0m\n",
      "\u001b[34mtrainable params: 167,772,160 || all params: 7,409,520,640 || trainable%: 2.2642781922259414\u001b[0m\n",
      "\u001b[34m0%|          | 0/320 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\u001b[0m\n",
      "\u001b[34m0%|          | 1/320 [01:12<6:23:24, 72.11s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/320 [02:23<6:21:07, 71.91s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/320 [03:35<6:19:23, 71.81s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 4/320 [04:47<6:17:57, 71.76s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/320 [05:58<6:16:37, 71.74s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/320 [07:10<6:15:21, 71.72s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/320 [08:22<6:14:06, 71.71s/it]\u001b[0m\n",
      "\u001b[34m2%|▎         | 8/320 [09:34<6:12:53, 71.71s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/320 [10:45<6:11:39, 71.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/320 [11:57<6:10:27, 71.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5958, 'learning_rate': 0.0002, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/320 [11:57<6:10:27, 71.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/320 [13:09<6:09:15, 71.70s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 12/320 [14:20<6:08:02, 71.70s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/320 [15:32<6:06:50, 71.70s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/320 [16:44<6:05:38, 71.70s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 15/320 [17:55<6:04:27, 71.70s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 16/320 [19:07<6:03:15, 71.70s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 17/320 [20:19<6:02:03, 71.70s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 18/320 [21:31<6:00:53, 71.70s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 19/320 [22:42<5:59:40, 71.70s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/320 [23:54<5:58:28, 71.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2229, 'learning_rate': 0.00019948693233918952, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/320 [23:54<5:58:28, 71.70s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 21/320 [25:06<5:57:17, 71.70s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 22/320 [26:17<5:56:05, 71.70s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/320 [27:29<5:54:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 24/320 [28:41<5:53:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 25/320 [29:52<5:52:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/320 [31:04<5:51:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/320 [32:16<5:50:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 28/320 [33:27<5:48:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 29/320 [34:39<5:47:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 30/320 [35:51<5:46:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2095, 'learning_rate': 0.00019795299412524945, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m9%|▉         | 30/320 [35:51<5:46:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 31/320 [37:03<5:45:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 32/320 [38:14<5:44:07, 71.69s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 33/320 [39:26<5:42:55, 71.69s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 34/320 [40:38<5:41:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 35/320 [41:49<5:40:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 36/320 [43:01<5:39:20, 71.69s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 37/320 [44:13<5:38:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 38/320 [45:24<5:36:57, 71.69s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 39/320 [46:36<5:35:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 40/320 [47:48<5:34:34, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1526, 'learning_rate': 0.00019541392564000488, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m12%|█▎        | 40/320 [47:48<5:34:34, 71.69s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 41/320 [48:59<5:33:22, 71.69s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 42/320 [50:11<5:32:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 43/320 [51:23<5:30:59, 71.69s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 44/320 [52:35<5:29:47, 71.70s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 45/320 [53:46<5:28:36, 71.70s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 46/320 [54:58<5:27:24, 71.70s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 47/320 [56:10<5:26:12, 71.70s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 48/320 [57:21<5:25:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 49/320 [58:33<5:23:49, 71.69s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/320 [59:45<5:22:37, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1634, 'learning_rate': 0.00019189578116202307, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/320 [59:45<5:22:37, 71.69s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 51/320 [1:00:56<5:21:25, 71.69s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 52/320 [1:02:08<5:20:13, 71.69s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 53/320 [1:03:20<5:19:02, 71.69s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 54/320 [1:04:31<5:17:50, 71.69s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 55/320 [1:05:43<5:16:38, 71.69s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 56/320 [1:06:55<5:15:26, 71.69s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 57/320 [1:08:07<5:14:14, 71.69s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 58/320 [1:09:18<5:13:03, 71.69s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 59/320 [1:10:30<5:11:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 60/320 [1:11:42<5:10:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.151, 'learning_rate': 0.00018743466161445823, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 60/320 [1:11:42<5:10:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 61/320 [1:12:53<5:09:27, 71.69s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 62/320 [1:14:05<5:08:16, 71.69s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 63/320 [1:15:17<5:07:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 64/320 [1:16:28<5:05:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 65/320 [1:17:40<5:04:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 66/320 [1:18:52<5:03:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 67/320 [1:20:03<5:02:17, 71.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 68/320 [1:21:15<5:01:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 69/320 [1:22:27<4:59:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 70/320 [1:23:39<4:58:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1362, 'learning_rate': 0.00018207634412072764, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 70/320 [1:23:39<4:58:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 71/320 [1:24:50<4:57:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 72/320 [1:26:02<4:56:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 73/320 [1:27:14<4:55:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 74/320 [1:28:25<4:53:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 75/320 [1:29:37<4:52:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 76/320 [1:30:49<4:51:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 77/320 [1:32:00<4:50:21, 71.69s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 78/320 [1:33:12<4:49:09, 71.69s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 79/320 [1:34:24<4:47:57, 71.69s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 80/320 [1:35:35<4:46:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1094, 'learning_rate': 0.0001758758122692791, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 80/320 [1:35:35<4:46:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 81/320 [1:36:47<4:45:34, 71.69s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 82/320 [1:37:59<4:44:22, 71.69s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 83/320 [1:39:11<4:43:11, 71.69s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 84/320 [1:40:22<4:41:59, 71.69s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 85/320 [1:41:34<4:40:48, 71.69s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 86/320 [1:42:46<4:39:36, 71.69s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 87/320 [1:43:57<4:38:24, 71.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 88/320 [1:45:09<4:37:12, 71.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 89/320 [1:46:21<4:36:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 90/320 [1:47:32<4:34:48, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0185, 'learning_rate': 0.00016889669190756868, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 90/320 [1:47:32<4:34:48, 71.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 91/320 [1:48:44<4:33:36, 71.69s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 92/320 [1:49:56<4:32:25, 71.69s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 93/320 [1:51:07<4:31:13, 71.69s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 94/320 [1:52:19<4:30:01, 71.69s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 95/320 [1:53:31<4:28:50, 71.69s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 96/320 [1:54:43<4:27:38, 71.69s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 97/320 [1:55:54<4:26:27, 71.69s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 98/320 [1:57:06<4:25:15, 71.69s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 99/320 [1:58:18<4:24:03, 71.69s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 100/320 [1:59:29<4:22:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.084, 'learning_rate': 0.0001612105982547663, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 100/320 [1:59:29<4:22:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 101/320 [2:00:41<4:21:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 102/320 [2:01:53<4:20:28, 71.69s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 103/320 [2:03:04<4:19:16, 71.69s/it]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 104/320 [2:04:16<4:18:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 105/320 [2:05:28<4:16:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 106/320 [2:06:39<4:15:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 107/320 [2:07:51<4:14:30, 71.69s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 108/320 [2:09:03<4:13:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 109/320 [2:10:14<4:12:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 110/320 [2:11:26<4:10:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.102, 'learning_rate': 0.00015289640103269625, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 110/320 [2:11:26<4:10:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 111/320 [2:12:38<4:09:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 112/320 [2:13:50<4:08:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 113/320 [2:15:01<4:07:20, 71.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 114/320 [2:16:13<4:06:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 115/320 [2:17:25<4:04:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 116/320 [2:18:36<4:03:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 117/320 [2:19:48<4:02:33, 71.69s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 118/320 [2:21:00<4:01:21, 71.69s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 119/320 [2:22:11<4:00:09, 71.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/320 [2:23:23<3:58:58, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0798, 'learning_rate': 0.00014403941515576344, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/320 [2:23:23<3:58:58, 71.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 121/320 [2:24:35<3:57:46, 71.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 122/320 [2:25:46<3:56:35, 71.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 123/320 [2:26:58<3:55:23, 71.69s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 124/320 [2:28:10<3:54:11, 71.69s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 125/320 [2:29:22<3:53:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 126/320 [2:30:33<3:51:48, 71.69s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 127/320 [2:31:45<3:50:36, 71.69s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 128/320 [2:32:57<3:49:24, 71.69s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 129/320 [2:34:08<3:48:13, 71.69s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 130/320 [2:35:20<3:47:01, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0604, 'learning_rate': 0.00013473052528448201, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m41%|████      | 130/320 [2:35:20<3:47:01, 71.69s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 131/320 [2:36:32<3:45:49, 71.69s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 132/320 [2:37:43<3:44:38, 71.69s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 133/320 [2:38:55<3:43:26, 71.69s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 134/320 [2:40:07<3:42:14, 71.69s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 135/320 [2:41:18<3:41:02, 71.69s/it]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 136/320 [2:42:30<3:39:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 137/320 [2:43:42<3:38:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 138/320 [2:44:54<3:37:27, 71.69s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 139/320 [2:46:05<3:36:16, 71.69s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 140/320 [2:47:17<3:35:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9854, 'learning_rate': 0.00012506525322587207, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 140/320 [2:47:17<3:35:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 141/320 [2:48:29<3:33:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 142/320 [2:49:40<3:32:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 143/320 [2:50:52<3:31:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 144/320 [2:52:04<3:30:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 145/320 [2:53:15<3:29:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 146/320 [2:54:27<3:27:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 147/320 [2:55:39<3:26:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 148/320 [2:56:50<3:25:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 149/320 [2:58:02<3:24:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 150/320 [2:59:14<3:23:07, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0057, 'learning_rate': 0.00011514277775045768, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 150/320 [2:59:14<3:23:07, 71.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 151/320 [3:00:26<3:21:55, 71.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 152/320 [3:01:37<3:20:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 153/320 [3:02:49<3:19:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 154/320 [3:04:01<3:18:20, 71.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 155/320 [3:05:12<3:17:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 156/320 [3:06:24<3:15:57, 71.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 157/320 [3:07:36<3:14:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 158/320 [3:08:47<3:13:33, 71.69s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 159/320 [3:09:59<3:12:22, 71.69s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 160/320 [3:11:11<3:11:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9813, 'learning_rate': 0.00010506491688387127, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 160/320 [3:11:11<3:11:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 161/320 [3:12:23<3:10:44, 71.98s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 162/320 [3:13:35<3:09:18, 71.89s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 163/320 [3:14:47<3:07:57, 71.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 164/320 [3:15:59<3:06:39, 71.79s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 165/320 [3:17:10<3:05:23, 71.76s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 166/320 [3:18:22<3:04:08, 71.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 167/320 [3:19:34<3:02:54, 71.73s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 168/320 [3:20:45<3:01:40, 71.72s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 169/320 [3:21:57<3:00:28, 71.71s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 170/320 [3:23:09<2:59:15, 71.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8875, 'learning_rate': 9.493508311612874e-05, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 170/320 [3:23:09<2:59:15, 71.70s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 171/320 [3:24:20<2:58:03, 71.70s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 172/320 [3:25:32<2:56:51, 71.70s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 173/320 [3:26:44<2:55:39, 71.70s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 174/320 [3:27:55<2:54:27, 71.70s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 175/320 [3:29:07<2:53:15, 71.69s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 176/320 [3:30:19<2:52:03, 71.69s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 177/320 [3:31:31<2:50:52, 71.69s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 178/320 [3:32:42<2:49:40, 71.69s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 179/320 [3:33:54<2:48:28, 71.69s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 180/320 [3:35:06<2:47:17, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8644, 'learning_rate': 8.485722224954237e-05, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 180/320 [3:35:06<2:47:17, 71.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 181/320 [3:36:17<2:46:05, 71.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 182/320 [3:37:29<2:44:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 183/320 [3:38:41<2:43:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 184/320 [3:39:52<2:42:30, 71.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 185/320 [3:41:04<2:41:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 186/320 [3:42:16<2:40:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 187/320 [3:43:27<2:38:55, 71.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 188/320 [3:44:39<2:37:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 189/320 [3:45:51<2:36:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 190/320 [3:47:03<2:35:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1223, 'learning_rate': 7.493474677412794e-05, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 190/320 [3:47:03<2:35:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 191/320 [3:48:14<2:34:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 192/320 [3:49:26<2:32:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 193/320 [3:50:38<2:31:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 194/320 [3:51:49<2:30:33, 71.69s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 195/320 [3:53:01<2:29:21, 71.69s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 196/320 [3:54:13<2:28:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 197/320 [3:55:24<2:26:58, 71.69s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 198/320 [3:56:36<2:25:46, 71.69s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 199/320 [3:57:48<2:24:35, 71.69s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 200/320 [3:58:59<2:23:23, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8889, 'learning_rate': 6.526947471551798e-05, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 200/320 [3:58:59<2:23:23, 71.69s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 201/320 [4:00:11<2:22:11, 71.70s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 202/320 [4:01:23<2:21:00, 71.70s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 203/320 [4:02:35<2:19:48, 71.70s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 204/320 [4:03:46<2:18:36, 71.70s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 205/320 [4:04:58<2:17:24, 71.69s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 206/320 [4:06:10<2:16:13, 71.69s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 207/320 [4:07:21<2:15:01, 71.69s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 208/320 [4:08:33<2:13:49, 71.69s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 209/320 [4:09:45<2:12:38, 71.70s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 210/320 [4:10:56<2:11:26, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8884, 'learning_rate': 5.596058484423656e-05, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 210/320 [4:10:56<2:11:26, 71.69s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 211/320 [4:12:08<2:10:14, 71.69s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 212/320 [4:13:20<2:09:02, 71.69s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 213/320 [4:14:31<2:07:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 214/320 [4:15:43<2:06:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 215/320 [4:16:55<2:05:27, 71.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 216/320 [4:18:07<2:04:16, 71.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 217/320 [4:19:18<2:03:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 218/320 [4:20:30<2:01:52, 71.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 219/320 [4:21:42<2:00:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 220/320 [4:22:53<1:59:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8346, 'learning_rate': 4.710359896730379e-05, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 220/320 [4:22:53<1:59:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 221/320 [4:24:05<1:58:17, 71.69s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 222/320 [4:25:17<1:57:05, 71.69s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 223/320 [4:26:28<1:55:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 224/320 [4:27:40<1:54:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 225/320 [4:28:52<1:53:30, 71.69s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 226/320 [4:30:03<1:52:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 227/320 [4:31:15<1:51:07, 71.69s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 228/320 [4:32:27<1:49:55, 71.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 229/320 [4:33:39<1:48:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 230/320 [4:34:50<1:47:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8206, 'learning_rate': 3.878940174523371e-05, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 230/320 [4:34:50<1:47:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 231/320 [4:36:02<1:46:20, 71.69s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 232/320 [4:37:14<1:45:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 233/320 [4:38:25<1:43:57, 71.69s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 234/320 [4:39:37<1:42:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 235/320 [4:40:49<1:41:33, 71.69s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 236/320 [4:42:00<1:40:22, 71.69s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 237/320 [4:43:12<1:39:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 238/320 [4:44:24<1:37:58, 71.69s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 239/320 [4:45:35<1:36:47, 71.69s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 240/320 [4:46:47<1:35:35, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.838, 'learning_rate': 3.110330809243134e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 240/320 [4:46:47<1:35:35, 71.69s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 241/320 [4:47:59<1:34:23, 71.69s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 242/320 [4:49:11<1:33:11, 71.69s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 243/320 [4:50:22<1:32:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 244/320 [4:51:34<1:30:48, 71.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 245/320 [4:52:46<1:29:36, 71.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 246/320 [4:53:57<1:28:25, 71.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 247/320 [4:55:09<1:27:13, 71.69s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 248/320 [4:56:21<1:26:01, 71.69s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 249/320 [4:57:32<1:24:50, 71.69s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 250/320 [4:58:44<1:23:38, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8692, 'learning_rate': 2.4124187730720917e-05, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 250/320 [4:58:44<1:23:38, 71.69s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 251/320 [4:59:56<1:22:26, 71.69s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 252/320 [5:01:07<1:21:15, 71.69s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 253/320 [5:02:19<1:20:03, 71.69s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 254/320 [5:03:31<1:18:51, 71.69s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 255/320 [5:04:43<1:17:40, 71.69s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 256/320 [5:05:54<1:16:28, 71.69s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 257/320 [5:07:06<1:15:16, 71.69s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 258/320 [5:08:18<1:14:05, 71.69s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 259/320 [5:09:29<1:12:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 260/320 [5:10:41<1:11:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8029, 'learning_rate': 1.7923655879272393e-05, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 260/320 [5:10:41<1:11:41, 71.69s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 261/320 [5:11:53<1:10:29, 71.69s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 262/320 [5:13:04<1:09:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 263/320 [5:14:16<1:08:06, 71.69s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 264/320 [5:15:28<1:06:54, 71.69s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 265/320 [5:16:40<1:05:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 266/320 [5:17:51<1:04:31, 71.69s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 267/320 [5:19:03<1:03:19, 71.69s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 268/320 [5:20:15<1:02:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 269/320 [5:21:26<1:00:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 270/320 [5:22:38<59:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8786, 'learning_rate': 1.2565338385541792e-05, 'epoch': 1.68}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 270/320 [5:22:38<59:44, 71.69s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 271/320 [5:23:50<58:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 272/320 [5:25:01<57:21, 71.69s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 273/320 [5:26:13<56:09, 71.69s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 274/320 [5:27:25<54:57, 71.69s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 275/320 [5:28:36<53:46, 71.69s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 276/320 [5:29:48<52:34, 71.69s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 277/320 [5:31:00<51:22, 71.69s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 278/320 [5:32:12<50:11, 71.69s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 279/320 [5:33:23<48:59, 71.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 280/320 [5:34:35<47:47, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8341, 'learning_rate': 8.10421883797694e-06, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 280/320 [5:34:35<47:47, 71.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 281/320 [5:35:47<46:36, 71.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 282/320 [5:36:58<45:24, 71.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 283/320 [5:38:10<44:12, 71.69s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 284/320 [5:39:22<43:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 285/320 [5:40:33<41:49, 71.69s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 286/320 [5:41:45<40:37, 71.69s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 287/320 [5:42:57<39:25, 71.69s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 288/320 [5:44:08<38:14, 71.69s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 289/320 [5:45:20<37:02, 71.69s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 290/320 [5:46:32<35:50, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.799, 'learning_rate': 4.586074359995119e-06, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 290/320 [5:46:32<35:50, 71.69s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 291/320 [5:47:44<34:39, 71.69s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 292/320 [5:48:55<33:27, 71.69s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 293/320 [5:50:07<32:15, 71.69s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 294/320 [5:51:19<31:04, 71.69s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 295/320 [5:52:30<29:52, 71.69s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 296/320 [5:53:42<28:40, 71.69s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 297/320 [5:54:54<27:28, 71.69s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 298/320 [5:56:05<26:17, 71.69s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 299/320 [5:57:17<25:05, 71.69s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 300/320 [5:58:29<23:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7626, 'learning_rate': 2.0470058747505516e-06, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 300/320 [5:58:29<23:53, 71.69s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 301/320 [5:59:40<22:42, 71.69s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 302/320 [6:00:52<21:30, 71.69s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 303/320 [6:02:04<20:18, 71.69s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 304/320 [6:03:16<19:07, 71.69s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 305/320 [6:04:27<17:55, 71.69s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 306/320 [6:05:39<16:43, 71.69s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 307/320 [6:06:51<15:32, 71.69s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 308/320 [6:08:02<14:20, 71.69s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 309/320 [6:09:14<13:08, 71.69s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 310/320 [6:10:26<11:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8702, 'learning_rate': 5.130676608104845e-07, 'epoch': 1.93}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 310/320 [6:10:26<11:56, 71.69s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 311/320 [6:11:37<10:45, 71.69s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 312/320 [6:12:49<09:33, 71.69s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 313/320 [6:14:01<08:21, 71.69s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 314/320 [6:15:12<07:10, 71.69s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 315/320 [6:16:24<05:58, 71.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 316/320 [6:17:36<04:46, 71.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 317/320 [6:18:48<03:35, 71.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 318/320 [6:19:59<02:23, 71.69s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 319/320 [6:21:11<01:11, 71.69s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 320/320 [6:22:23<00:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7845, 'learning_rate': 0.0, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m100%|██████████| 320/320 [6:22:23<00:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 22944.1217, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.014, 'train_loss': 0.9938687697052956, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m100%|██████████| 320/320 [6:22:24<00:00, 71.69s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 320/320 [6:22:24<00:00, 71.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\u001b[0m\n",
      "\n",
      "2024-02-06 18:30:58 Uploading - Uploading generated training model\u001b[34mtokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 13.2MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 110MB/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 459kB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|██████████| 101/101 [00:00<00:00, 958kB/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m2024-02-06 18:30:53,774 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-06 18:30:53,774 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-06 18:30:53,775 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-02-06 18:31:29 Completed - Training job completed\n",
      "Training seconds: 23554\n",
      "Billable seconds: 23554\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aabe5a2b-8bce-4879-aa20-05d455c6cc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579782f2-c003-47f4-9643-1115757382ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-south-1-005418323977/huggingface-qlora-teknium-OpenHermes-2--2024-02-06-11-57-39-842/output/model/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e138cd-f76b-4b2d-895d-3d727493fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoenv",
   "language": "python",
   "name": "recoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
